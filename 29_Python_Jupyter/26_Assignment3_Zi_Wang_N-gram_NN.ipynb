{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.Summary\"></a>\n",
    "# 1.Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to use the Penn Treebank corpus for this assignment. Four data fles are provided: train.txt,\n",
    "train.5k.txt, valid.txt, and input.txt. You can use train.txt to train your models and use valid.txt for testing.\n",
    "File input.txt can be used for a sanity check on whether the model produces coherent sequences of words for\n",
    "unseen data with no next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.Table of Contents\"></a>\n",
    "# 2.Table of Contents\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><a href=\"#1.Summary\">Summary</a></li>\n",
    "    <li><a href=\"#2.Table of Contents\">Table of Contents</a></li>\n",
    "    <li><a href=\"#3.N-gram\">N-gram</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#3.1 Preprocess\">3.1 Preprocess</a></li>\n",
    "       <li><a href=\"#3.2 Bigram\">3.2 Bigram</a></li> \n",
    "       <li><a href=\"#3.3 Trigram\">3.3 Trigram</a></li>\n",
    "       <li><a href=\"#3.4 Good Turing Smoothing\">3.4 Good Turing Smoothing</a></li> \n",
    "       <li><a href=\"#3.5 Kneser-Ney Smoothing\">3.5 Kneser-Ney Smoothing</a></li> \n",
    "       <li><a href=\"#3.6 Predict\">3.6 Predict</a></li> \n",
    "       <li><a href=\"#3.7 Kneser-Ney N-gram Prediction\">3.7 Kneser-Ney N-gram Prediction</a></li>  \n",
    "    </ul>\n",
    "    <li><a href=\"#4.RNN\">RNN</a></li>\n",
    "    <ul>\n",
    "       <li><a href=\"#4.1 Initial Parameters\">4.1 Initial Parameters</a></li>\n",
    "       <li><a href=\"#4.2 Forward Pass\">4.2 Forward Pass</a></li> \n",
    "       <li><a href=\"#4.3 Loss\">4.3 Loss</a></li>\n",
    "       <li><a href=\"#4.4 Traing Step\">4.4 Traing Step</a></li> \n",
    "       <li><a href=\"#4.5 Perplexity\">4.5 Perplexity</a></li> \n",
    "       <li><a href=\"#4.6 Predict Words\">4.6 Predict Words</a></li>   \n",
    "    </ul>\n",
    "    <li><a href=\"#5.Unused Code\">Unused Code</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.N-gram\"></a>\n",
    "# 3.N-gram\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_transform as tft\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "import nltk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1 Preprocess\"></a>\n",
    "## 3.1 Preprocess\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the train and validation data, build the vocabulary, tokenize, etc. Here is another concen, in our raw data, there are a lot of <unk> and punctuation. Sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess(object):\n",
    "    \"\"\"\n",
    "    This class contain read_file() and token() functions. In the future, maybe we can use main() to direct\n",
    "    process and generate a new DataFrame with cleaned data\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This function is used to transfer hyperparameters between functions in CLASS \n",
    "        \"\"\"\n",
    "        #self.test = test\n",
    "        \n",
    "    def read_file(self, path):\n",
    "        \"\"\"\n",
    "        Provide local path and output a DataFrame with new column\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path,names= ['test'])\n",
    "        self.df = df\n",
    "        #print(path)\n",
    "        return df\n",
    "        \n",
    "    def token(self, df):\n",
    "        \"\"\"\n",
    "        clean->tokenizer->sequence\n",
    "        Arugs:\n",
    "        -----\n",
    "        df:array-like\n",
    "            input a full\n",
    "        \"\"\"\n",
    "        # create a new list to fill in clearned text\n",
    "        clean_list=[]\n",
    "        for i in range(len(df)):\n",
    "            # using regix to only extract letters, other puncuation \\n \\t will not be included\n",
    "            cleaned_data = re.sub(r'[^A-Za-z. ]', '', df['test'][i])\n",
    "            # \n",
    "            clean_list.append(cleaned_data)\n",
    "        #create a new column with cleaned data\n",
    "        df['cleaned'] = clean_list\n",
    "        \n",
    "        #\n",
    "        tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "        #transfor sentence to number vector\n",
    "        tokenizer.fit_on_texts(df['cleaned'])\n",
    "        # get the index of each indivudule word\n",
    "        word_index = tokenizer.word_index\n",
    "        \n",
    "        #only choose most frequency 10,000 word_index and transfrom sentences \n",
    "        #with them into number vector like[564, 452, 23, 1]\n",
    "        #the out of vocabulary wiht be give 1. Maximize number of texts_to_sequences()\n",
    "        #will be 9999, which means the 10000th frequceny will \n",
    "        #not show in this sequcen\n",
    "        sequences = tokenizer.texts_to_sequences(df['cleaned'])\n",
    "        \n",
    "        # ***********************Random Print Part***********************\n",
    "        # random pick a row from sequnce\n",
    "        random_num = np.random.randint(0,len(sequences))\n",
    "        print(f\"Random pick the {random_num}th from corpus sequence\")\n",
    "        print(f\"Current vectorized sequnce is {sequences[random_num]}\")\n",
    "        print(f\"Converted back to string:\", \" \".join(list(word_index.keys())[list(word_index.values()).index(i)] \\\n",
    "                                                     for i in sequences[random_num]))\n",
    "        \n",
    "        # ***********************Random Print Part***********************\n",
    "        \n",
    "        return sequences, word_index, df\n",
    "    \n",
    "    def main(self):\n",
    "        path_input = \"03_data/13_a3-data/input.txt\" \n",
    "        path_train5k = \"03_data/13_a3-data/train.5k.txt\"\n",
    "        path_train = \"03_data/13_a3-data/train.txt\"\n",
    "        path_valid = \"03_data/13_a3-data/valid.txt\"\n",
    "        df_input = read_file(path_input)\n",
    "        seq, word_index,df = token(df_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2 Bigram\"></a>\n",
    "## 3.2 Bigram\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(seq, word_index, df, grams=2):\n",
    "    \"\"\"\n",
    "    Maunally put sequence transform to 2-grams sequence with one by one and inser 2-gram sequcen into \n",
    "    DataFrame with a new column ['bigram']\n",
    "    \n",
    "    Argus:\n",
    "    -----\n",
    "    seq:list\n",
    "        sequence come from preprocess() function, which is tokenizer and sequence result. Each number represent\n",
    "        a word by a int number\n",
    "    \n",
    "    word_index:index\n",
    "        used to find mapping relation between sequence number and real word\n",
    "    \"\"\"\n",
    "    # add DataFrame with a new column \"sequence\" and fill it with seq list\n",
    "    df['sequence'] = seq\n",
    "    # cereate a new empty column to contain bigram\n",
    "    df['bigram'] = \"\"\n",
    "    # change this column into changable type 'object'\n",
    "    df['bigram'] = df['bigram'].astype('object')\n",
    "    \n",
    "    # j iterate through samples of seq/df\n",
    "    for j in tqdm(range(len(seq))):\n",
    "        # clear  ngram list to contain bigram list\n",
    "        ngrams_list=[]\n",
    "        # iterate through every element in one row list\n",
    "        for i in range(len(seq[j])-grams):\n",
    "            # create a new sub-list from i to i+grams, like [i:i+2]\n",
    "            seq_bi = seq[j][i:i+grams]\n",
    "#                print(seq_bi)\n",
    "#                 if seq_bi not in ngrams.keys():\n",
    "#                     ngrams[seq_bi] = []\n",
    "            # add these sub-list into\n",
    "            ngrams_list.append(seq_bi)\n",
    "            #print(ngrams_list)\n",
    "        # for one sell in df, insert with bigram list\n",
    "        df.at[j,'bigram'] = ngrams_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3 Trigram\"></a>\n",
    "## 3.3 Trigram\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_nltk(df, num, col_num='cleaned'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # nltk.\n",
    "    #n_grams_list = []\n",
    "    df_copy = df.copy()\n",
    "    df_copy['bigram_nltk'] = \"\"\n",
    "    # change this column into changable type 'object'\n",
    "    df_copy['bigram_nltk'] = df_copy['bigram_nltk'].astype('object')\n",
    "    n_grams_list = []\n",
    "    for idx,i in tqdm(enumerate(df_copy[col_num])):\n",
    "        #print(idx)\n",
    "        n_grams = ngrams(nltk.word_tokenize(str(i)), num)\n",
    "        ngrams_str = [ ' '.join(grams) for grams in n_grams]\n",
    "        for j in ngrams_str:\n",
    "            n_grams_list.append(j)\n",
    "        df_copy.loc[idx,'bigram_nltk'] = ngrams_str\n",
    "    df_copy.loc['total', 'bigram_nltk'] = n_grams_list\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3165it [00:00, 3800.49it/s]\n"
     ]
    }
   ],
   "source": [
    "df_3 = ngram_nltk(df_input, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4 Good Turing Smoothing\"></a>\n",
    "## 3.4 Good Turing Smoothing\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_sents_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-464e4f69eadf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mall_trigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_sents_tokens\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mfreq_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_trigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mgt_trigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleGoodTuringProbDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_sents_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "def nltk_tri_pred(model, w1, w2):\n",
    "    prob_sum = 0\n",
    "    possible_next_w = defaultdict(lambda: 0)\n",
    "    for i in model.samples():\n",
    "        if i[0] == w1 and i[1] == w2: # given previous word\n",
    "            prob_sum += model.prob(i)\n",
    "#             print(\"{0}:{1}\".format(i, model.prob(i)))\n",
    "            possible_next_w[i[2]] = model.prob(i)\n",
    "    return possible_next_w\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# all_trigrams = ( ngram for sent in all_sents_tokens for ngram in ngrams(sent, 3, pad_left = True, pad_right = True))\n",
    "# freq_dist = nltk.FreqDist(all_trigrams)\n",
    "# gt_trigram = nltk.SimpleGoodTuringProbDist(freq_dist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5 Kneser-Ney Smoothing\"></a>\n",
    "## 3.5 Kneser-Ney Smoothing\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tri_pred(model, w1, w2):\n",
    "    prob_sum = 0\n",
    "    possible_next_w = defaultdict(lambda: None)\n",
    "    for i in model.samples():\n",
    "        if i[0] == w1 and i[1] == w2: # given previous word\n",
    "            prob_sum += model.prob(i)\n",
    "#             print(\"{0}:{1}\".format(i, model.prob(i)))\n",
    "            possible_next_w[i[2]] = model.prob(i)\n",
    "    return possible_next_w\n",
    "\n",
    "# all_trigrams = ( ngram for sent in all_sents_tokens for ngram in ngrams(sent, 3, pad_left = True, pad_right = True))\n",
    "# freq_dist = nltk.FreqDist(all_trigrams)\n",
    "# kneser_ney = nltk.KneserNeyProbDist(freq_dist, discount=0.75)\n",
    "\n",
    "w1,w2 = \"said\",'milton'\n",
    "nltk_tri_pred(kneser_ney, w1, w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.6 Predict\"></a>\n",
    "## 3.6 Predict\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ngram(df, first_row):\n",
    "    \"\"\"\n",
    "    This functino used only for predict bigram model\n",
    "    \n",
    "    Argus:\n",
    "    ---\n",
    "    df:ndarry-like\n",
    "        This dataframe must come from preprocess() function, which means have ['cleaned'] column\n",
    "    \"\"\"\n",
    "    #********************Build Unigram DataFrame***********************************\n",
    "    #create a dictionary {key=word : value=probabilty}\n",
    "    #create a DataFrame, index is words and column['value_cout', 'probabilty']\n",
    "    #before we caculate bigram, we need cacualte unigram first\n",
    "    df_unigram = ngram_nltk(df, 1, 'cleaned')\n",
    "    # count the occurance times of very \n",
    "    unigram_series = pd.Series(df_unigram.loc['total','bigram_nltk']).value_counts()\n",
    "    #create a new dataframe,which contain words itself and its cooresponding counts\n",
    "    df_prob_uni = pd.DataFrame(unigram_series, index=unigram_series.index, columns=['value_counts'])\n",
    "    # caculate the condition probabitlyf of indivudal words\n",
    "    df_prob_uni['probability'] = df_prob_uni['value_counts'] / unigram_series.sum()\n",
    "    #********************Build Unigram DataFrame***********************************\n",
    "    \n",
    "    \n",
    "    #********************Build Bigram DataFrame***********************************\n",
    "    # create bigram dataframe from ngram_nltk() fuction\n",
    "    df_bigram = ngram_nltk(df, 2, 'cleaned')\n",
    "    # count the time of very pair of two gram key pair\n",
    "    bigram_series = pd.Series(df_bigram.loc['total','bigram_nltk']).value_counts()\n",
    "    # create bigram dataframe with words pair as index. we need change index later\n",
    "    df_prob_bi = pd.DataFrame(bigram_series.index, \\\n",
    "                          index=bigram_series.index, \\\n",
    "                          columns=['bigram_pairs'])\n",
    "    # create a new column with his word pair, next step we need change index to numercial\n",
    "    df_prob_bi['value_counts'] = bigram_series\n",
    "    # create a new column for ward pair, this is conditinal probability\n",
    "    df_prob_bi['probability'] = df_prob_bi['value_counts'] / bigram_series.sum()\n",
    "    #extract two words into split column, we need use i_1 and i to caculate bigram probabilty\n",
    "    for i in df_prob_bi['bigram_pairs']:\n",
    "        #split bigram word pair into two column\n",
    "        m,n = df_prob_bi['bigram_pairs'][i].split(' ')\n",
    "        df_prob_bi.loc[i, 'First(i-1)'] = m\n",
    "        df_prob_bi.loc[i, 'Second(i)'] = n\n",
    "    # caculate row and column\n",
    "    m, n = df_prob_bi.shape\n",
    "    # change index into numercial value\n",
    "    df_prob_bi.index = np.arange(0, m)\n",
    "    #********************Build Bigram DataFrame***********************************\n",
    "    \n",
    "    \n",
    "    #*********************Cacualte Next Word****************************\n",
    "    #p(w_i | w_i-1) = C(w_i-1, w_i) / c(w_i-1)\n",
    "    for i in range(m):\n",
    "        c_bigram = df_prob_bi.loc[i,'probability']\n",
    "        #select the probabilty from unigram index == first word in bigram model\n",
    "        c_w_i_1 = df_prob_uni.loc[df_prob_uni.index == df_prob_bi.loc[i,'First(i-1)'], 'probability']\n",
    "        df_prob_bi.loc[i, 'joint_prob'] = float(c_bigram / c_w_i_1)\n",
    "    #     df_prob_bi.loc[i, 'joint_prob'] = 0\n",
    "    #*********************Cacualte Next Word****************************\n",
    "    \n",
    "    \n",
    "    #***********************Display Part*******************************\n",
    "    for i in range(30):\n",
    "        sentence = df_input.loc[i, 'cleaned']\n",
    "        word = sentence.split(' ')[-2]\n",
    "        #print(word)\n",
    "        next_prob = df_prob_bi.loc[df_prob_bi['First(i-1)']==word, 'joint_prob'].max()\n",
    "        #print(next_prob)\n",
    "    #     next_word = df_prob_bi.loc[df_prob_bi['joint_prob']==next_prob, 'Second(i)'].iloc[0]\n",
    "        try:\n",
    "            next_word = df_prob_bi.loc[df_prob_bi['joint_prob']==next_prob, 'Second(i)'].iloc[0]\n",
    "        except:\n",
    "            next_word = \"OUT OF VOCABULARY\"\n",
    "        print(f\"SENTCEN: {sentence}, PREDICT WORD: {next_word}\")\n",
    "    #***********************Display Part*******************************\n",
    "    \n",
    "    \n",
    "    return df_prob_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3165it [00:00, 3948.39it/s]\n",
      "3165it [00:00, 3402.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTCEN: but while the new york stock exchange did nt fall , PREDICT WORD: the\n",
      "SENTCEN: some circuit breakers installed after the october N crash failed , PREDICT WORD: to\n",
      "SENTCEN: the N stock specialist firms on the big board floor , PREDICT WORD: market\n",
      "SENTCEN: big investment banks refused to step up to the plate , PREDICT WORD: OUT OF VOCABULARY\n",
      "SENTCEN: heavy selling of standard  poor s stock index futures , PREDICT WORD: said\n",
      "SENTCEN: seven big board stocks ual amr bankamerica walt disney capital , PREDICT WORD: of\n",
      "SENTCEN: once again the specialists were not able to handle the , PREDICT WORD: unk\n",
      "SENTCEN: unk james unk chairman of specialists henderson brothers inc. it , PREDICT WORD: s\n",
      "SENTCEN: when the dollar is in a unk even central banks , PREDICT WORD: the\n",
      "SENTCEN: speculators are calling for a degree of liquidity that is , PREDICT WORD: the\n",
      "SENTCEN: many money managers and some traders had already left their , PREDICT WORD: the\n",
      "SENTCEN: then in a unk plunge the dow jones industrials in , PREDICT WORD: the\n",
      "SENTCEN: unk trading accelerated to N million shares a record for , PREDICT WORD: the\n",
      "SENTCEN: at the end of the day N million shares were , PREDICT WORD: nt\n",
      "SENTCEN: the dow s decline was second in point terms only , PREDICT WORD: N\n",
      "SENTCEN: in percentage terms however the dow s dive was the , PREDICT WORD: unk\n",
      "SENTCEN: shares of ual the parent of united airlines were extremely , PREDICT WORD: OUT OF VOCABULARY\n",
      "SENTCEN: wall street s takeoverstock speculators or risk arbitragers had placed , PREDICT WORD: market\n",
      "SENTCEN: at N p.m. edt came the unk news the big , PREDICT WORD: board\n",
      "SENTCEN: on the exchange floor as soon as ual stopped trading , PREDICT WORD: do\n",
      "SENTCEN: several traders could be seen shaking their heads when the , PREDICT WORD: unk\n",
      "SENTCEN: for weeks the market had been nervous about takeovers after , PREDICT WORD: be\n",
      "SENTCEN: and N minutes after the ual trading halt came news , PREDICT WORD: just\n",
      "SENTCEN: arbitragers could nt dump their ual stock but they rid , PREDICT WORD: for\n",
      "SENTCEN: for example their selling caused trading halts to be declared , PREDICT WORD: OUT OF VOCABULARY\n",
      "SENTCEN: but as panic spread speculators began to sell bluechip stocks , PREDICT WORD: are\n",
      "SENTCEN: when trading was halted in philip morris the stock was , PREDICT WORD: unk\n",
      "SENTCEN: selling unk because of waves of automatic stoploss orders which , PREDICT WORD: is\n",
      "SENTCEN: most of the stock selling pressure came from wall street , PREDICT WORD: N\n",
      "SENTCEN: traders said most of their major institutional investors on the , PREDICT WORD: unk\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_pairs</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>probability</th>\n",
       "      <th>First(i-1)</th>\n",
       "      <th>Second(i)</th>\n",
       "      <th>joint_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>of the</td>\n",
       "      <td>187</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>0.256686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N N</td>\n",
       "      <td>167</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.243673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the unk</td>\n",
       "      <td>167</td>\n",
       "      <td>0.005902</td>\n",
       "      <td>the</td>\n",
       "      <td>unk</td>\n",
       "      <td>0.085291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in the</td>\n",
       "      <td>147</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>0.261507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unk unk</td>\n",
       "      <td>134</td>\n",
       "      <td>0.004735</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>0.080884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17741</th>\n",
       "      <td>most competitive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>most</td>\n",
       "      <td>competitive</td>\n",
       "      <td>0.019170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17742</th>\n",
       "      <td>a rough</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>a</td>\n",
       "      <td>rough</td>\n",
       "      <td>0.001540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17743</th>\n",
       "      <td>will get</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>will</td>\n",
       "      <td>get</td>\n",
       "      <td>0.007668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17744</th>\n",
       "      <td>growing official</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>growing</td>\n",
       "      <td>official</td>\n",
       "      <td>0.123539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17745</th>\n",
       "      <td>the phoenix</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>the</td>\n",
       "      <td>phoenix</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17746 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bigram_pairs  value_counts  probability First(i-1)    Second(i)  \\\n",
       "0                of the           187     0.006608         of          the   \n",
       "1                   N N           167     0.005902          N            N   \n",
       "2               the unk           167     0.005902        the          unk   \n",
       "3                in the           147     0.005195         in          the   \n",
       "4               unk unk           134     0.004735        unk          unk   \n",
       "...                 ...           ...          ...        ...          ...   \n",
       "17741  most competitive             1     0.000035       most  competitive   \n",
       "17742           a rough             1     0.000035          a        rough   \n",
       "17743          will get             1     0.000035       will          get   \n",
       "17744  growing official             1     0.000035    growing     official   \n",
       "17745       the phoenix             1     0.000035        the      phoenix   \n",
       "\n",
       "       joint_prob  \n",
       "0        0.256686  \n",
       "1        0.243673  \n",
       "2        0.085291  \n",
       "3        0.261507  \n",
       "4        0.080884  \n",
       "...           ...  \n",
       "17741    0.019170  \n",
       "17742    0.001540  \n",
       "17743    0.007668  \n",
       "17744    0.123539  \n",
       "17745    0.000511  \n",
       "\n",
       "[17746 rows x 6 columns]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ngram(df_input, first_row = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.7 Kneser-Ney N-gram Prediction\"></a>\n",
    "## 3.7 Kneser-Ney N-gram Prediction\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random pick the 252th from corpus sequence\n",
      "Current vectorized sequnce is [205, 9, 456, 12, 1008, 2450, 6, 533, 3, 427]\n",
      "Converted back to string: quantum s lot is mostly tied to polyethylene unk used\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    path_input = \"03_data/13_a3-data/input.txt\" \n",
    "    path_train5k = \"03_data/13_a3-data/train.5k.txt\"\n",
    "    path_train = \"03_data/13_a3-data/train.txt\"\n",
    "    path_valid = \"03_data/13_a3-data/valid.txt\"\n",
    "    pre = preprocess()\n",
    "    df_input = pre.read_file(path_input)\n",
    "    df_train5k = pre.read_file(path_train5k)\n",
    "    df_train = pre.read_file(path_train)\n",
    "    df_valid = pre.read_file(path_valid)\n",
    "    seq, word_index,df = pre.token(df_input)\n",
    "    #df = ngram(seq, word_index, df, grams=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.RNN\"></a>\n",
    "# 4.RNN\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1 Initial Parameters\"></a>\n",
    "## 4.1 Initial Parameters\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max lengh of vocaubulary \n",
    "# vocab_size = 11\n",
    "#embedding layer dimension\n",
    "# embedding_dim = 32\n",
    "#unit of SimpleRNN\n",
    "#rnn_units = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2 Forward Pass\"></a>\n",
    "## 4.2 Forward Pass\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 11\n",
    "# embedding_dim = 32\n",
    "# rnn_units = 64\n",
    "# BATCH_SIZE = 16\n",
    "# model_1 = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model():\n",
    "    # before we input data, we need set up seq_length and padding our data\n",
    "    # seq_length should use plot to decide boundary\n",
    "\n",
    "    # ********************************************************\n",
    "    # for each input sequence, maybe we need right shift one word\n",
    "    # ********************************************************\n",
    "\n",
    "    seq_length =11\n",
    "    #input_layer1 = layers.Input(shape=seq_length)\n",
    "    # first layer is Embedding, output_idm is number of units/cells\n",
    "    emb_layer2 = layers.Embedding(input_dim = seq_length, output_dim = 32)(input_layer1)\n",
    "    # second layer is RNN\n",
    "    rnn_layer3 = layers.SimpleRNN(units = 32, activation='relu', return_sequences=True)(emb_layer2)\n",
    "    # rnn_layer2 = layers.GRU(units = 32, activation='relu')(input_layer1)\n",
    "    # only contain one hidden layer\n",
    "    output_layer4 = layers.Dense(units=seq_length)(rnn_layer3)\n",
    "    model_1 = Model(inputs = input_layer1, outputs = output_layer4)\n",
    "\n",
    "\n",
    "    model_1.summary()\n",
    "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    # model_rnn = \n",
    "    \n",
    "    return model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(total_words, max_sequence_len, emb_dim=1024):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # before we input data, we need set up total_words and padding our data\n",
    "    # total_words should use plot to decide boundary\n",
    "\n",
    "    # ********************************************************\n",
    "    # for each input sequence, maybe we need right shift one word\n",
    "    # ********************************************************\n",
    "\n",
    "    #total_words =11\n",
    "    #input_layer1 = layers.Input(shape=total_words)\n",
    "    # first layer is Embedding, output_idm is number of units/cells\n",
    "    emb_layer2 = layers.Embedding(input_dim = total_words, \\\n",
    "                                  output_dim = emb_dim, \\\n",
    "                                  input_length=max_sequence_len-1)\n",
    "    # second layer is RNN\n",
    "#     lstm_layer3 = layers.LSTM(units = 2048, activation='relu', return_sequences=True)(emb_layer2)\n",
    "    lstm_layer3 = layers.Bidirectional(layers.LSTM(2048))(emb_layer2)\n",
    "    # rnn_layer2 = layers.GRU(units = 32, activation='relu')(input_layer1)\n",
    "    # only contain one hidden layer\n",
    "    output_layer4 = layers.Dense(units=total_words)(lstm_layer3)\n",
    "    model = Model(inputs = emb_layer2, outputs = output_layer4)\n",
    "    adam = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "    # model_rnn = \n",
    "    # history = model.fit(xs, ys, epochs=10, verbos=1 )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_mod(total_words, max_sequence_len):\n",
    "    print(total_words)\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(150)))\n",
    "    model.add(layers.Dense(total_words, activation='softmax'))\n",
    "    adam = tf.keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    #earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9868"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9868\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 11, 100)           986800    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 300)               301200    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 9868)              2970268   \n",
      "=================================================================\n",
      "Total params: 4,258,268\n",
      "Trainable params: 4,258,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = lstm_mod(total_words, max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28337 samples\n",
      "Epoch 1/10\n",
      "28337/28337 [==============================] - 24s 857us/sample - loss: 6.4521 - accuracy: 0.1000\n",
      "Epoch 2/10\n",
      "28337/28337 [==============================] - 18s 648us/sample - loss: 5.5810 - accuracy: 0.1514\n",
      "Epoch 3/10\n",
      "28337/28337 [==============================] - 18s 653us/sample - loss: 4.8539 - accuracy: 0.1825\n",
      "Epoch 4/10\n",
      "28337/28337 [==============================] - 19s 654us/sample - loss: 4.1897 - accuracy: 0.2221\n",
      "Epoch 5/10\n",
      "28337/28337 [==============================] - 19s 670us/sample - loss: 3.6203 - accuracy: 0.2699- loss: 3.6192 - accuracy: 0.\n",
      "Epoch 6/10\n",
      "28337/28337 [==============================] - 19s 662us/sample - loss: 3.1579 - accuracy: 0.3242\n",
      "Epoch 7/10\n",
      "28337/28337 [==============================] - 19s 660us/sample - loss: 2.8197 - accuracy: 0.3682\n",
      "Epoch 8/10\n",
      "28337/28337 [==============================] - 18s 652us/sample - loss: 2.5343 - accuracy: 0.4109\n",
      "Epoch 9/10\n",
      "28337/28337 [==============================] - 19s 655us/sample - loss: 2.3248 - accuracy: 0.4516\n",
      "Epoch 10/10\n",
      "28337/28337 [==============================] - 19s 676us/sample - loss: 2.1388 - accuracy: 0.4872\n"
     ]
    }
   ],
   "source": [
    "# history = model_3.fit(xs, ys, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "\n",
    "# SVG(plot_model(model_1,dpi=60,show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))\n",
    "\n",
    "# dot_img_file = '04_images/14_model_1.png'\n",
    "# # plot_model(model_1, to_file=dot_img_file, show_shapes=True)\n",
    "# plot_model(model_1, to_file='04_images/15_model_96DPI.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3 Loss\"></a>\n",
    "## 4.3 Loss\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4 Traing Step\"></a>\n",
    "## 4.4 Traing Step\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random pick the 733th from corpus sequence\n",
      "Current vectorized sequnce is [7, 141, 14, 2, 33, 9, 77, 111, 12, 2690]\n",
      "Converted back to string: a spokesman said the company s first quarter is historically\n",
      "Random pick the 4844th from corpus sequence\n",
      "Current vectorized sequnce is [85, 43, 936, 1223, 209, 731, 305, 882, 3, 436, 23, 1106, 53, 5, 2, 3, 1092, 63, 92, 8, 936, 34, 8, 2, 589, 632]\n",
      "Converted back to string: last year pennsylvania supreme court justice john p unk called mr okicki one of the unk judges not only in pennsylvania but in the united states\n",
      "Random pick the 21347th from corpus sequence\n",
      "Current vectorized sequnce is [68, 2, 947, 516, 67, 926, 186, 4447, 22, 126]\n",
      "Converted back to string: if the situation changes i ll get stuck with them\n",
      "Random pick the 266th from corpus sequence\n",
      "Current vectorized sequnce is [45, 3, 12, 958, 3, 7, 3, 534, 19, 2458]\n",
      "Converted back to string: one unk is george unk a unk analyst at oppenheimer\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    path_input = \"03_data/13_a3-data/input.txt\" \n",
    "    path_train5k = \"03_data/13_a3-data/train.5k.txt\"\n",
    "    path_train = \"03_data/13_a3-data/train.txt\"\n",
    "    path_valid = \"03_data/13_a3-data/valid.txt\"\n",
    "    \n",
    "    #call Class preprocess()\n",
    "    pre = preprocess()\n",
    "    df_input = pre.read_file(path_input)\n",
    "    df_train5k = pre.read_file(path_train5k)\n",
    "    df_train = pre.read_file(path_train)\n",
    "    df_valid = pre.read_file(path_valid)\n",
    "    seq_input, word_index_input,df_input = pre.token(df_input)\n",
    "    seq_train5k, word_index_train5k,df_train5k = pre.token(df_train5k)\n",
    "    seq_train, word_index_train,df_train = pre.token(df_train)\n",
    "    seq_valid, word_index_valid,df_valid = pre.token(df_input)\n",
    "    #df = ngram(seq, word_index, df, grams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42068, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9868\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 79, 100)           986800    \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 300)               301200    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 9868)              2970268   \n",
      "=================================================================\n",
      "Total params: 4,258,268\n",
      "Trainable params: 4,258,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 42068 samples\n",
      "Epoch 1/30\n",
      "42068/42068 [==============================] - 69s 2ms/sample - loss: 6.4533 - accuracy: 0.1913\n",
      "Epoch 2/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 5.1368 - accuracy: 0.2625\n",
      "Epoch 3/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 4.3004 - accuracy: 0.2954\n",
      "Epoch 4/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 3.8990 - accuracy: 0.3176\n",
      "Epoch 5/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 3.2078 - accuracy: 0.3716\n",
      "Epoch 6/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 2.8467 - accuracy: 0.4115\n",
      "Epoch 7/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 2.5983 - accuracy: 0.4410\n",
      "Epoch 8/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 2.4368 - accuracy: 0.4655\n",
      "Epoch 9/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 2.3002 - accuracy: 0.4852\n",
      "Epoch 10/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 2.2079 - accuracy: 0.4987\n",
      "Epoch 11/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 2.1274 - accuracy: 0.5101\n",
      "Epoch 12/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 2.0492 - accuracy: 0.5227\n",
      "Epoch 13/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 2.0092 - accuracy: 0.5314\n",
      "Epoch 14/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 1.9503 - accuracy: 0.5425\n",
      "Epoch 15/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 1.9491 - accuracy: 0.5419\n",
      "Epoch 16/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 1.8931 - accuracy: 0.5496\n",
      "Epoch 17/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 1.8263 - accuracy: 0.5602\n",
      "Epoch 18/30\n",
      "42068/42068 [==============================] - 67s 2ms/sample - loss: 1.7915 - accuracy: 0.5711\n",
      "Epoch 19/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 1.7784 - accuracy: 0.5703\n",
      "Epoch 20/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.7612 - accuracy: 0.5724\n",
      "Epoch 21/30\n",
      "42068/42068 [==============================] - 66s 2ms/sample - loss: 1.7603 - accuracy: 0.5705\n",
      "Epoch 22/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.7395 - accuracy: 0.5783\n",
      "Epoch 23/30\n",
      "42068/42068 [==============================] - ETA: 0s - loss: 1.6657 - accuracy: 0.59 - 68s 2ms/sample - loss: 1.6660 - accuracy: 0.5912\n",
      "Epoch 24/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.6663 - accuracy: 0.5928s - los\n",
      "Epoch 25/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.6496 - accuracy: 0.5972\n",
      "Epoch 26/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.6281 - accuracy: 0.6015\n",
      "Epoch 27/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.6291 - accuracy: 0.6010\n",
      "Epoch 28/30\n",
      "42068/42068 [==============================] - 65s 2ms/sample - loss: 1.5998 - accuracy: 0.6036\n",
      "Epoch 29/30\n",
      "42068/42068 [==============================] - 72s 2ms/sample - loss: 1.5840 - accuracy: 0.6098\n",
      "Epoch 30/30\n",
      "42068/42068 [==============================] - 73s 2ms/sample - loss: 1.5564 - accuracy: 0.6145\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "#     # seq_length is the max length of these questions\n",
    "#     seq_length = 11\n",
    "#     # maintain the sentence into same length\n",
    "#     # padded = pad_sequences(sequences)\n",
    "#     # If you want zero at the end, using padding='post'\n",
    "#     padded = pad_sequences(seq, padding='post', maxlen=seq_length, truncating='post')\n",
    "\n",
    "    # ************************************Train Data*****************************\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['cleaned'])\n",
    "total_words = len(word_index_train) + 1\n",
    "\n",
    "# we will populate this with sub sequence with \n",
    "input_sequences = []\n",
    "for line in df_train['cleaned']:\n",
    "    #texts_to sequnce will create a list, so it will be list nested into another list. So we need [0] to extarct\n",
    "    #provide current token for current line according to tokenizer.fit_on_text() dictonary\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    token_list = [i+1 for i in token_list ]\n",
    "    #print(token_list)\n",
    "    for i in range(len(token_list)-1, len(token_list)):\n",
    "        #generate a sequence contain from first one to last one\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences, because we need last one as a non-zero real words, so we add zero in the front\n",
    "# we can see out very last word as Y and other, from first to end before Y as \"input_X\"\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "#print(max_sequence_len)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs_train, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "# Finally we want our our Y to be catigorical and become one-hot encoding\n",
    "# Here is a odd \"+1\", because I add 1 tiwice, first is to match index for <oov> missing, \n",
    "# second is len(tokenizer.word_index) = 4016, but len(word_index) = 4017\n",
    "ys_train = tf.keras.utils.to_categorical(labels, num_classes=total_words, dtype='uint64')\n",
    "\n",
    "# m = len(labels)\n",
    "# y_mapped=np.zeros((m, total_words))\n",
    "# for i in range(0,m):\n",
    "#     #print(i)\n",
    "#     y_mapped[i, labels[i]]=1;\n",
    "# ************************************Train Data*****************************\n",
    "\n",
    "\n",
    "model_3 = lstm_mod(total_words, max_sequence_len)\n",
    "history = model_3.fit(xs_train, ys_train, epochs=30,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# !mkdir -p saved_model_3\n",
    "model_3.save('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.save(\"my_h5_model_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3165/3165 [==============================] - 3s 985us/sample - loss: 18.7225 - accuracy: 0.0562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18.722465580221602, 0.056240126]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "# ************************************Test Data********************************\n",
    "tokenizer.fit_on_texts(df_train['cleaned'])\n",
    "total_words = len(word_index_train) + 1\n",
    "\n",
    "# we will populate this with sub sequence with \n",
    "input_sequences = []\n",
    "for line in df_valid['cleaned']:\n",
    "    #texts_to sequnce will create a list, so it will be list nested into another list. So we need [0] to extarct\n",
    "    #provide current token for current line according to tokenizer.fit_on_text() dictonary\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    token_list = [i+1 for i in token_list ]\n",
    "    #print(token_list),only extract the last full sequence\n",
    "    for i in range(len(token_list)-1, len(token_list)):\n",
    "        #generate a sequence contain from first one to last one\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences, because we need last one as a non-zero real words, so we add zero in the front\n",
    "# we can see out very last word as Y and other, from first to end before Y as \"input_X\"\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "#print(max_sequence_len)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=80, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs_test, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "# Finally we want our our Y to be catigorical and become one-hot encoding\n",
    "# Here is a odd \"+1\", because I add 1 tiwice, first is to match index for <oov> missing, \n",
    "# second is len(tokenizer.word_index) = 4016, but len(word_index) = 4017\n",
    "ys_test = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_3.evaluate(xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.title('Training  accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.title('Training  loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.5 Perplexity\"></a>\n",
    "## 4.5 Perplexity\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_perp(y_true, y_pred):\n",
    "    CE = K.categorical_crossentropy(y_true, y_pred)\n",
    "    perplexity = K.exp(K.mean(CE))\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss \n",
    "def np_perp(y_true, y_pred):\n",
    "    CE = log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
    "    perplexity = np.exp(np.mean(CE))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6 Predict Words\"></a>\n",
    "## 4.6 Predict Words\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict setence is but while the new york stock exchange did nt fall  [longest]\n",
      "predict setence is some circuit breakers installed after the october N crash failed  [remember]\n",
      "predict setence is the N stock specialist firms on the big board floor  [of]\n",
      "predict setence is big investment banks refused to step up to the plate  [n]\n",
      "predict setence is heavy selling of standard  poor s stock index futures  [n]\n",
      "predict setence is seven big board stocks ual amr bankamerica walt disney capital  [july]\n",
      "predict setence is once again the specialists were not able to handle the  [spent]\n",
      "predict setence is unk james unk chairman of specialists henderson brothers inc. it  [significant]\n",
      "predict setence is when the dollar is in a unk even central banks  [n]\n",
      "predict setence is speculators are calling for a degree of liquidity that is  [also]\n",
      "predict setence is many money managers and some traders had already left their  [defense]\n",
      "predict setence is then in a unk plunge the dow jones industrials in  [lawsuits]\n",
      "predict setence is unk trading accelerated to N million shares a record for  [athletic]\n",
      "predict setence is at the end of the day N million shares were  [selling]\n",
      "predict setence is the dow s decline was second in point terms only  [lines]\n",
      "predict setence is in percentage terms however the dow s dive was the  [on]\n",
      "predict setence is shares of ual the parent of united airlines were extremely  [by]\n",
      "predict setence is wall street s takeoverstock speculators or risk arbitragers had placed  [on]\n",
      "predict setence is at N p.m. edt came the unk news the big  [revenue]\n",
      "predict setence is on the exchange floor as soon as ual stopped trading  [n]\n",
      "predict setence is several traders could be seen shaking their heads when the  [thing]\n",
      "predict setence is for weeks the market had been nervous about takeovers after  [n]\n",
      "predict setence is and N minutes after the ual trading halt came news  [jones]\n",
      "predict setence is arbitragers could nt dump their ual stock but they rid  [committee]\n",
      "predict setence is for example their selling caused trading halts to be declared  [of]\n",
      "predict setence is but as panic spread speculators began to sell bluechip stocks  [of]\n",
      "predict setence is when trading was halted in philip morris the stock was  [n]\n",
      "predict setence is selling unk because of waves of automatic stoploss orders which  [much]\n",
      "predict setence is most of the stock selling pressure came from wall street  [n]\n",
      "predict setence is traders said most of their major institutional investors on the  [n]\n"
     ]
    }
   ],
   "source": [
    "for line in df_input['cleaned'][0:30]:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=79, padding='pre')\n",
    "    predicted = model_3.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    line += \" \" + \"[\" + output_word + \"]\"\n",
    "    print(f\"predict setence is {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.Unused Code\"></a>\n",
    "# 5.Unused Code\n",
    "<a href=\"#1.Summary\">Click this Link back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in seq[50]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((a, b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(mydict.keys())[list(mydict.values()).index(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word_index.keys())[list(word_index.values()).index(156)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in word_index.items():\n",
    "    if idx == 156:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list=[]\n",
    "for i in seq:\n",
    "    len_list.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(len_list)),len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"I've got a bad feeling about this\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n"
     ]
    }
   ],
   "source": [
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    " \n",
    "data = 'A class is a blueprint for the object.'\n",
    " \n",
    "print(\"1-gram: \", extract_ngrams(data, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance  \n",
    "for sentence in df_input:\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    " \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sentence: 0     but while the new york stock exchange did nt f...\n",
      "1     some circuit breakers installed after the octo...\n",
      "2     the N stock specialist firms on the big board ...\n",
      "3     big investment banks refused to step up to the...\n",
      "4     heavy selling of standard  poor s stock index ...\n",
      "5     seven big board stocks ual amr bankamerica wal...\n",
      "6     once again the specialists were not able to ha...\n",
      "7     unk james unk chairman of specialists henderso...\n",
      "8       when the dollar is in a unk even central banks \n",
      "9     speculators are calling for a degree of liquid...\n",
      "10    many money managers and some traders had alrea...\n",
      "11    then in a unk plunge the dow jones industrials...\n",
      "12    unk trading accelerated to N million shares a ...\n",
      "13         at the end of the day N million shares were \n",
      "14    the dow s decline was second in point terms only \n",
      "15    in percentage terms however the dow s dive was...\n",
      "16    shares of ual the parent of united airlines we...\n",
      "17    wall street s takeoverstock speculators or ris...\n",
      "18             at N p.m. edt came the unk news the big \n",
      "19    on the exchange floor as soon as ual stopped t...\n",
      "20    several traders could be seen shaking their he...\n",
      "21    for weeks the market had been nervous about ta...\n",
      "22    and N minutes after the ual trading halt came ...\n",
      "23    arbitragers could nt dump their ual stock but ...\n",
      "24    for example their selling caused trading halts...\n",
      "25    but as panic spread speculators began to sell ...\n",
      "26    when trading was halted in philip morris the s...\n",
      "27    selling unk because of waves of automatic stop...\n",
      "28    most of the stock selling pressure came from w...\n",
      "29    traders said most of their major institutional...\n",
      "Name: cleaned, dtype: object\n",
      "2-gram sentence: \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-b361fd917d23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2-gram sentence: \\\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mget2GramSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-122-b361fd917d23>\u001b[0m in \u001b[0;36mget2GramSentence\u001b[1;34m(sentence, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget2GramSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Find Next word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgram2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xrange' is not defined"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(sentence, n = 1):\n",
    "    for i in range(n):\n",
    "        print(sentence)\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "\n",
    "word = df_input[\"cleaned\"][0:30]\n",
    "print(\"Start sentence: %s\" % word)\n",
    "\n",
    "print(\"2-gram sentence: \\\"\",)\n",
    "get2GramSentence(word, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
