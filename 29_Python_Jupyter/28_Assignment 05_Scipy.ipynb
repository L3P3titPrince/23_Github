{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.We have 158945 tokens\n",
      "2.We have 18198 verbs\n",
      "3.The most frequnecy name entity is: [('Elizabeth', 554)]\n",
      "4.We have 7156 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████                                                                    | 951/6827 [00:39<04:00, 24.38it/s]d:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "d:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▊| 6810/6827 [05:15<00:01, 16.41it/s]d:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▊| 6812/6827 [05:15<00:00, 15.91it/s]d:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 6824/6827 [05:16<00:00, 16.66it/s]d:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 6826/6827 [05:16<00:00, 14.96it/s]d:\\ProgramData\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \"__main__\", mod_spec)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 6827/6827 [05:16<00:00, 21.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Of all the sentences in the text that are at least 10 words in length, 1658 and 1659are most similar sim_value                                             0.980834\n",
      "sent_1       His being such a charming young\\r\\n      man, ...\n",
      "sent1_pos                                                 1658\n",
      "sent_2       It was, moreover, such a promising\\r\\n      th...\n",
      "sent2_pos                                                 1659\n",
      "Name: 1658, dtype: object\n",
      "1.We have 158945 tokens\n",
      "2.We have 18198 verbs\n",
      "3.The most frequnecy name entity is: [('Elizabeth', 554)]\n",
      "4.We have 7156 sentences\n",
      "5. Of all the sentences in the text that are at least 10 words in length, 1658 and 1659are most similar sim_value                                             0.980834\n",
      "sent_1       His being such a charming young\\r\\n      man, ...\n",
      "sent1_pos                                                 1658\n",
      "sent_2       It was, moreover, such a promising\\r\\n      th...\n",
      "sent2_pos                                                 1659\n",
      "Name: 1658, dtype: object\n",
      "6. the vector representation of the first word in the 15th sentence in the document is  4.718869\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#spacy is used for NLP to extract token and entity\n",
    "import spacy\n",
    "#beautifu soup is used for extract text in this mission\n",
    "import bs4\n",
    "import pandas as pd\n",
    "\n",
    "#Counter is very easy to get numbe of something\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy import displacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "#thi pre-process function is ues to delete some none neccssary words to increase accuracy for similiary function\n",
    "def process_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.text in nlp.Defaults.stop_words:     \n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            continue\n",
    "        result.append(token.lemma_)       # only get core part of a sentence and ignore stop word and punctuation \n",
    "    return \" \".join(result)\n",
    "\n",
    "#I define a function to calcuate similarity and return similairy value\n",
    "def calculate_similarity(text1, text2):\n",
    "    base = nlp(process_text(text1))\n",
    "    compare = nlp(process_text(text2))\n",
    "    return base.similarity(compare)\n",
    "\n",
    "# this def function is used to order 10 max number in a list\n",
    "def Nmaxelements(list1, N): \n",
    "    final_list = [] \n",
    "  \n",
    "    for i in range(0, N):  \n",
    "        max1 = 0\n",
    "          \n",
    "        for j in range(len(list1)):      \n",
    "            if list1[j] > max1: \n",
    "                max1 = list1[j]; \n",
    "                  \n",
    "        list1.remove(max1); \n",
    "        final_list.append(max1) \n",
    "          \n",
    "    print(final_list) \n",
    "\n",
    "#input Large english corpse library\n",
    "nlp=spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "#request part, used to extract text from target\n",
    "url_1 = 'https://www.gutenberg.org/files/1342/1342-h/1342-h.htm'\n",
    "r_1 = requests.get(url_1)\n",
    "\n",
    "soup_1=bs4.BeautifulSoup(r_1.content,'lxml')\n",
    "\n",
    "#print(soup_1.get_text())\n",
    "\n",
    "#this is for apply nlp parese on webiste text result\n",
    "doc = nlp(soup_1.get_text())\n",
    "\n",
    "#this for count token and verb in doc\n",
    "token_list = []\n",
    "token_count = 0\n",
    "token_verb_count =0\n",
    "for token in doc:\n",
    "    token_count +=1\n",
    "    token_list.append([token.text, token.pos_])\n",
    "    if token.pos_=='VERB':\n",
    "        token_verb_count += 1\n",
    "        \n",
    "\n",
    "print(\"1.We have {} tokens\".format(token_count))\n",
    "print(\"2.We have {} verbs\".format(token_verb_count))\n",
    "\n",
    "#try to find each part number\n",
    "#labels = [x.label_ for x in doc.ents]\n",
    "#Counter(labels)\n",
    "\n",
    "#find the most frequency entity and count number\n",
    "items = [x.text for x in doc.ents]\n",
    "print(\"3.The most frequnecy name entity is:\",Counter(items).most_common(1))\n",
    "\n",
    "#get sents for count\n",
    "sents_count = 0\n",
    "for sentences in doc.sents:\n",
    "    sents_count += 1\n",
    "print(\"4.We have {} sentences\".format(sents_count))  \n",
    "\n",
    "\n",
    "sent_df = pd.DataFrame(columns=['sent_text'])\n",
    "sent_list=[]\n",
    "for sent1 in doc.sents:\n",
    "    if len(sent1.text)>10:\n",
    "        sent_df = sent_df.append({'sent_text': sent1.text}, ignore_index=True)\n",
    "        #sent_list.append(sent1.text)\n",
    "sent_df = sent_df.iloc[100:,]\n",
    "\n",
    "result_df = pd.DataFrame(columns=['sim_value','sent_1','sent1_pos','sent_2','sent2_pos'])\n",
    "for i in tqdm(range(0,len(sent_df)-1)):\n",
    "    v = nlp(sent_df.iloc[i,0]).similarity(nlp(sent_df.iloc[i+1,0]))\n",
    "    result_df = result_df.append({'sim_value': v,\n",
    "                                  'sent_1':sent_df.iloc[i,0],\n",
    "                                  'sent1_pos': i,\n",
    "                                  'sent_2':sent_df.iloc[i+1,0],\n",
    "                                  'sent2_pos':i+1}, ignore_index=True)\n",
    "    \n",
    "print(\"5. Of all the sentences in the text that are at least 10 words in length, 1658 and 1659are most similar\", result_df.loc[result_df['sim_value'].idxmax()])\n",
    "#print(\"We have {} sentences that have more than 10 words.\".format(len(sent_list)))\n",
    "\n",
    "sent_15 = nlp(sent_df.iloc[14,0])\n",
    "\n",
    "print(\"1.We have {} tokens\".format(token_count))\n",
    "print(\"2.We have {} verbs\".format(token_verb_count))\n",
    "print(\"3.The most frequnecy name entity is:\",Counter(items).most_common(1))\n",
    "print(\"4.We have {} sentences\".format(sents_count))  \n",
    "print(\"5. Of all the sentences in the text that are at least 10 words in length, 1658 and 1659are most similar\", result_df.loc[result_df['sim_value'].idxmax()])\n",
    "\n",
    "for token in sent_15:\n",
    "    print(\"6. the vector representation of the first word in the 15th sentence in the document is \",token.vector_norm)\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
