{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Extract data using regular expression (2 points)\n",
    "Suppose you have scraped the text shown below from an online source. \n",
    "Define function `extract(text)` which:\n",
    "- takes a piece of text (in the format of shown below) as an input\n",
    "- extracts data into a list of tuples using regular expression, e.g.  [('Consumer Price Index', '+0.2%', 'Sep 2020'), ...]\n",
    "- returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''Consumer Price Index:\n",
    "        +0.2% in Sep 2020\n",
    "\n",
    "        Unemployment Rate:\n",
    "        +7.9% in Sep 2020\n",
    "\n",
    "        Producer Price Index:\n",
    "        +0.4% in Sep 2020\n",
    "\n",
    "        Employment Cost Index:\n",
    "        +0.5% in 2nd Qtr of 2020\n",
    "\n",
    "        Productivity:\n",
    "        +10.1% in 2nd Qtr of 2020\n",
    "\n",
    "        Import Price Index:\n",
    "        +0.3% in Sep 2020\n",
    "\n",
    "        Export Price Index:\n",
    "        +0.6% in Sep 2020'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Develop a QA system for COVID-19 (8 points)\n",
    "\n",
    "A curated COVID-19 Question and Answer (QA) dataset has been provided. Now you are required to develop a QA system, which can search for the best answers to any question related to COVID-19. The dataset looks like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can I get COVID-19 from animals when travellin...</td>\n",
       "      <td>Although the current spread and growth of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I protect myself and others?</td>\n",
       "      <td>The best way to prevent illness from COVID-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did COVID-19 come from?</td>\n",
       "      <td>It was first found in Wuhan City, Hubei Provin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can my pet or other animals get sick from COVI...</td>\n",
       "      <td>There is currently no evidence to suggest that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I protect my child from COVID-19?</td>\n",
       "      <td>By having them practice the same things you ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Can I get COVID-19 from animals when travellin...   \n",
       "1               How can I protect myself and others?   \n",
       "2                      Where did COVID-19 come from?   \n",
       "3  Can my pet or other animals get sick from COVI...   \n",
       "4          How can I protect my child from COVID-19?   \n",
       "\n",
       "                                              answer  \n",
       "0  Although the current spread and growth of the ...  \n",
       "1  The best way to prevent illness from COVID-19 ...  \n",
       "2  It was first found in Wuhan City, Hubei Provin...  \n",
       "3  There is currently no evidence to suggest that...  \n",
       "4  By having them practice the same things you ha...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"03_data/03_covid_qa.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1.** Define a function `tokenize(doc, lemmatized = True, stopword = True, punctuation = True)`  as follows:\n",
    "   - Take three parameters: \n",
    "       - `doc`: an input string (e.g. a question)\n",
    "       - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
    "       - `stopword`: an optional bookean parameter to keep stop words. The default value is True (i.e. keep stop words). \n",
    "       - `punctuation`: optional bookean parameter to keep punctuations. The default values is True (i.e. keep all punctuations)\n",
    "   - Split the input text into unigrams and also clean up tokens as follows:\n",
    "       - if `lemmatized` is turned on, lemmatize all unigrams.\n",
    "       - if `stopword` is set to False, remove all stop words.\n",
    "       - if `punctuation` is set to False, remove all punctuations.\n",
    "   - Convert all unigrams to the lower case and remove empty ones\n",
    "   - Return the list of unigrams after all the processing. (Hint: you can use spacy package for this task. For reference, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2.** Define a function `compute_tf_idf(docs, lemmatized = True, stopword = True, punctuation = True)` as follows: \n",
    "- Take the following inputs:\n",
    "    - `docs`: a corpus consisting of a list of strings (e.g. all questions)\n",
    "    - `lemmatized, stopword, punctuation`:  similar to those defined in Q2.1\n",
    "- Tokenize each string in `docs` using the function defined in Q2.1. Note, you need to pass the value of `lemmatized, stopword, punctuation` to `tokenize` function.\n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: you can reuse the last code segment in NLP Lecture Notes (II))\n",
    "- Return the following three variables:\n",
    "    - a smoothed normalized `tf_idf` array, \n",
    "    - the list of `words` (i.e. unigrams) in the vocabulary of the corpus, and \n",
    "    - inverse document frequency (`idf`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3.** Define a function `vectorize_doc(doc, words, idf, lemmatized = True, stopword = True, punctuation = True)`  to calculate the tf_idf weights for a document, as follows: \n",
    "- Take four inputs:\n",
    "   - `doc`: a new document (e.g. a new question)\n",
    "   - `words`: the list of words from the corpus (i.e. the return from Q2.2)\n",
    "   - `idf`: inverse document frequency from the corpus (i.e. the return from Q2.2)\n",
    "   - `lemmatized, stopword, punctuation`:  similar to those defined in Q2.1\n",
    "- Tokenize `doc` using the `tokenize` function as defined in Q2.1.\n",
    "- Compute the term frequency of the document\n",
    "- Calculate the smoothed normalized tf_idf weights for the single document\n",
    "- Return the tf_idf weight vector, which should have the same shape as `idf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.4.** Define a function `find_answer(doc_vect, tf_idf, docs)` as follows: \n",
    "\n",
    "   - Take three inputs: \n",
    "      - `doc_vect`: A tf_idf weight vector for a new question. This is the return from Q2.3. \n",
    "      - `tf_idf`: A tf_idf array. This is a return from Q2.2\n",
    "      - `docs`: the set of documents from which `tf_idf` was created. Note, if there are `m` documents, `n` words, the shape of `tf_idf` is `(m,n)`, the shape of `doc_vect` should be `(n,)`. \n",
    "   - Caluclate the cosine similarity between `doc_vect` and `tf_idf`. This returns a vector of `(m,)`, indicating the similarities between the question and each document in `docs`\n",
    "   - Find the indexes of the top-3 similarities \n",
    "   - Return the documents corresponding to these indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.5. Test**: \n",
    "- For better match, you can concate a pair of question and answer as a single document (see code below)\n",
    "- Test your solution using different options in in the tokenize function, i.e. with or without lemmatization, with or without removing stop words/punctuation, to see how these options may affect the accuracy of answers. \n",
    "- Test your solution with different questions to see how effective your system is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 (bonus): Analysis\n",
    "\n",
    "Perhaps this could be your first QA system. If you try this system with different questions, you may notice this solution is not perfect and it may not find the right answers to some questions. \n",
    "- Please summarize `three drawbacks` you observed about this QA system . Use examples to illustrate each of the drawbacks.\n",
    "- Research to find possible solutions to each of the drawbacks\n",
    "- You do not need to implement these solutions. Just explain them  conceptually.\n",
    "- This is an open-ended question. Just show your observation and thinking here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "#import spacy\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#nltk.download()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.1\n",
    "\n",
    "def extract(text):\n",
    "    \n",
    "    a = re.findall('(\\w[a-zA-Z ]+)\\:\\s+(\\+\\d+.\\d\\%)\\sin\\s([\\w ]+)', text)\n",
    "       \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1\n",
    "\n",
    "def tokenize(doc, lemmatized=True, stopword=True, punctuation = True):\n",
    "    \"\"\"\n",
    "    Argus:\n",
    "    ------\n",
    "    doc:string\n",
    "        input is a sentence, question or answer\n",
    "    lemmatized:\n",
    "        If True, token will be lemmatized.\n",
    "        \n",
    "    Returns:\n",
    "     -------\n",
    "    tokens:list\n",
    "    \"\"\"\n",
    "    # tokenzie word (actually split word and puncuation by space) and lowercase\n",
    "    tokens = nltk.word_tokenize(doc.lower())\n",
    "    # tag each tokenized word\n",
    "  \n",
    "    # False = remove stopword  ||   True = keep stopword\n",
    "    if stopword == False:\n",
    "        # using english stopwords\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        # stop_words +=[\"covid\"]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    # False=remove punctuation,   ||   True=keep punctuation\n",
    "    if punctuation == False:\n",
    "        tokens = [token.strip(string.punctuation) for token in tokens]\n",
    "        # remove empty tokens\n",
    "        tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # lemmatized\n",
    "    if lemmatized == True:\n",
    "        # set WordNetLemmatirzer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # create word contains\n",
    "        word_list = []\n",
    "        for i in tokens:\n",
    "            # I only can extra lemmatize without specifty tag\n",
    "            # this part i can improve, use different tag to get different lemmaitze word, for now, i simply provide VERB as defualt \n",
    "            word = wordnet_lemmatizer.lemmatize(i,wordnet.VERB)\n",
    "            word_list.append(word)\n",
    "        tokens = word_list\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "                       \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.2\n",
    "def compute_tfidf(docs, lemmatized=True, stopword=True, punctuation=True):\n",
    "    \"\"\"\n",
    "    Argus:\n",
    "    ------\n",
    "    docs:list\n",
    "        input is not string. It's a list from list(df['question'])\n",
    "        \n",
    "    Return:\n",
    "    ------\n",
    "    smoothed_tf_idf:\n",
    "    \n",
    "    \"\"\"\n",
    "    # in order to prevent reuse, empty these variables\n",
    "    smoothed_tf_idf, smoothed_idf, words = None, None, None\n",
    "    \n",
    "    # STEP 1. get preprocessed tokens of each document into list\n",
    "    token_list = []\n",
    "    # our function tokenize only process string level, out docs are corpus(multi-sentences) level,\n",
    "    # so we need iterate to individual sencent(sting)\n",
    "    for i in docs:\n",
    "        # i is each question individually, default parameters is enable lemmatized but don't delete stopwords and punctuation\n",
    "        # In test(), we enable lemmatized, don't delete stopwords and delete punctuation\n",
    "        token = tokenize(i, lemmatized=lemmatized, stopword=stopword, punctuation=punctuation)\n",
    "        # add list into a big list with nested structure\n",
    "        token_list.append(token)\n",
    "        # add new tokens into end of old list and integrate into a big corpus\n",
    "#         token_list.extend(token)\n",
    "    #create token count dictionary, we need input a list corpus\n",
    "#     token_count = nltk.FreqDist(token_list)\n",
    "    \n",
    "    # STEP 2. process all documents a dictonary of dictionaries\n",
    "    # idx is the order of each token, FreqDist will be the occurance count number for each token in this single sentence\n",
    "    # we built a dictionary, key is the index of this word (vlaue) in this sentence (string),\n",
    "    # value is another dictionary contain frequence of indivude words in this sentence\n",
    "    tokens_docs = {idx:nltk.FreqDist(token) for idx,token in enumerate(token_list)}\n",
    "    \n",
    "    # STEP 3.get document-term matrix, \n",
    "    # constrcut a document-term matrix where each row is a doc, each column is a token and the value is the ferquency ot the token\n",
    "    # here has a very important merge dupulicate mechanism. If we use orient='index', each dict pair {key:value} will fill DataFrame\n",
    "    # column with key, fill value into cooresponding row index. \n",
    "    # for instance, if index =0 : {oil:1}, this 1 will be put into slot (0,0) if the oil is the first word in column\n",
    "    # next step, we got a index=1:{oil:2}, this 2 will be put into slot (1,0) if the oil is the first word in column\n",
    "    # this mechanism wil elimate duplication can record the frequence of each word in each sentence\n",
    "    df_corpus = pd.DataFrame.from_dict(tokens_docs, orient='index')\n",
    "    # fill np.Nan (beacuase the didn't appear in this sentence) with 0\n",
    "    df_corpus = df_corpus.fillna(0)\n",
    "    \n",
    "    # sort the index will help you match the right sentence with its correct sentence length for caculation\n",
    "    # sort by index (i.e. doc id)\n",
    "    df_corpus = df_corpus.sort_index(axis=0)\n",
    "    \n",
    "    # STEP 4.get normalized term frequency (tf) matrix\n",
    "    # convert df_corpus to numpy arrays\n",
    "    tf = df_corpus.values\n",
    "    # sum the value of each row, this is the length of each sentcent in our question, will be the denominator of tf caculation\n",
    "    doc_len = tf.sum(axis=1)\n",
    "    # divide df_corpus matrix by the doc length matrix\n",
    "    # tf will have a length of each sentence (different for each sentence)\n",
    "    tf = np.divide(tf, doc_len[:,None])\n",
    "    # set float precision to print nicely, only reserve 3 deciaml places\n",
    "#     np.set_printoptions(precision=3)\n",
    "    \n",
    "    # STEP 5.get idf\n",
    "    # get document frequent, if tf.value>1, then this slot is 1, otherwise, this slot is 0, this is used to count appearence\n",
    "    # df_corpus is the frequnce of words, df_idf is the occurance of each words\n",
    "    df_idf = np.where(tf>0, 1, 0)\n",
    "    # get idf. df_idf represent the appear count for each words. If we sum then by column and get a row result.\n",
    "    # so row result will be the occurance count for each word in all document\n",
    "    # plus 1 is to avoide log0\n",
    "    # numerator is length of each sentence, denominator is the occcurance count of each token(words).\n",
    "    # idf is corpus level, so this idf will be length = (unique word number)\n",
    "    idf = np.log(np.divide(len(token_list), np.sum(df_idf, axis=0)))+1\n",
    "    # in order to get more smooth result we add 1 in each step\n",
    "    smoothed_idf = np.log(np.divide(len(token_list)+1, np.sum(df_idf,axis=0)+1))+1\n",
    "    \n",
    "    # STEP 6.get tf-idf\n",
    "    # tf here is a (m,n) matrix, m is the number of exmaple/sentence, n is the number of unique words inheried from df_courps\n",
    "    # idf is a (n,) vector, n is the number of unique word inheried from df_idf(extract from tf)\n",
    "    # so tf*idf will be a element-wise multilication, result will be (m,n) matrix\n",
    "    tf_idf = normalize(tf*idf)\n",
    "    smoothed_tf_idf = normalize(tf*smoothed_idf)\n",
    "    \n",
    "    # for better visulization, make a tf-idf arrray dataframe\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    #\n",
    "    df_smooth = pd.DataFrame(smoothed_tf_idf, columns = df_corpus.columns)\n",
    "    \n",
    "    # smoothed_tf_idf = tf_idf weights\n",
    "    # smoothed_idf represent: how important each word in the corpus\n",
    "    # list(df_corpus.columns) = words = vocabulary = feature sets\n",
    "    return smoothed_tf_idf, smoothed_idf, list(df_corpus.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.3\n",
    "\n",
    "def vectorize_doc(doc, words, idf, lemmatized=True, stopword=True, punctuation = True):\n",
    "    \"\"\"\n",
    "    Argus:\n",
    "    ------\n",
    "    doc:string:\n",
    "        a new sentence\n",
    "        \n",
    "    words:list\n",
    "        unique words from former courps\n",
    "        \n",
    "    idf:vector\n",
    "        lenght is the unquice number of words from former corpus\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vect:\n",
    "        \n",
    "    \"\"\"\n",
    "    # add your code here\n",
    "    vect = None\n",
    "    # tokenize doc using the tokenize() function \n",
    "    token = tokenize(doc, lemmatized=lemmatized, stopword=stopword, punctuation=punctuation)\n",
    "    # compute the term frequency to the document \n",
    "    # STEP 2. process all documents a dictonary of dictionaries\n",
    "    # idx is the order of each token, FreqDist will be the occurance count number for each token in this single sentence\n",
    "#     tokens_docs = {idx:nltk.FreqDist(word) for idx,word in enumerate(token)}\n",
    "    token_count = nltk.FreqDist(token)\n",
    "    \n",
    "    # STEP 3.get document-term matrix, \n",
    "    # constrcut a document-term matrix where each row is a doc, each column is a token and the value is the ferquency ot the token\n",
    "    # create a empty vectors, reshape its dimention to (1,m) so we can create a one row dataframe\n",
    "    token_zero = np.zeros(len(words)).reshape(1,len(words))\n",
    "    # cerate a empty one row vector\n",
    "    df_corpus = pd.DataFrame(token_zero,columns=words)\n",
    "    # check every words in df_corpus.columns, which is actually words\n",
    "    for idx, word in enumerate(df_corpus.columns):\n",
    "        if list(token_count)[idx] in df_corpus.columns:\n",
    "#             print(list(token_count)[idx])\n",
    "            df_corpus[list(token_count)[idx]]=token_count[ list(token_count)[idx] ]\n",
    "        else:\n",
    "            break\n",
    "#     # fill np.Nan (beacuase the didn't appear in this sentence) with 0\n",
    "#     df_corpus = df_corpus.fillna(0)\n",
    "    \n",
    "    # STEP 4.get normalized term frequency (tf) matrix\n",
    "    # convert df_corpus to numpy arrays\n",
    "    tf = df_corpus.values\n",
    "    # sum the value of each row\n",
    "    doc_len = tf.sum(axis=1)\n",
    "    # divide df_corpus matrix by the doc length matrix\n",
    "    tf = np.divide(tf, doc_len[:,None])\n",
    "    # set float precision to print nicely\n",
    "    np.set_printoptions(precision=3)\n",
    "    \n",
    "#     # STEP 5.get idf\n",
    "#     # get document frequent, if tf.value>1, then this slot is 1, otherwise, this slot is 0, this is used to count appearence\n",
    "#     df_idf = np.where(tf>0, 1, 0)\n",
    "#     # get idf. df_idf represent the appear count for each words. If we sum then by column and get a row result.\n",
    "#     # so row result will be the occurance count for each word in all document\n",
    "#     # plus 1 is to prevent log0\n",
    "#     idf = np.log(np.divide(len(token_list), np.sum(df_idf, axis=0)))+1\n",
    "#     # in order to get more smooth result we add 1 in each step\n",
    "#     smoothed_idf = np.log(np.divide(len(token_list)+1, np.sum(df_idf,axis=0)+1))+1\n",
    "    \n",
    "#     # STEP 6.get tf-idf\n",
    "#     tf_idf = normalize(tf*idf)\n",
    "    smoothed_tf_idf = normalize(tf*idf)\n",
    "    # transorm shape from (1,1176) to (1176,1)\n",
    "    vect = smoothed_tf_idf.reshape(len(words),)\n",
    "\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = 'Is it safe to travel by plane?'  # What kind of masks should I use?\n",
    "# vect= vectorize_doc(doc, words, idf, lemmatized=True, stopword=True, punctuation =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([(words[idx], i) for idx, i in enumerate(vect) if i>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.4\n",
    "\n",
    "def find_answer(doc_vect, tf_idf, docs):\n",
    "    \"\"\"\n",
    "    top_docs  = []\n",
    "    doc_vect: A tf_idf weight vector for a new question. This is the return from Q2.3.\n",
    "    tf_idf: A tf_idf array. This is a return from Q2.2\n",
    "    docs: the set of documents from which tf_idf was created. Note, if there are m documents, n words, \n",
    "    the shape of tf_idf is (m,n), the shape of doc_vect should be (n,).\n",
    "    \n",
    "    \"\"\"\n",
    "    # add your code here\n",
    "#     Caluclate the cosine similarity between doc_vect and tf_idf. This returns a vector of (m,), indicating the similarities between the question and each document in docs\n",
    "#     Find the indexes of the top-3 similarities\n",
    "#     Return the documents corresponding to these indexes\n",
    "    cos_list = []\n",
    "    for idx,i in enumerate(tf_idf):\n",
    "        cos = np.dot(vect, i)/(np.linalg.norm(vect)*np.linalg.norm(i))\n",
    "        cos_list.append(cos)\n",
    "    cos_vect = pd.DataFrame(cos_list)\n",
    "    top_index = cos_vect[0].nlargest(3).index\n",
    "    top_docs = docs[top_index]\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "\n",
      "Test Q1\n",
      "[('Consumer Price Index', '+0.2%', 'Sep 2020'), ('Unemployment Rate', '+7.9%', 'Sep 2020'), ('Producer Price Index', '+0.4%', 'Sep 2020'), ('Employment Cost Index', '+0.5%', '2nd Qtr of 2020'), ('Productivity', '+10.1%', '2nd Qtr of 2020'), ('Import Price Index', '+0.3%', 'Sep 2020'), ('Export Price Index', '+0.6%', 'Sep 2020')]\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2.1 - Try different parameter values to make sure all options work\n",
      "\n",
      "===Lemmatize words, keep stop words/punctuations===\n",
      "\n",
      "['can', 'i', 'get', 'covid-19', 'from', 'animals', 'when', 'travel', 'to', 'other', 'countries', '?', 'although', 'the', 'current', 'spread', 'and', 'growth', 'of', 'the', 'covid-19', 'outbreak', 'be', 'primarily', 'associate', 'with', 'spread', 'from', 'person', 'to', 'person', ',', 'experts', 'agree', 'that', 'the', 'virus', 'likely', 'originate', 'from', 'bat', 'and', 'may', 'have', 'pass', 'through', 'an', 'intermediary', 'animal', 'source', '(', 'currently', 'unknown', ')', 'in', 'china', 'before', 'be', 'transmit', 'to', 'humans', '.', 'it', 'be', 'recommend', 'that', 'individuals', 'who', 'travel', 'to', 'avoid', 'contact', 'with', 'animals', ',', 'include', 'wild', 'meat', 'and', 'wet', '(', 'live', 'animal', ')', 'market', '.', 'if', 'you', 'be', 'consider', 'travel', ',', 'check', 'the', 'latest', 'travel', 'health', 'notice', 'for', 'the', 'most', 'up-to-date', 'travel', 'advice', 'prior', 'to', 'travel', '.', 'importers', ',', 'rescue', 'organizations', 'and', 'adoptive', 'families', 'should', 'limit', 'or', 'postpone', 'import', 'animals', 'from', 'affect', 'areas', '.', 'if', 'animals', 'be', 'import', 'from', 'an', 'affect', 'area', ':', 'they', 'should', 'be', 'closely', 'monitor', 'for', 'sign', 'of', 'illness', ',', 'you', 'should', 'contact', 'a', 'veterinarian', 'if', 'they', 'become', 'sick', ',', 'and', 'call', 'ahead', 'to', 'ensure', 'they', 'be', 'aware', 'of', 'the', 'circumstances']\n",
      "\n",
      "\n",
      "===Lemmatize words, remove stop words/punctuations==\n",
      "\n",
      "['get', 'covid-19', 'animals', 'travel', 'countries', 'although', 'current', 'spread', 'growth', 'covid-19', 'outbreak', 'primarily', 'associate', 'spread', 'person', 'person', 'experts', 'agree', 'virus', 'likely', 'originate', 'bat', 'may', 'pass', 'intermediary', 'animal', 'source', 'currently', 'unknown', 'china', 'transmit', 'humans', 'recommend', 'individuals', 'travel', 'avoid', 'contact', 'animals', 'include', 'wild', 'meat', 'wet', 'live', 'animal', 'market', 'consider', 'travel', 'check', 'latest', 'travel', 'health', 'notice', 'up-to-date', 'travel', 'advice', 'prior', 'travel', 'importers', 'rescue', 'organizations', 'adoptive', 'families', 'limit', 'postpone', 'import', 'animals', 'affect', 'areas', 'animals', 'import', 'affect', 'area', 'closely', 'monitor', 'sign', 'illness', 'contact', 'veterinarian', 'become', 'sick', 'call', 'ahead', 'ensure', 'aware', 'circumstances']\n",
      "\n",
      "\n",
      "===Do not lemmatize words, remove stop words, but keep punctuations===\n",
      "\n",
      "['get', 'covid-19', 'animals', 'travelling', 'countries', '?', 'although', 'current', 'spread', 'growth', 'covid-19', 'outbreak', 'primarily', 'associated', 'spread', 'person', 'person', ',', 'experts', 'agree', 'virus', 'likely', 'originated', 'bats', 'may', 'passed', 'intermediary', 'animal', 'source', '(', 'currently', 'unknown', ')', 'china', 'transmitted', 'humans', '.', 'recommended', 'individuals', 'travel', 'avoid', 'contact', 'animals', ',', 'including', 'wild', 'meat', 'wet', '(', 'live', 'animal', ')', 'markets', '.', 'considering', 'travel', ',', 'check', 'latest', 'travel', 'health', 'notices', 'up-to-date', 'travel', 'advice', 'prior', 'travelling', '.', 'importers', ',', 'rescue', 'organizations', 'adoptive', 'families', 'limit', 'postpone', 'importing', 'animals', 'affected', 'areas', '.', 'animals', 'imported', 'affected', 'area', ':', 'closely', 'monitored', 'signs', 'illness', ',', 'contact', 'veterinarian', 'become', 'sick', ',', 'call', 'ahead', 'ensure', 'aware', 'circumstances']\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2.2\n",
      "TF_IDF Shape:  (93, 1176)\n",
      "IDF Shape:  (1176,)\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2.3 -- You can try different questions related to Covid-19 here\n",
      "[('travel', 0.5581968422115329), ('to', 0.19582304295037772), ('be', 0.16860582243348515), ('it', 0.23895877379391126), ('by', 0.45210016774796324), ('safe', 0.6001215215149591)]\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2.4\n",
      "Can I get COVID-19 from animals when travelling to other countries? Although the current spread and growth of the COVID-19 outbreak is primarily associated with spread from person to person, experts agree that the virus likely originated from bats and may have passed through an intermediary animal source (currently unknown) in China before being transmitted to humans.  It is recommended that individuals who travel to avoid contact with animals, including wild meat and wet (live animal) markets.  If you are considering travel, check the latest travel health notices for the most up-to-date travel advice prior to travelling.  Importers, rescue organizations and adoptive families should limit or postpone importing animals from affected areas. If animals are imported from an affected area: they should be closely monitored for signs of illness, you should contact a veterinarian if they become sick, and call ahead to ensure they are aware of the circumstances \n",
      "\n",
      "There have been a lot of mixed messages about kids being 'immune' or 'unlikely' to be affected. Can you provide some clarity around what this looks like for small children? Currently, children are not at a higher risk of getting COVID-19 than adult. Some kids and infants have been sick with COVID-19, but it is still adults who make up most of the cases we are seeing today. If a child gets COVID-19, their symptoms are going to be similar to the symptoms adults get. This includes: cold-like symptoms like fever, runny nose, and cough. Vomiting and diarrhea are also possible.   [More information on how to stay safe and healthy] It is still important that your child help stop the spread of COVID-19 by practicing the same things you are doing to stay healthy and safe. This includes cleaning hands often with soap water, or an alcohol based hand sanitizer, avoiding people who ar sick, and cleaning/disinfecting high-touch surfaces regularly. \n",
      "\n",
      "What is the current advice if I am planning to travel? 1. self-isolate for 14 days after returning from travel  2. monitor for symptoms of COVID-19 (fever, cough or difficulty breathing) for 14 days after returning to Canada 3. wash your hands often for 20 seconds and cough or sneeze into a tissue or the bend of your arm, not your hand \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Q1\n",
    "    \n",
    "    text='''Consumer Price Index:\n",
    "            +0.2% in Sep 2020\n",
    "\n",
    "            Unemployment Rate:\n",
    "            +7.9% in Sep 2020\n",
    "\n",
    "            Producer Price Index:\n",
    "            +0.4% in Sep 2020\n",
    "\n",
    "            Employment Cost Index:\n",
    "            +0.5% in 2nd Qtr of 2020\n",
    "\n",
    "            Productivity:\n",
    "            +10.1% in 2nd Qtr of 2020\n",
    "\n",
    "            Import Price Index:\n",
    "            +0.3% in Sep 2020\n",
    "\n",
    "            Export Price Index:\n",
    "            +0.6% in Sep 2020'''\n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q1\")\n",
    "    print(extract(text))\n",
    "      \n",
    "    data=pd.read_csv(\"03_data/03_covid_qa.csv\")\n",
    "    # concatenate a pair of question and answer as a single doc\n",
    "    docs = data.apply(lambda x: x[\"question\"] + \" \" + x[\"answer\"], axis = 1) \n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2.1 - Try different parameter values to make sure all options work\\n\")\n",
    "    \n",
    "    # Let's tokenize the first document\n",
    "    doc = docs[0]\n",
    "    \n",
    "    print(\"===Lemmatize words, keep stop words/punctuations===\\n\")\n",
    "    tokens = tokenize(doc, lemmatized=True, stopword=True, punctuation = True)\n",
    "    print(tokens)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"===Lemmatize words, remove stop words/punctuations==\\n\")\n",
    "    tokens = tokenize(doc, lemmatized=True, stopword=False, punctuation = False)\n",
    "    print(tokens)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"===Do not lemmatize words, remove stop words, but keep punctuations===\\n\")\n",
    "    tokens = tokenize(doc, lemmatized=False, stopword=False, punctuation = True)\n",
    "    print(tokens)\n",
    "     \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2.2\")\n",
    "    tf_idf, idf, words = compute_tfidf(docs, lemmatized=True, stopword=True, punctuation = False)\n",
    "    print(\"TF_IDF Shape: \", tf_idf.shape)\n",
    "    print(\"IDF Shape: \", idf.shape)\n",
    "\n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2.3 -- You can try different questions related to Covid-19 here\")\n",
    "    doc = 'Is it safe to travel by plane?'  # What kind of masks should I use?\n",
    "    vect = vectorize_doc(doc, words, idf, lemmatized=True, stopword=True, punctuation = True)\n",
    "    # print words with non-zero tf_idf weights\n",
    "    print([(words[idx], i) for idx, i in enumerate(vect) if i>0])\n",
    "  \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2.4\")\n",
    "    answers = find_answer(vect, tf_idf, docs)\n",
    "    for a in answers:\n",
    "        print(a, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
