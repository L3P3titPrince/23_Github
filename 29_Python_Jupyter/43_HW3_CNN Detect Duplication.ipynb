{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Paper Reading\n",
    "- Pick one of the following papers, understand its key ideas, and replicate the experiments using the datasets described in the paper or your own dataset.\n",
    "    - Convolutional Neural Network Architectures for Matching Natural Language Sentences, https://arxiv.org/pdf/1503.03244.pdf\n",
    "    - Matching Networks for One Shot Learning , https://arxiv.org/pdf/1606.04080v2.pdf\n",
    "    - Prototypical Networks for Few-shot Learning, https://arxiv.org/pdf/1703.05175v2.pdf\n",
    "- Requirements:\n",
    "    - Write a report summarizing the key idea, your experiment results, and your analysis and critique\n",
    "    - Give a 5-10 min oral presentation in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Siamese CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Assignment, let's use CNN to detect duplicate sentences. Two datasets have been prepared for you: train.csv and test.csv. Both files are in the following format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|question1 | question2 |is_duplicate|\n",
    "|------|------|-------|\n",
    "|How do you take a screenshot on a Mac laptop?|  How do I take a screenshot on my MacBook Pro? ...|   1 |\n",
    "|Is the US election rigged?|  Was the US election rigged?|   1 |\n",
    "|How scary is it to drive on the road to Hana g...|  Do I need a four-wheel-drive car to drive all ...\t|  0  |\n",
    "|...|...| ...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions below to clasify the sentence pairs in the training dataset and then test the model using test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define a **class** called \"text_processor\" to preporcess text:\n",
    "- first create **\"\\_\\_init\\_\\_\"** function:\n",
    "    - set the *class attributes*: MAX_SEN_LEN (max sentence length) and MAX_WORDS (max number of words in corpus). You'll need to explore the dataset to set these two parameters properly\n",
    "    - initialize a tokenizer with parameter num_words = MAX_WORDS and set the tokenizer object as a *class object*\n",
    "    - fit the tokenizer using the training sentence pairs (i.e. method \"*fit_on_texts*\")\n",
    "- create a function **\"generate_seq\"** which does the following:\n",
    "    - take a list of sentences as an input \n",
    "    - generates padded sequences from the sentences using the class tokenizer object define above\n",
    "    - retrun the padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\ProgramData\\Anaconda3\\envs\\03_ten1131_keras231\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\03_ten1131_keras231\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\03_ten1131_keras231\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\03_ten1131_keras231\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\03_ten1131_keras231\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ProgramData\\Anaconda3\\envs\\03_ten1131_keras231\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Add import\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# fix random number\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text_preprocessor class\n",
    "class text_preprocessor(object):\n",
    "    \n",
    "    # define __init__ function    \n",
    "    def __init__(self, max_sen_len, max_words, docs):\n",
    "        \n",
    "        # add your code here\n",
    "        # set sentence/document length\n",
    "        self.max_sen_len = max_sen_len\n",
    "        \n",
    "        # set the maximum number of words to be used\n",
    "        #max_word will define the max value of tokenizer\n",
    "        self.max_words = max_words\n",
    "        \n",
    "        #set data to self.doc\n",
    "        self.docs = docs\n",
    "        #print(self.docs.head()) #success\n",
    "        \n",
    "        #self.word = tokenizer.word_index\n",
    "        #these two line create dictornary, give index to each words\n",
    "        \n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        tokenizer.fit_on_texts(self.docs)\n",
    "    # define generate_seq function \n",
    "    def generate_seq(self, docs):\n",
    "        \n",
    "        sequences = None\n",
    "        \n",
    "        # add your code here\n",
    "        # convert each document to a list of word index as a sequence\n",
    "        # get a Keras tokenizer\n",
    "        # https://keras.io/preprocessing/text/\n",
    "        #don't put tokenize in here\n",
    "        #if you tokenize here, you will create a brand new dictonary/volcapbulaer\n",
    "        tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        \n",
    "        #Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. \n",
    "        #So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 \n",
    "        #it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. \n",
    "        #So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "        \n",
    "        tokenizer.fit_on_texts(self.docs)\n",
    "        #Transforms each text in texts to a sequence of integers. So it basically takes each word in the text \n",
    "        #and replaces it with its corresponding integer value from the word_index dictionary. \n",
    "        #Nothing more, nothing less, certainly no magic involved.\n",
    "        \n",
    "        sequences = tokenizer.texts_to_sequences(self.docs)\n",
    "        \n",
    "        #this is final result, it's a \n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=self.max_sen_len, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        word_count = tokenizer.word_counts\n",
    "        #print('Number of Unique Tokens',len(word_index))\n",
    "        #print(word_count)\n",
    "        #print(padded_sequences.shape)\n",
    "        #print(type(padded_sequences))\n",
    "        self.word = tokenizer.word_index\n",
    "        \n",
    "        return padded_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a function \"cnn_model\" to define a CNN model as follows:\n",
    "- take parameters: FILTER_SIZERS (a list of Conv1D filter sizes), NUM_FILTERS (the number of filters), MAX_WORDS, MAX_SEN_LEN, and EMBEDDING_DIM (dimision of word vectors)\n",
    "- define a CNN model with **Conv1D** using the specifified FILTER_SIZERS and NUM_FILTERS. For example, if FILTER_SIZERS=[1,2,3] and NUM_FILTERS=64, your model may look like the figure below.\n",
    "- return this CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='04_Images/12_cnn_model.png' width='50%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "def cnn_model(EMBEDDING_DIM, \\\n",
    "              # word vector dimension\n",
    "              FILTER_SIZES,\\\n",
    "              # filter sizes as a list\n",
    "              MAX_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_SEN_LEN, \\\n",
    "              # max words in a doc\n",
    "              #NAME = 'cnn',\\\n",
    "              NUM_FILTERS\\\n",
    "              #add input layer from outside\n",
    "              #outside_input\n",
    "              ):            \n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    # define input layer, where a sentence represented as\n",
    "    # 1 dimension array with integers\n",
    "    main_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='main_input')\n",
    "    \n",
    "    # define the embedding layer\n",
    "    # input_dim is the size of all words +1,because we will use wide convolution, we add zero before first input layer\n",
    "    # where 1 is for the padding symbol\n",
    "    # output_dim is the word vector dimension\n",
    "    # input_length is the max. length of a document\n",
    "    # input to embedding layer is the \"main_input\" layer\n",
    "    embed_1 = Embedding(input_dim=MAX_WORDS+1, \\\n",
    "                    output_dim=EMBEDDING_DIM, \\\n",
    "                    input_length=MAX_SEN_LEN,\\\n",
    "                    name='embedding')(main_input)  \n",
    "    \n",
    "\n",
    "    # define 1D convolution layer\n",
    "    # 64 filters are used\n",
    "    # a filter slides through each word (kernel_size=1)\n",
    "    # input to this layer is the embedding layer\n",
    "    conv1d_1= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_SIZES[0], \\\n",
    "                     name='conv_unigram',\\\n",
    "                     activation='relu')(embed_1)\n",
    "\n",
    "    # define a 1-dimension MaxPooling \n",
    "    # to take the output of the previous convolution layer\n",
    "    # the convolution layer produce \n",
    "    # MAX_SEN_LEN-1+1 values as ouput (???)\n",
    "    pool_1 = MaxPooling1D(MAX_SEN_LEN-1+1, \\\n",
    "                          name='pool_unigram')(conv1d_1)\n",
    "\n",
    "    # The pooling layer creates output \n",
    "    # in the size of (# of sample, 1, 64)  \n",
    "    # remove one dimension since the size is 1\n",
    "    flat_1 = Flatten(name='flat_unigram')(pool_1)\n",
    "\n",
    "    \n",
    "    #***********************************************************************************#\n",
    "    \n",
    "    \n",
    "    # following the same logic to define \n",
    "    # filters for bigram\n",
    "    conv1d_2= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_SIZES[1], \\\n",
    "                     name='conv_bigram',\\\n",
    "                     activation='relu')(embed_1)\n",
    "    pool_2 = MaxPooling1D(MAX_SEN_LEN-2+1, name='pool_bigram')(conv1d_2)\n",
    "    flat_2 = Flatten(name='flat_bigram')(pool_2)\n",
    "\n",
    "    \n",
    "    \n",
    "    #***********************************************************************************#\n",
    "        \n",
    "    # filters for trigram\n",
    "    conv1d_3= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_SIZES[2], \\\n",
    "                     name='conv_trigram',activation='relu')(embed_1)\n",
    "    pool_3 = MaxPooling1D(MAX_SEN_LEN-3+1, name='pool_trigram')(conv1d_3)\n",
    "    flat_3 = Flatten(name='flat_trigram')(pool_3)\n",
    "\n",
    "    # Concatenate flattened output\n",
    "    z=Concatenate(name='concate')([flat_1, flat_2, flat_3])\n",
    "\n",
    "    # create the model with input layer\n",
    "    # and the output layer\n",
    "    model = Model(inputs=main_input, outputs=z)\n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define three architecutres as described below. You may add appropriate regularizers in each model:\n",
    "- Model A: Use one CNN described above to process each question **without any parameter sharing**. Concatenate features extracted from both CNNs and then use a dense layer to predict the output\n",
    "- Model B: Use a **shared** CNN to process both questions. Concatenate features extracted from the CNN and then use a dense layer to predict the output\n",
    "- Model C: Use a shared CNN to process both questions. Then take the **absolute difference** between the feature vectors extracted from the CNN (hint, use keras \"Lambda\" layer), and connect the difference to a dense layer to predict the output\n",
    "- Model D (**Bonus**): You can come up with your own architecture as long as it can outperform the above models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model A | Model B   | Model C |\n",
    "|:------:|:------:|:---------:|\n",
    "|   <img src=\"04_Images/09_model_a.png\"/>| <img src=\"04_Images/10_model_b.png\" />| <img src=\"04_Images/11_model_c.png\"/> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model A\n",
    "def model_A(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS,):\n",
    "    \n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    q1_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q1_input')\n",
    "    q2_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q2_input')\n",
    "    \n",
    "    \n",
    "    left_cnn = cnn_model(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS)\n",
    "    left = left_cnn(q1_input)\n",
    "    #print(type(left_cnn))\n",
    "    right_cnn = cnn_model(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS)\n",
    "    right = right_cnn(q2_input)\n",
    "    #print(type(right_cnn))\n",
    "    con_layer=Concatenate(name='concate')([left, right])\n",
    "    \n",
    "    # Create a dropout layer\n",
    "    # In each iteration only 50% units are turned on\n",
    "    drop_1=Dropout(rate=0.5, name='dropout')(con_layer)\n",
    "\n",
    "    # Create a dense layer\n",
    "    dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "    # Create the output layer\n",
    "    preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "    model = Model(inputs=[q1_input,q2_input], outputs=preds)\n",
    "    #model.summary()\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model B\n",
    "\n",
    "def model_B(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS,):\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    q1_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q1_input')\n",
    "    q2_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q2_input')\n",
    "    \n",
    "    \n",
    "    cnn = cnn_model(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS)\n",
    "\n",
    "    con_layer=Concatenate(name='concate')([cnn(q1_input), cnn(q2_input)])\n",
    "    \n",
    "    # Create a dropout layer\n",
    "    # In each iteration only 50% units are turned on\n",
    "    drop_1=Dropout(rate=0.5, name='dropout')(con_layer)\n",
    "\n",
    "    # Create a dense layer\n",
    "    dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "    # Create the output layer\n",
    "    preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "    model = Model(inputs=[q1_input,q2_input], outputs=preds)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model C\n",
    "\n",
    "def model_C(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS,):\n",
    "    from keras.layers import Lambda\n",
    "    from keras import backend as K\n",
    "#     from keras.utils import plot_model\n",
    "#     import pydot\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    q1_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q1_input')\n",
    "    q2_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q2_input')\n",
    "    \n",
    "    \n",
    "    cnn = cnn_model(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS)\n",
    "\n",
    "    #con_layer=Concatenate(name='concate')([cnn(q1_input), cnn(q2_input)])\n",
    "    \n",
    "    Diff = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))([cnn(q1_input), cnn(q2_input)]) \n",
    "    # Create a dropout layer\n",
    "    # In each iteration only 50% units are turned on\n",
    "    drop_1=Dropout(rate=0.5, name='dropout')(Diff)\n",
    "\n",
    "    # Create a dense layer\n",
    "    dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "    # Create the output layer\n",
    "    preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "    model = Model(inputs=[q1_input,q2_input], outputs=preds)\n",
    "    model.summary()\n",
    "   \n",
    "    #plot_model(model, to_file='04_Images/model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define a function \"train_model\" to:\n",
    "- Train a model provided as an input parameter\n",
    "- Use appropriate techniques to ensure you don't overfit the model\n",
    "- Plot training history to make sure your model is reasonable good\n",
    "- Using the testing dataset, calculate precision, recall, and F-1 score of each class (assuming 0.5 probabbility threshould), and also report AUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_model function\n",
    "\n",
    "def train_model(model, \\\n",
    "                q1_train, q2_train, y_train, \\\n",
    "                q1_val, q2_val, y_val, \\\n",
    "                q1_test, q2_test, y_test, \\\n",
    "                BATCH_SIZE, \\\n",
    "                NUM_EPOCHES\n",
    "               ):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "    \n",
    "    # compile and train model\n",
    "    # process test dataset\n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", \\\n",
    "              metrics=[\"accuracy\"]\n",
    "                 )\n",
    "    \n",
    "    #BATCH_SIZE = 64\n",
    "    #NUM_EPOCHES = 10\n",
    "\n",
    "    # fit the model and save fitting history to \"training\"\n",
    "    training=model.fit([q1_train, q2_train],y_train, \\\n",
    "                   batch_size=BATCH_SIZE, \\\n",
    "                   epochs=NUM_EPOCHES,\\\n",
    "                   validation_data=[[q1_val, q2_val],y_val], \\\n",
    "                   #verbose=2\n",
    "                        )\n",
    "    \n",
    "    # plot training history\n",
    "    # the fitting history is saved as dictionary\n",
    "    # covert the dictionary to dataframe\n",
    "    df_hist=pd.DataFrame.from_dict(training.history)\n",
    "    df_hist.columns=[ \"val_loss\", \"val_acc\", \"train_loss\",\"train_acc\"]\n",
    "    df_hist.index.name='epoch'\n",
    "    print(df_hist)\n",
    "\n",
    "    # plot training history\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3));\n",
    "\n",
    "    df_hist[[\"train_acc\", \"val_acc\"]].plot(ax=axes[0]);\n",
    "    df_hist[[\"train_loss\", \"val_loss\"]].plot(ax=axes[1]);\n",
    "    plt.show();\n",
    "    \n",
    "    # predict testing samples\n",
    "    y_pred = model.predict([q1_test,q2_test])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print classfication report\n",
    "    \n",
    "    print(classification_report(y_test, y_pred[:] > 0.5))\n",
    "    \n",
    "    # calculate ROC AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "    print('AUC: {:.2f}'.format(auc(fpr, tpr)))\n",
    "\n",
    "    plt.figure(figsize = (15, 7))\n",
    "    plt.plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'Logistic')\n",
    "    plt.plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--', label = 'No skill')\n",
    "    plt.title('AUC of Siamese Network Model')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Call the function \"train_model\" to get the performance of each model. Show your analysis as markdowns on the following:\n",
    "- How did you choose the hyperparaters: FILTER_SIZES,NUM_FILTERS, MAX_SEN_LEN, MAX_WORDS, EMBEDDING_DIM\n",
    "- Analyze each architecture to understand its pros and cons\n",
    "- Which architecture is the most effective and why is it effective for this classification task?\n",
    "- What regularizers did you use and why did it work?\n",
    "- What features do you think CNN can successfully extract? What kind of useful features could be missed by CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-04d6fc4cdccd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mseq_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_preprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMAX_SEN_LEN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_WORDS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mdata1_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mseq_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and test each model\n",
    "if __name__ == \"__main__\":  \n",
    " \n",
    "    #read data\n",
    "    #our train.csv have three column ,[questin1] and [question2] are string sentence. Next step we should transfer sentence into vector\n",
    "    df_train = pd.read_csv('03_data/21_train.csv')\n",
    "    df_test = pd.read_csv('03_data/22_test.csv')\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\\\n",
    "                        df_train.iloc[:,:2], df_train['is_duplicate'],\\\n",
    "                        test_size=0.3, random_state=1)\n",
    "    # Set hyper parameters\n",
    "    #maybe means, first is unigram(kernel_size=1), second is bigram(kernel_size=2), third is trigram(kernel_size=3)\n",
    "    FILTER_SIZES = [1, 2, 3]\n",
    "    \n",
    "    #how manny feature maps to extract\n",
    "    NUM_FILTERS = 64\n",
    "    \n",
    "    #I think this parameter is saame as MAX_SEN_LEN\n",
    "    #MAX_DOC_LEN = 500\n",
    "    \n",
    "    #for these two column sentence, the lengthist is 105 words in single sentence\n",
    "    MAX_SEN_LEN = len(df_train['question1'].max())\n",
    "    \n",
    "    #this is definnation of tokenzier max token value, smaller and less feature, and greater value will get more features\n",
    "    #typical we setup 10000 to adapat requirement of word feature extract\n",
    "    MAX_WORDS = 10000\n",
    "    \n",
    "    #the output dimention of embedding input layers \n",
    "    EMBEDDING_DIM = 200\n",
    "    \n",
    "    BATCH_SIZE = 256\n",
    "    \n",
    "    NUM_EPOCHES = 5\n",
    "    \n",
    "    #try to find the max length of these two questions set\n",
    "    print(len(df_train['question1'].max()))\n",
    "    \n",
    "\n",
    "    \n",
    "    data1_test = df_test['question1']\n",
    "    data2_test = df_test['question2']\n",
    "    Y_test = df_test['is_duplicate']\n",
    "    \n",
    "    \n",
    "    seq_1 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, X_train['question1'])\n",
    "    data1_train= seq_1.generate_seq(X_train['question1'])\n",
    "    \n",
    "    seq_2 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, X_train['question2'])\n",
    "    data2_train= seq_2.generate_seq(X_train['question2'])\n",
    "    \n",
    "    seq_3 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, X_val['question1'])\n",
    "    data1_val= seq_3.generate_seq(X_val['question1'])\n",
    "    \n",
    "    seq_4 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, X_val['question2'])\n",
    "    data2_val = seq_4.generate_seq(X_val['question2'])\n",
    "    \n",
    "    seq_5 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, df_test['question1'])\n",
    "    data1_test = seq_5.generate_seq(df_test['question1'])\n",
    "    \n",
    "    seq_6 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, df_test['question2'])\n",
    "    data2_test = seq_6.generate_seq(df_test['question2'])\n",
    "    \n",
    "    #print(type(data_1))\n",
    "    \n",
    "    \n",
    "    #print(q1_train)\n",
    "    #print(q1_train.shape)\n",
    "   \n",
    "    # train and test model A/B/C\n",
    "    a= model_A(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS,MAX_SEN_LEN,NUM_FILTERS)\n",
    "    b= model_B(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS,MAX_SEN_LEN,NUM_FILTERS)\n",
    "    c= model_C(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS,MAX_SEN_LEN,NUM_FILTERS)\n",
    "\n",
    "\n",
    "    train_model(a, \\\n",
    "                data1_train, data2_train,Y_train, \\\n",
    "                data1_val, data2_val,Y_val, \\\n",
    "                data1_test, data2_test,Y_test, \\\n",
    "                BATCH_SIZE, \\\n",
    "                NUM_EPOCHES\n",
    "               )\n",
    "\n",
    "\n",
    "    model=None\n",
    "\n",
    "\n",
    "    train_model(b, \\\n",
    "                data1_train, data2_train,Y_train, \\\n",
    "                data1_val, data2_val,Y_val, \\\n",
    "                data1_test, data2_test,Y_test, \\\n",
    "                BATCH_SIZE, \\\n",
    "                NUM_EPOCHES\n",
    "               )\n",
    "    \n",
    "    model=None\n",
    "    train_model(c, \\\n",
    "                data1_train, data2_train,Y_train, \\\n",
    "                data1_val, data2_val,Y_val, \\\n",
    "                data1_test, data2_test,Y_test, \\\n",
    "                BATCH_SIZE, \\\n",
    "                NUM_EPOCHES\n",
    "               )\n",
    "    \n",
    "\n",
    "    \n",
    "    # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Hyperparameter choose and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word frequency: \n",
      "         freq\n",
      "android    57\n",
      "phone      83\n",
      "is       3347\n",
      "best      851\n",
      "up         97\n",
      "to       2493\n",
      "range      19\n",
      "of       1984\n",
      "15000       5\n",
      "i        2498\n",
      "   word_freq  count   percent    cumsum\n",
      "0          1   6338  0.527683  0.527683\n",
      "1          2   1878  0.156357  0.684040\n",
      "2          3    875  0.072850  0.756890\n",
      "3          4    538  0.044792  0.801682\n",
      "4          5    390  0.032470  0.834152\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "MAX_NB_WORDS =10000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df_train[\"question1\"])\n",
    "\n",
    "\n",
    "df_count=pd.DataFrame.from_dict(tokenizer.word_counts, \\\n",
    "                          orient=\"index\")\n",
    "df_count.columns=['freq']\n",
    "print(\"word frequency: \")\n",
    "print(df_count.head(10))\n",
    "\n",
    "# get histogram of word count\n",
    "df_count=df_count['freq'].value_counts().reset_index()\n",
    "df_count.columns=['word_freq','count']\n",
    "\n",
    "# sort by word_freq\n",
    "df_count=df_count.sort_values(by='word_freq')\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df_count['percent']=df_count['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df_count['cumsum']=df_count['percent'].cumsum()\n",
    "\n",
    "print(df_count.head())\n",
    "\n",
    "df_count.iloc[0:50].plot(x='word_freq', y='cumsum');\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q:How did you choose the hyperparaters: FILTER_SIZES,NUM_FILTERS, MAX_SEN_LEN, MAX_WORDS, EMBEDDING_DIM\n",
    "- A:FILTER_SIZES is the filter matrix-grim, in this problem we chouse[1,2,3], sometimes we can choose [2,3,4] which are less sensitive than [1,2,3]. \n",
    "- NUM_FILTERs are number of featuer maps, if we want more feature map we should increase but this will increaes running time.\n",
    "- MAX_SEN_LEN is decide by most of length of sentcen ,if 80% sentenc nomore than 30 words, we should choose word to in accerlate model. - - MAX_WORDS are max number of word_vecter can represent, 10000 is too big, according to our model 2000 is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: Analyze each architecture to understand its pros and cons\n",
    "- A:Model A have didn't input and increase the diffcults of calsscifation. Model B is better than A from classiciation. And Model C is the best of all from effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: Which architecture is the most effective and why is it effective for this classification task?\n",
    "- A: Model C is most effective because it cacluate the distance between qustion1 vector and question2 vecter and give a more small dimention to classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q:What regularizers did you use and why did it work?\n",
    "- A:dropout decrease the nosize of useless word without harm accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q:What features do you think CNN can successfully extract? What kind of useful features could be missed by CNN?\n",
    "- A:short phrase which have similar meaning will be every easy to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#test_model:\n",
    "df_5 = pd.read_csv('03_data/21_train.csv')\n",
    "\n",
    "df_5\n",
    "\n",
    "\n",
    "\n",
    "len(df_5['question1'])\n",
    "\n",
    "print(type(df_5['question1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Android phone is best up to range of 15000?</td>\n",
       "      <td>Which is the best smartphone in India under Rs...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I forgot my Gmail username and have no access ...</td>\n",
       "      <td>How would I retrieve my Yahoo account without ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What were the major effects of the cambodia ea...</td>\n",
       "      <td>What were the major effects of the cambodia ea...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the best ways to clean a Jansport bac...</td>\n",
       "      <td>What are some safe ways of washing my JanSport...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I download Arrow season 5?</td>\n",
       "      <td>How can I download Arrow Season 3?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>How is personification presented in Macbeth?</td>\n",
       "      <td>What are examples of personification in Macbeth?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>What are catapults? How are they used?</td>\n",
       "      <td>What are catapults?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>What country has never kicked out the jews?</td>\n",
       "      <td>Is there a single stupid egotist here in Quora...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>How do I recover my Google account password if...</td>\n",
       "      <td>How can I recover my Google account if my reco...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Where can I find a list of the best Telegram g...</td>\n",
       "      <td>How can I join Telegram philosopgy group?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question1  \\\n",
       "0           Android phone is best up to range of 15000?   \n",
       "1     I forgot my Gmail username and have no access ...   \n",
       "2     What were the major effects of the cambodia ea...   \n",
       "3     What are the best ways to clean a Jansport bac...   \n",
       "4                    How can I download Arrow season 5?   \n",
       "...                                                 ...   \n",
       "9995       How is personification presented in Macbeth?   \n",
       "9996             What are catapults? How are they used?   \n",
       "9997        What country has never kicked out the jews?   \n",
       "9998  How do I recover my Google account password if...   \n",
       "9999  Where can I find a list of the best Telegram g...   \n",
       "\n",
       "                                              question2  is_duplicate  \n",
       "0     Which is the best smartphone in India under Rs...           1.0  \n",
       "1     How would I retrieve my Yahoo account without ...           0.0  \n",
       "2     What were the major effects of the cambodia ea...           1.0  \n",
       "3     What are some safe ways of washing my JanSport...           1.0  \n",
       "4                    How can I download Arrow Season 3?           0.0  \n",
       "...                                                 ...           ...  \n",
       "9995   What are examples of personification in Macbeth?           0.0  \n",
       "9996                                What are catapults?           1.0  \n",
       "9997  Is there a single stupid egotist here in Quora...           0.0  \n",
       "9998  How can I recover my Google account if my reco...           1.0  \n",
       "9999          How can I join Telegram philosopgy group?           0.0  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'The quick brown fox jumped over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, hashing_trick, one_hot, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four way to preprocess text type:\n",
    " - Tokenizer\n",
    " - hashing_trick\n",
    " - one_hot\n",
    " - text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1 = text_to_word_sequence(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = [one_hot(d, n=9) for d in words_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2], [8], [1], [1], [7], [4], [2], [6], [6]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('t', 2), ('h', 2), ('e', 4), ('q', 1), ('u', 2), ('i', 1), ('c', 1), ('k', 1), ('b', 1), ('r', 2), ('o', 4), ('w', 1), ('n', 1), ('f', 1), ('x', 1), ('j', 1), ('m', 1), ('p', 1), ('d', 2), ('v', 1), ('l', 1), ('a', 1), ('z', 1), ('y', 1), ('g', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(token.word_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
