{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorce from https://dumps.wikimedia.org/simplewiki/20200301/\n",
    "\n",
    "For a LDA model of Wikipedia we have these steps\n",
    "- Data Pre-processing\n",
    "- Train\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Libray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "#jupyter sometimes messes up the Logging setup:resore it\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "#return the first 'n' elements of the streatm, in here we want first 10 lines\n",
    "def head(stream, n=10):\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change wikipedia dump to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "import gensim.corpora\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from gensim.utils import simple_preprocess, file_or_filename\n",
    "#from gensim.corpora import WikiCorpus\n",
    "from gensim.corpora.wikicorpus import extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we splist sentence into words and test wheather it is in STOPWORDS, \n",
    "#if not in STOPWORDS, we tokenize this word into word frequence with its appear time\n",
    "#simple_preprocess also transfer into lowercase \n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text, min_len=2, max_len=20) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfore dumpu_file into (title, token) tuple to output\n",
    "\n",
    "def iter_wiki(dump_file, ignore_str):\n",
    "    idx = 0\n",
    "    #if the sentence start from this word, we think it is not that useful, we eliminate it in preprocess \n",
    "    ignore_namespaces = ignore_str.split()\n",
    "    #driectly use extract_pages from dump files. The yeild is tuple of (str or None, str, str) – Title, text and page id.\n",
    "    for title, text, pageid in extract_pages(file_or_filename(dump_file)):\n",
    "        #filter_wiki is used to extract text from tuple, output is str\n",
    "        text_filter = filter_wiki(text)\n",
    "        #get token from frome text_filter\n",
    "        tokens = tokenize(text_filter)\n",
    "        #ignore short articles and various meta-article\n",
    "        if len(tokens) < 50 or any(title.startswith(ns+':') for ns in ignore_namespaces):\n",
    "            continue\n",
    "        yield title, tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom nltk.tokenize import RegexpTokenizer\\n# Lemmatize the documents.\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\n\\ndef nltk_wiki(dump_file, ignore_str):\\n    idx = 0\\n    docs=[]\\n    #if the sentence start from this word, we think it is not that useful, we eliminate it in preprocess \\n    ignore_namespaces = ignore_str.split()\\n    #driectly use extract_pages from dump files. The yeild is tuple of (str or None, str, str) – Title, text and page id.\\n    for title, text, pageid in extract_pages(file_or_filename(dump_file)):\\n        #filter_wiki is used to extract text from tuple, output is str\\n        #text_filter = filter_wiki(text)\\n        #get token from frome text_filter\\n        #tokens = tokenize(text_filter)\\n        #**********************************************************\\n        #i just want to use an alternative way to tokenize my raw material\\n        docs[idx] = docs[idx].lower()  # Convert to lowercase.\\n        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\\n        idx+=1\\n        print(idx)\\n        \\n    \\n    # Remove numbers, but not words that contain numbers.\\n    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\\n\\n    # Remove words that are only one character.\\n    docs = [[token for token in doc if len(token) > 1] for doc in docs]\\n\\n    #We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a stemmer in this case because it produces more readable words. \\n    #Output that is easy to read is very desirable in topic modelling.\\n    lemmatizer = WordNetLemmatizer()\\n    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\\n    yield docs\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def nltk_wiki(dump_file, ignore_str):\n",
    "    idx = 0\n",
    "    docs= (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))\n",
    "    #if the sentence start from this word, we think it is not that useful, we eliminate it in preprocess \n",
    "    ignore_namespaces = ignore_str.split()\n",
    "    #driectly use extract_pages from dump files. The yeild is tuple of (str or None, str, str) – Title, text and page id.\n",
    "    for title, text, pageid in extract_pages(file_or_filename(dump_file)):\n",
    "        #filter_wiki is used to extract text from tuple, output is str\n",
    "        #text_filter = filter_wiki(text)\n",
    "        #get token from frome text_filter\n",
    "        #tokens = tokenize(text_filter)\n",
    "        #**********************************************************\n",
    "        #i just want to use an alternative way to tokenize my raw material\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "        idx+=1\n",
    "        print(idx)\n",
    "        \n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    #We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a stemmer in this case because it produces more readable words. \n",
    "    #Output that is easy to read is very desirable in topic modelling.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "    yield docs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents.\n",
    "from gensim.corpora import Dictionary\n",
    "def dictionaries(dump_file,ignore_str):\n",
    "    doc_stream = (tokens for _, tokens in iter_wiki(dump_file,ignore_str))\n",
    "    %time id2word_wiki = Dictionary(doc_stream)\n",
    "    id2word_wiki.filter_extremes(no_below=20, no_above=0.1)\n",
    "    print(id2word_wiki)\n",
    "    yield id2word_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse the first 'clip_docs' Wikipedia documents form file 'dump_file'\n",
    "#Yield each document in turn, as as list of tokens(unicode strings)\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary,  ignore_str, clip_docs):\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.ignore_str = ignore_str\n",
    "        self.clip_docs =clip_docs\n",
    "        \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_wiki(self.dump_file, self.ignore_str), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Convert html to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def url_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    test_tokens = tokenize(response.text)\n",
    "    bow_test_doc = id2word_wiki.doc2bow(test_tokens)\n",
    "    return print(\"This {} link have topic possibility : \\n{}\".format(url, lda_model.get_document_topics(bow_test_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Main Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(163649 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #20000 to Dictionary(241250 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #30000 to Dictionary(302107 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #40000 to Dictionary(365107 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #50000 to Dictionary(429710 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #60000 to Dictionary(463634 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #70000 to Dictionary(522621 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : built Dictionary(547170 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...) from 74654 documents (total 15921985 corpus positions)\n",
      "INFO : discarding 0 tokens: []...\n",
      "INFO : keeping 34462 tokens which were in no less than 20 and no more than 7465 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(34462 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 54s\n",
      "Dictionary(34462 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "April ['april', 'fourth', 'month', 'year', 'comes', 'march', 'months', 'days', 'april', 'begins']\n",
      "August ['august', 'aug', 'eighth', 'month', 'year', 'gregorian', 'calendar', 'coming', 'july', 'september']\n",
      "Art ['painting', 'renoir', 'work', 'art', 'art', 'creative', 'activity', 'expresses', 'imaginative', 'technical']\n",
      "A ['writing', 'cursive', 'letter', 'english', 'alphabet', 'small', 'letter', 'lower', 'case', 'vowel']\n",
      "Air ['air', 'fan', 'air', 'air', 'earth', 'atmosphere', 'air', 'mixture', 'gases', 'dust']\n",
      "Autonomous communities of Spain ['spain', 'divided', 'parts', 'called', 'autonomous', 'communities', 'autonomous', 'means', 'autonomous', 'communities']\n",
      "Alan Turing ['statue', 'alan', 'turing', 'turing', 'idea', 'bombe', 'mechanical', 'details', 'added', 'built']\n",
      "Alanis Morissette ['alanis', 'nadine', 'morissette', 'born', 'june', 'grammy', 'award', 'winning', 'canadian', 'american']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    #these pages are meta-data of wikipedia, which don't contain so much info. We shold ignore these pages\n",
    "    ignore_str = 'Wikipedia Category File Portal template MediaWiki User Help Book Draft'\n",
    "    source = 'D:/Downloads/31_simplewiki-20200301-pages-articles.xml.bz2'\n",
    "    stream = iter_wiki(source,ignore_str)\n",
    "    #word2vector = dictionaries(source,ignore_str)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #for dmenstration, we only print first 8 line with first 10 words\n",
    "    for title, tokens in itertools.islice(iter_wiki(source, ignore_str),8):\n",
    "        print(title, tokens[:10])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to D:/Downloads/35_wiki_bow_0317.mm\n",
      "INFO : saving sparse matrix to D:/Downloads/35_wiki_bow_0317.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : PROGRESS: saving document #26000\n",
      "INFO : PROGRESS: saving document #27000\n",
      "INFO : PROGRESS: saving document #28000\n",
      "INFO : PROGRESS: saving document #29000\n",
      "INFO : PROGRESS: saving document #30000\n",
      "INFO : PROGRESS: saving document #31000\n",
      "INFO : PROGRESS: saving document #32000\n",
      "INFO : PROGRESS: saving document #33000\n",
      "INFO : PROGRESS: saving document #34000\n",
      "INFO : PROGRESS: saving document #35000\n",
      "INFO : PROGRESS: saving document #36000\n",
      "INFO : PROGRESS: saving document #37000\n",
      "INFO : PROGRESS: saving document #38000\n",
      "INFO : PROGRESS: saving document #39000\n",
      "INFO : PROGRESS: saving document #40000\n",
      "INFO : PROGRESS: saving document #41000\n",
      "INFO : PROGRESS: saving document #42000\n",
      "INFO : PROGRESS: saving document #43000\n",
      "INFO : PROGRESS: saving document #44000\n",
      "INFO : PROGRESS: saving document #45000\n",
      "INFO : PROGRESS: saving document #46000\n",
      "INFO : PROGRESS: saving document #47000\n",
      "INFO : PROGRESS: saving document #48000\n",
      "INFO : PROGRESS: saving document #49000\n",
      "INFO : PROGRESS: saving document #50000\n",
      "INFO : PROGRESS: saving document #51000\n",
      "INFO : PROGRESS: saving document #52000\n",
      "INFO : PROGRESS: saving document #53000\n",
      "INFO : PROGRESS: saving document #54000\n",
      "INFO : PROGRESS: saving document #55000\n",
      "INFO : PROGRESS: saving document #56000\n",
      "INFO : PROGRESS: saving document #57000\n",
      "INFO : PROGRESS: saving document #58000\n",
      "INFO : PROGRESS: saving document #59000\n",
      "INFO : PROGRESS: saving document #60000\n",
      "INFO : PROGRESS: saving document #61000\n",
      "INFO : PROGRESS: saving document #62000\n",
      "INFO : PROGRESS: saving document #63000\n",
      "INFO : PROGRESS: saving document #64000\n",
      "INFO : PROGRESS: saving document #65000\n",
      "INFO : PROGRESS: saving document #66000\n",
      "INFO : PROGRESS: saving document #67000\n",
      "INFO : PROGRESS: saving document #68000\n",
      "INFO : PROGRESS: saving document #69000\n",
      "INFO : PROGRESS: saving document #70000\n",
      "INFO : PROGRESS: saving document #71000\n",
      "INFO : PROGRESS: saving document #72000\n",
      "INFO : PROGRESS: saving document #73000\n",
      "INFO : PROGRESS: saving document #74000\n",
      "INFO : saved 74654x34462 matrix, density=0.279% (7169754/2572726148)\n",
      "INFO : saving MmCorpus index to D:/Downloads/35_wiki_bow_0317.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#create a stream of bag-of-words vectors\n",
    "wiki_corps = WikiCorpus(source,word2vector,ignore_str,None)\n",
    "#store all those bag-of-words vectors into a file, so we don't have to parse the bzipped Wikipedia XML every time over and over:\n",
    "%time gensim.corpora.MmCorpus.serialize('D:/Downloads/35_wiki_bow_0317.mm',wiki_corps)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 2), (10, 1), (11, 3), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 5), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 4), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 3), (35, 3), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 5), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 10), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 2), (70, 2), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (84, 2), (85, 1), (86, 2), (87, 1), (88, 2), (89, 1), (90, 2), (91, 1), (92, 1), (93, 1), (94, 1), (95, 2), (96, 1), (97, 2), (98, 2), (99, 2), (100, 4), (101, 2), (102, 1), (103, 1), (104, 2), (105, 1), (106, 1), (107, 2), (108, 1), (109, 1), (110, 1), (111, 6), (112, 2), (113, 2), (114, 3), (115, 2), (116, 1), (117, 2), (118, 1), (119, 2), (120, 4), (121, 1), (122, 1), (123, 7), (124, 1), (125, 2), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 3), (134, 1), (135, 2), (136, 1), (137, 1), (138, 1), (139, 2), (140, 2), (141, 2), (142, 1), (143, 2), (144, 1), (145, 1), (146, 4), (147, 1), (148, 3), (149, 5), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 3), (158, 1), (159, 1), (160, 1), (161, 3), (162, 1), (163, 1), (164, 4), (165, 1), (166, 2), (167, 1), (168, 1), (169, 1), (170, 1), (171, 2), (172, 1), (173, 3), (174, 4), (175, 12), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 2), (182, 1), (183, 2), (184, 4), (185, 4), (186, 4), (187, 1), (188, 1), (189, 2), (190, 1), (191, 1), (192, 1), (193, 3), (194, 7), (195, 6), (196, 2), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 2), (206, 1), (207, 6), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (213, 2), (214, 3), (215, 1), (216, 2), (217, 1), (218, 5), (219, 1), (220, 1), (221, 2), (222, 5), (223, 1), (224, 5), (225, 2), (226, 1), (227, 6), (228, 1), (229, 1), (230, 1), (231, 1), (232, 4), (233, 1), (234, 2), (235, 4), (236, 3), (237, 1), (238, 1), (239, 1), (240, 1), (241, 2), (242, 1), (243, 1), (244, 1), (245, 1), (246, 2), (247, 3), (248, 1), (249, 1), (250, 3), (251, 3), (252, 2), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 2), (259, 1), (260, 1), (261, 2), (262, 1), (263, 1), (264, 1), (265, 2), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 5), (274, 1), (275, 3), (276, 1), (277, 1), (278, 1), (279, 1), (280, 2), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 2), (289, 1), (290, 2), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 6), (297, 1), (298, 1), (299, 1), (300, 6), (301, 1), (302, 1), (303, 1), (304, 8), (305, 3), (306, 1), (307, 2), (308, 1), (309, 5), (310, 1), (311, 1), (312, 1), (313, 2), (314, 1), (315, 2), (316, 3), (317, 3), (318, 1), (319, 1), (320, 1), (321, 1), (322, 2), (323, 2), (324, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 3), (330, 1), (331, 1), (332, 1), (333, 3), (334, 4), (335, 1), (336, 11), (337, 5), (338, 1), (339, 2), (340, 4), (341, 2), (342, 5), (343, 1), (344, 1), (345, 1), (346, 1), (347, 1), (348, 1), (349, 1), (350, 1), (351, 7), (352, 1), (353, 1), (354, 1), (355, 3), (356, 1), (357, 1), (358, 1), (359, 2), (360, 1), (361, 2), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 1), (369, 1), (370, 1), (371, 1), (372, 2), (373, 2), (374, 1), (375, 2), (376, 1), (377, 1), (378, 2), (379, 1), (380, 2), (381, 2), (382, 1), (383, 1), (384, 1), (385, 2), (386, 1), (387, 3), (388, 1), (389, 1), (390, 2), (391, 1), (392, 1), (393, 2), (394, 1), (395, 9), (396, 5), (397, 1), (398, 1), (399, 1), (400, 1), (401, 1), (402, 1), (403, 1), (404, 1), (405, 1), (406, 1), (407, 2), (408, 1), (409, 1), (410, 2), (411, 1), (412, 14), (413, 1), (414, 1), (415, 3), (416, 6), (417, 1), (418, 1), (419, 2), (420, 1), (421, 2), (422, 2), (423, 1), (424, 2), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 2), (431, 1), (432, 1), (433, 2), (434, 2), (435, 1), (436, 1), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 1), (443, 1), (444, 2), (445, 1), (446, 1), (447, 1), (448, 1), (449, 1), (450, 1), (451, 1), (452, 1), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 2), (462, 1), (463, 3), (464, 2), (465, 2), (466, 6), (467, 6), (468, 1), (469, 1), (470, 1), (471, 1), (472, 1), (473, 1), (474, 1), (475, 1), (476, 1), (477, 8), (478, 1), (479, 1), (480, 1), (481, 1), (482, 1), (483, 1), (484, 1), (485, 1), (486, 1), (487, 1), (488, 1), (489, 1), (490, 1), (491, 1), (492, 1), (493, 1), (494, 4), (495, 2), (496, 1), (497, 1), (498, 1), (499, 2), (500, 3), (501, 1), (502, 1), (503, 1), (504, 1), (505, 1), (506, 1), (507, 1), (508, 2), (509, 1), (510, 1), (511, 1), (512, 1), (513, 3), (514, 1), (515, 4), (516, 1), (517, 3), (518, 1), (519, 1), (520, 1), (521, 1), (522, 1), (523, 2), (524, 1), (525, 1), (526, 1), (527, 2), (528, 1), (529, 1), (530, 1), (531, 2), (532, 1), (533, 1), (534, 1), (535, 3), (536, 1), (537, 1), (538, 1), (539, 1), (540, 1), (541, 1), (542, 1), (543, 1), (544, 3), (545, 1), (546, 1), (547, 2), (548, 4), (549, 1), (550, 1), (551, 3), (552, 1), (553, 3), (554, 1), (555, 1), (556, 5), (557, 4), (558, 1), (559, 1), (560, 1), (561, 1), (562, 4), (563, 1), (564, 1), (565, 2), (566, 4), (567, 2), (568, 1), (569, 2), (570, 1), (571, 1), (572, 2), (573, 1), (574, 2), (575, 1), (576, 1), (577, 1), (578, 1), (579, 2), (580, 4), (581, 1), (582, 2), (583, 1), (584, 3), (585, 1), (586, 1), (587, 1), (588, 2), (589, 1), (590, 1), (591, 1), (592, 2), (593, 2), (594, 1), (595, 1), (596, 1), (597, 1), (598, 1), (599, 1), (600, 1), (601, 1), (602, 1), (603, 1), (604, 1), (605, 2), (606, 2), (607, 1), (608, 1), (609, 2), (610, 1), (611, 1), (612, 1), (613, 1), (614, 2), (615, 1), (616, 1), (617, 1), (618, 2), (619, 1), (620, 15), (621, 1), (622, 5), (623, 1), (624, 3), (625, 2), (626, 2), (627, 1), (628, 2), (629, 1), (630, 2), (631, 1), (632, 1), (633, 3), (634, 1), (635, 1), (636, 2), (637, 1), (638, 5), (639, 2), (640, 1)]\n"
     ]
    }
   ],
   "source": [
    "#test for show tokeniza resule\n",
    "vector = next(iter(wiki_corps))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from D:/Downloads/35_wiki_bow_0317.mm.index\n",
      "INFO : initializing cython corpus reader from D:/Downloads/35_wiki_bow_0317.mm\n",
      "INFO : accepted corpus with 74654 documents, 34462 features, 7169754 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(74654 documents, 34462 features, 7169754 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "#next time we can use mm_corpurs form file\n",
    "mm_corpus = gensim.corpora.MmCorpus('D:/Downloads/35_wiki_bow_0317.mm')\n",
    "print(mm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mm_corpus now contains exactly the same bag-of-words vectors as wiki_corpus before, but they are backed by the .mm file, rather than extracted on the fly from the xml.bz2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 2.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 2.0), (7, 1.0), (8, 1.0), (9, 2.0), (10, 1.0), (11, 3.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 2.0), (16, 1.0), (17, 5.0), (18, 1.0), (19, 1.0), (20, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 2.0), (26, 4.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 2.0), (31, 1.0), (32, 1.0), (33, 1.0), (34, 3.0), (35, 3.0), (36, 1.0), (37, 1.0), (38, 2.0), (39, 1.0), (40, 1.0), (41, 1.0), (42, 1.0), (43, 1.0), (44, 1.0), (45, 1.0), (46, 2.0), (47, 1.0), (48, 1.0), (49, 5.0), (50, 1.0), (51, 1.0), (52, 1.0), (53, 1.0), (54, 1.0), (55, 1.0), (56, 1.0), (57, 1.0), (58, 1.0), (59, 10.0), (60, 2.0), (61, 1.0), (62, 1.0), (63, 1.0), (64, 1.0), (65, 1.0), (66, 2.0), (67, 1.0), (68, 1.0), (69, 2.0), (70, 2.0), (71, 1.0), (72, 2.0), (73, 1.0), (74, 1.0), (75, 1.0), (76, 1.0), (77, 1.0), (78, 1.0), (79, 1.0), (80, 1.0), (81, 1.0), (82, 1.0), (83, 2.0), (84, 2.0), (85, 1.0), (86, 2.0), (87, 1.0), (88, 2.0), (89, 1.0), (90, 2.0), (91, 1.0), (92, 1.0), (93, 1.0), (94, 1.0), (95, 2.0), (96, 1.0), (97, 2.0), (98, 2.0), (99, 2.0), (100, 4.0), (101, 2.0), (102, 1.0), (103, 1.0), (104, 2.0), (105, 1.0), (106, 1.0), (107, 2.0), (108, 1.0), (109, 1.0), (110, 1.0), (111, 6.0), (112, 2.0), (113, 2.0), (114, 3.0), (115, 2.0), (116, 1.0), (117, 2.0), (118, 1.0), (119, 2.0), (120, 4.0), (121, 1.0), (122, 1.0), (123, 7.0), (124, 1.0), (125, 2.0), (126, 1.0), (127, 1.0), (128, 1.0), (129, 1.0), (130, 1.0), (131, 1.0), (132, 1.0), (133, 3.0), (134, 1.0), (135, 2.0), (136, 1.0), (137, 1.0), (138, 1.0), (139, 2.0), (140, 2.0), (141, 2.0), (142, 1.0), (143, 2.0), (144, 1.0), (145, 1.0), (146, 4.0), (147, 1.0), (148, 3.0), (149, 5.0), (150, 1.0), (151, 1.0), (152, 1.0), (153, 1.0), (154, 1.0), (155, 1.0), (156, 1.0), (157, 3.0), (158, 1.0), (159, 1.0), (160, 1.0), (161, 3.0), (162, 1.0), (163, 1.0), (164, 4.0), (165, 1.0), (166, 2.0), (167, 1.0), (168, 1.0), (169, 1.0), (170, 1.0), (171, 2.0), (172, 1.0), (173, 3.0), (174, 4.0), (175, 12.0), (176, 1.0), (177, 1.0), (178, 1.0), (179, 1.0), (180, 1.0), (181, 2.0), (182, 1.0), (183, 2.0), (184, 4.0), (185, 4.0), (186, 4.0), (187, 1.0), (188, 1.0), (189, 2.0), (190, 1.0), (191, 1.0), (192, 1.0), (193, 3.0), (194, 7.0), (195, 6.0), (196, 2.0), (197, 1.0), (198, 1.0), (199, 1.0), (200, 1.0), (201, 1.0), (202, 1.0), (203, 1.0), (204, 1.0), (205, 2.0), (206, 1.0), (207, 6.0), (208, 1.0), (209, 1.0), (210, 1.0), (211, 1.0), (212, 1.0), (213, 2.0), (214, 3.0), (215, 1.0), (216, 2.0), (217, 1.0), (218, 5.0), (219, 1.0), (220, 1.0), (221, 2.0), (222, 5.0), (223, 1.0), (224, 5.0), (225, 2.0), (226, 1.0), (227, 6.0), (228, 1.0), (229, 1.0), (230, 1.0), (231, 1.0), (232, 4.0), (233, 1.0), (234, 2.0), (235, 4.0), (236, 3.0), (237, 1.0), (238, 1.0), (239, 1.0), (240, 1.0), (241, 2.0), (242, 1.0), (243, 1.0), (244, 1.0), (245, 1.0), (246, 2.0), (247, 3.0), (248, 1.0), (249, 1.0), (250, 3.0), (251, 3.0), (252, 2.0), (253, 1.0), (254, 1.0), (255, 1.0), (256, 1.0), (257, 1.0), (258, 2.0), (259, 1.0), (260, 1.0), (261, 2.0), (262, 1.0), (263, 1.0), (264, 1.0), (265, 2.0), (266, 1.0), (267, 1.0), (268, 1.0), (269, 1.0), (270, 1.0), (271, 1.0), (272, 2.0), (273, 5.0), (274, 1.0), (275, 3.0), (276, 1.0), (277, 1.0), (278, 1.0), (279, 1.0), (280, 2.0), (281, 1.0), (282, 1.0), (283, 1.0), (284, 1.0), (285, 1.0), (286, 1.0), (287, 1.0), (288, 2.0), (289, 1.0), (290, 2.0), (291, 1.0), (292, 1.0), (293, 1.0), (294, 1.0), (295, 1.0), (296, 6.0), (297, 1.0), (298, 1.0), (299, 1.0), (300, 6.0), (301, 1.0), (302, 1.0), (303, 1.0), (304, 8.0), (305, 3.0), (306, 1.0), (307, 2.0), (308, 1.0), (309, 5.0), (310, 1.0), (311, 1.0), (312, 1.0), (313, 2.0), (314, 1.0), (315, 2.0), (316, 3.0), (317, 3.0), (318, 1.0), (319, 1.0), (320, 1.0), (321, 1.0), (322, 2.0), (323, 2.0), (324, 1.0), (325, 1.0), (326, 1.0), (327, 1.0), (328, 1.0), (329, 3.0), (330, 1.0), (331, 1.0), (332, 1.0), (333, 3.0), (334, 4.0), (335, 1.0), (336, 11.0), (337, 5.0), (338, 1.0), (339, 2.0), (340, 4.0), (341, 2.0), (342, 5.0), (343, 1.0), (344, 1.0), (345, 1.0), (346, 1.0), (347, 1.0), (348, 1.0), (349, 1.0), (350, 1.0), (351, 7.0), (352, 1.0), (353, 1.0), (354, 1.0), (355, 3.0), (356, 1.0), (357, 1.0), (358, 1.0), (359, 2.0), (360, 1.0), (361, 2.0), (362, 1.0), (363, 1.0), (364, 1.0), (365, 1.0), (366, 1.0), (367, 1.0), (368, 1.0), (369, 1.0), (370, 1.0), (371, 1.0), (372, 2.0), (373, 2.0), (374, 1.0), (375, 2.0), (376, 1.0), (377, 1.0), (378, 2.0), (379, 1.0), (380, 2.0), (381, 2.0), (382, 1.0), (383, 1.0), (384, 1.0), (385, 2.0), (386, 1.0), (387, 3.0), (388, 1.0), (389, 1.0), (390, 2.0), (391, 1.0), (392, 1.0), (393, 2.0), (394, 1.0), (395, 9.0), (396, 5.0), (397, 1.0), (398, 1.0), (399, 1.0), (400, 1.0), (401, 1.0), (402, 1.0), (403, 1.0), (404, 1.0), (405, 1.0), (406, 1.0), (407, 2.0), (408, 1.0), (409, 1.0), (410, 2.0), (411, 1.0), (412, 14.0), (413, 1.0), (414, 1.0), (415, 3.0), (416, 6.0), (417, 1.0), (418, 1.0), (419, 2.0), (420, 1.0), (421, 2.0), (422, 2.0), (423, 1.0), (424, 2.0), (425, 1.0), (426, 1.0), (427, 1.0), (428, 1.0), (429, 1.0), (430, 2.0), (431, 1.0), (432, 1.0), (433, 2.0), (434, 2.0), (435, 1.0), (436, 1.0), (437, 1.0), (438, 1.0), (439, 1.0), (440, 1.0), (441, 1.0), (442, 1.0), (443, 1.0), (444, 2.0), (445, 1.0), (446, 1.0), (447, 1.0), (448, 1.0), (449, 1.0), (450, 1.0), (451, 1.0), (452, 1.0), (453, 1.0), (454, 1.0), (455, 1.0), (456, 1.0), (457, 1.0), (458, 1.0), (459, 1.0), (460, 1.0), (461, 2.0), (462, 1.0), (463, 3.0), (464, 2.0), (465, 2.0), (466, 6.0), (467, 6.0), (468, 1.0), (469, 1.0), (470, 1.0), (471, 1.0), (472, 1.0), (473, 1.0), (474, 1.0), (475, 1.0), (476, 1.0), (477, 8.0), (478, 1.0), (479, 1.0), (480, 1.0), (481, 1.0), (482, 1.0), (483, 1.0), (484, 1.0), (485, 1.0), (486, 1.0), (487, 1.0), (488, 1.0), (489, 1.0), (490, 1.0), (491, 1.0), (492, 1.0), (493, 1.0), (494, 4.0), (495, 2.0), (496, 1.0), (497, 1.0), (498, 1.0), (499, 2.0), (500, 3.0), (501, 1.0), (502, 1.0), (503, 1.0), (504, 1.0), (505, 1.0), (506, 1.0), (507, 1.0), (508, 2.0), (509, 1.0), (510, 1.0), (511, 1.0), (512, 1.0), (513, 3.0), (514, 1.0), (515, 4.0), (516, 1.0), (517, 3.0), (518, 1.0), (519, 1.0), (520, 1.0), (521, 1.0), (522, 1.0), (523, 2.0), (524, 1.0), (525, 1.0), (526, 1.0), (527, 2.0), (528, 1.0), (529, 1.0), (530, 1.0), (531, 2.0), (532, 1.0), (533, 1.0), (534, 1.0), (535, 3.0), (536, 1.0), (537, 1.0), (538, 1.0), (539, 1.0), (540, 1.0), (541, 1.0), (542, 1.0), (543, 1.0), (544, 3.0), (545, 1.0), (546, 1.0), (547, 2.0), (548, 4.0), (549, 1.0), (550, 1.0), (551, 3.0), (552, 1.0), (553, 3.0), (554, 1.0), (555, 1.0), (556, 5.0), (557, 4.0), (558, 1.0), (559, 1.0), (560, 1.0), (561, 1.0), (562, 4.0), (563, 1.0), (564, 1.0), (565, 2.0), (566, 4.0), (567, 2.0), (568, 1.0), (569, 2.0), (570, 1.0), (571, 1.0), (572, 2.0), (573, 1.0), (574, 2.0), (575, 1.0), (576, 1.0), (577, 1.0), (578, 1.0), (579, 2.0), (580, 4.0), (581, 1.0), (582, 2.0), (583, 1.0), (584, 3.0), (585, 1.0), (586, 1.0), (587, 1.0), (588, 2.0), (589, 1.0), (590, 1.0), (591, 1.0), (592, 2.0), (593, 2.0), (594, 1.0), (595, 1.0), (596, 1.0), (597, 1.0), (598, 1.0), (599, 1.0), (600, 1.0), (601, 1.0), (602, 1.0), (603, 1.0), (604, 1.0), (605, 2.0), (606, 2.0), (607, 1.0), (608, 1.0), (609, 2.0), (610, 1.0), (611, 1.0), (612, 1.0), (613, 1.0), (614, 2.0), (615, 1.0), (616, 1.0), (617, 1.0), (618, 2.0), (619, 1.0), (620, 15.0), (621, 1.0), (622, 5.0), (623, 1.0), (624, 3.0), (625, 2.0), (626, 2.0), (627, 1.0), (628, 2.0), (629, 1.0), (630, 2.0), (631, 1.0), (632, 1.0), (633, 3.0), (634, 1.0), (635, 1.0), (636, 2.0), (637, 1.0), (638, 5.0), (639, 2.0), (640, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(mm_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.Train LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling in gensim is realized via transformations. A transformation is something that takes a corpus and spits out another corpus on output, using corpus_out = transformation_object[corpus_in] syntax. What exactly happens in between is determined by what kind of transformation we will using---Latenet Dirchlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online (multi-pass) LDA training, 10 topics, 4 passes over the supplied corpus of 4000 documents, updating model once every 2000 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : PROGRESS: pass 0, at document #2000/4000\n",
      "INFO : optimized alpha [0.060138345, 0.064541206, 0.07009296, 0.06771941, 0.06987323, 0.05989014, 0.058921017, 0.057302512, 0.064253375, 0.060070235]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #7 (0.057): 0.007*\"member\" + 0.005*\"republic\" + 0.004*\"singer\" + 0.003*\"actress\" + 0.003*\"politician\" + 0.003*\"british\" + 0.003*\"president\" + 0.003*\"french\" + 0.003*\"great\" + 0.003*\"league\"\n",
      "INFO : topic #6 (0.059): 0.006*\"river\" + 0.004*\"league\" + 0.003*\"energy\" + 0.003*\"water\" + 0.003*\"premier\" + 0.003*\"cells\" + 0.003*\"jpg\" + 0.002*\"bridge\" + 0.002*\"example\" + 0.002*\"usa\"\n",
      "INFO : topic #3 (0.068): 0.005*\"actor\" + 0.005*\"tower\" + 0.005*\"mast\" + 0.005*\"transmission\" + 0.004*\"british\" + 0.004*\"actress\" + 0.004*\"singer\" + 0.004*\"politician\" + 0.004*\"german\" + 0.003*\"footballer\"\n",
      "INFO : topic #4 (0.070): 0.008*\"hex\" + 0.007*\"rgb\" + 0.004*\"president\" + 0.004*\"color\" + 0.003*\"german\" + 0.003*\"country\" + 0.003*\"light\" + 0.002*\"kingdom\" + 0.002*\"earth\" + 0.002*\"london\"\n",
      "INFO : topic #2 (0.070): 0.004*\"water\" + 0.004*\"february\" + 0.003*\"music\" + 0.003*\"example\" + 0.002*\"president\" + 0.002*\"things\" + 0.002*\"numbers\" + 0.002*\"actress\" + 0.002*\"food\" + 0.002*\"king\"\n",
      "INFO : topic diff=6.092409, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #4000/4000\n",
      "INFO : optimized alpha [0.057407312, 0.06760561, 0.076491654, 0.08375002, 0.070166744, 0.060061596, 0.056977656, 0.060518183, 0.06324749, 0.05849731]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #6 (0.057): 0.008*\"river\" + 0.004*\"jpg\" + 0.004*\"league\" + 0.003*\"cells\" + 0.003*\"bridge\" + 0.003*\"energy\" + 0.003*\"file\" + 0.003*\"water\" + 0.003*\"blood\" + 0.002*\"live\"\n",
      "INFO : topic #0 (0.057): 0.004*\"bc\" + 0.003*\"language\" + 0.003*\"century\" + 0.003*\"player\" + 0.003*\"february\" + 0.003*\"football\" + 0.003*\"series\" + 0.002*\"movie\" + 0.002*\"television\" + 0.002*\"ball\"\n",
      "INFO : topic #4 (0.070): 0.010*\"rgb\" + 0.010*\"hex\" + 0.005*\"color\" + 0.004*\"president\" + 0.004*\"country\" + 0.003*\"german\" + 0.003*\"blue\" + 0.003*\"green\" + 0.003*\"light\" + 0.003*\"red\"\n",
      "INFO : topic #2 (0.076): 0.006*\"music\" + 0.003*\"water\" + 0.003*\"movie\" + 0.003*\"example\" + 0.002*\"things\" + 0.002*\"february\" + 0.002*\"person\" + 0.002*\"award\" + 0.002*\"movies\" + 0.002*\"way\"\n",
      "INFO : topic #3 (0.084): 0.012*\"actor\" + 0.011*\"politician\" + 0.010*\"singer\" + 0.010*\"actress\" + 0.010*\"german\" + 0.009*\"footballer\" + 0.008*\"french\" + 0.008*\"player\" + 0.008*\"british\" + 0.008*\"writer\"\n",
      "INFO : topic diff=0.752869, rho=0.707107\n",
      "INFO : PROGRESS: pass 1, at document #2000/4000\n",
      "INFO : optimized alpha [0.052257534, 0.06681422, 0.07117991, 0.073176704, 0.062692806, 0.060118333, 0.05307618, 0.05386484, 0.057278674, 0.0570173]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.052): 0.005*\"bc\" + 0.004*\"language\" + 0.003*\"football\" + 0.003*\"ball\" + 0.003*\"century\" + 0.003*\"team\" + 0.003*\"series\" + 0.003*\"player\" + 0.003*\"play\" + 0.003*\"television\"\n",
      "INFO : topic #6 (0.053): 0.008*\"river\" + 0.007*\"league\" + 0.006*\"energy\" + 0.004*\"water\" + 0.004*\"jpg\" + 0.004*\"cells\" + 0.003*\"premier\" + 0.003*\"bridge\" + 0.003*\"chemical\" + 0.003*\"file\"\n",
      "INFO : topic #1 (0.067): 0.004*\"person\" + 0.004*\"water\" + 0.003*\"things\" + 0.003*\"body\" + 0.003*\"example\" + 0.002*\"way\" + 0.002*\"large\" + 0.002*\"food\" + 0.002*\"means\" + 0.002*\"light\"\n",
      "INFO : topic #2 (0.071): 0.007*\"music\" + 0.004*\"water\" + 0.003*\"example\" + 0.003*\"numbers\" + 0.003*\"february\" + 0.003*\"things\" + 0.002*\"movie\" + 0.002*\"award\" + 0.002*\"person\" + 0.002*\"way\"\n",
      "INFO : topic #3 (0.073): 0.012*\"actor\" + 0.010*\"politician\" + 0.010*\"singer\" + 0.009*\"actress\" + 0.009*\"german\" + 0.009*\"footballer\" + 0.008*\"french\" + 0.008*\"british\" + 0.007*\"player\" + 0.007*\"writer\"\n",
      "INFO : topic diff=0.493254, rho=0.500000\n",
      "INFO : PROGRESS: pass 1, at document #4000/4000\n",
      "INFO : optimized alpha [0.05202014, 0.068068795, 0.07550924, 0.08122537, 0.06265918, 0.0613746, 0.05284183, 0.05602725, 0.058615558, 0.05650819]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.052): 0.004*\"series\" + 0.004*\"bc\" + 0.004*\"player\" + 0.003*\"language\" + 0.003*\"team\" + 0.003*\"ball\" + 0.003*\"play\" + 0.003*\"television\" + 0.003*\"played\" + 0.003*\"football\"\n",
      "INFO : topic #6 (0.053): 0.009*\"river\" + 0.006*\"league\" + 0.005*\"jpg\" + 0.005*\"energy\" + 0.004*\"water\" + 0.004*\"cells\" + 0.004*\"bridge\" + 0.003*\"file\" + 0.003*\"live\" + 0.003*\"chemical\"\n",
      "INFO : topic #1 (0.068): 0.006*\"person\" + 0.004*\"body\" + 0.003*\"things\" + 0.003*\"water\" + 0.003*\"blood\" + 0.003*\"example\" + 0.003*\"food\" + 0.002*\"man\" + 0.002*\"way\" + 0.002*\"common\"\n",
      "INFO : topic #2 (0.076): 0.008*\"music\" + 0.003*\"movie\" + 0.003*\"band\" + 0.003*\"album\" + 0.003*\"water\" + 0.003*\"example\" + 0.003*\"award\" + 0.003*\"rock\" + 0.002*\"movies\" + 0.002*\"disney\"\n",
      "INFO : topic #3 (0.081): 0.013*\"actor\" + 0.012*\"politician\" + 0.011*\"singer\" + 0.010*\"actress\" + 0.010*\"german\" + 0.010*\"footballer\" + 0.009*\"french\" + 0.009*\"player\" + 0.008*\"british\" + 0.008*\"writer\"\n",
      "INFO : topic diff=0.444645, rho=0.500000\n",
      "INFO : PROGRESS: pass 2, at document #2000/4000\n",
      "INFO : optimized alpha [0.049405582, 0.06938327, 0.07182479, 0.074246354, 0.058425754, 0.06335193, 0.050816864, 0.052158117, 0.054980736, 0.05701094]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.049): 0.005*\"bc\" + 0.004*\"team\" + 0.004*\"ball\" + 0.004*\"language\" + 0.004*\"football\" + 0.004*\"series\" + 0.004*\"player\" + 0.004*\"play\" + 0.003*\"television\" + 0.003*\"words\"\n",
      "INFO : topic #6 (0.051): 0.009*\"river\" + 0.009*\"league\" + 0.007*\"energy\" + 0.005*\"water\" + 0.005*\"jpg\" + 0.004*\"cells\" + 0.004*\"premier\" + 0.004*\"bridge\" + 0.003*\"file\" + 0.003*\"chemical\"\n",
      "INFO : topic #1 (0.069): 0.005*\"person\" + 0.005*\"water\" + 0.004*\"body\" + 0.004*\"things\" + 0.003*\"example\" + 0.003*\"food\" + 0.003*\"types\" + 0.003*\"common\" + 0.003*\"light\" + 0.002*\"way\"\n",
      "INFO : topic #2 (0.072): 0.009*\"music\" + 0.003*\"movie\" + 0.003*\"example\" + 0.003*\"numbers\" + 0.003*\"award\" + 0.003*\"water\" + 0.003*\"movies\" + 0.002*\"band\" + 0.002*\"things\" + 0.002*\"wrote\"\n",
      "INFO : topic #3 (0.074): 0.012*\"actor\" + 0.011*\"politician\" + 0.010*\"singer\" + 0.010*\"actress\" + 0.010*\"german\" + 0.009*\"footballer\" + 0.008*\"french\" + 0.008*\"player\" + 0.008*\"british\" + 0.008*\"writer\"\n",
      "INFO : topic diff=0.408165, rho=0.447214\n",
      "INFO : PROGRESS: pass 2, at document #4000/4000\n",
      "INFO : optimized alpha [0.050083503, 0.07037908, 0.07614935, 0.08107922, 0.0594684, 0.064894296, 0.0514383, 0.05465994, 0.057048004, 0.056888282]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.050): 0.005*\"player\" + 0.005*\"series\" + 0.004*\"team\" + 0.004*\"game\" + 0.004*\"players\" + 0.004*\"play\" + 0.004*\"ball\" + 0.004*\"television\" + 0.004*\"bc\" + 0.004*\"played\"\n",
      "INFO : topic #6 (0.051): 0.010*\"river\" + 0.008*\"league\" + 0.006*\"energy\" + 0.006*\"jpg\" + 0.005*\"water\" + 0.004*\"cells\" + 0.004*\"file\" + 0.004*\"bridge\" + 0.004*\"live\" + 0.003*\"premier\"\n",
      "INFO : topic #1 (0.070): 0.006*\"person\" + 0.005*\"body\" + 0.004*\"water\" + 0.004*\"things\" + 0.003*\"blood\" + 0.003*\"food\" + 0.003*\"example\" + 0.003*\"common\" + 0.003*\"types\" + 0.002*\"way\"\n",
      "INFO : topic #2 (0.076): 0.010*\"music\" + 0.005*\"band\" + 0.005*\"album\" + 0.004*\"movie\" + 0.003*\"rock\" + 0.003*\"award\" + 0.003*\"songs\" + 0.003*\"movies\" + 0.003*\"song\" + 0.003*\"released\"\n",
      "INFO : topic #3 (0.081): 0.013*\"actor\" + 0.012*\"politician\" + 0.011*\"singer\" + 0.011*\"actress\" + 0.010*\"german\" + 0.010*\"footballer\" + 0.009*\"french\" + 0.009*\"player\" + 0.008*\"writer\" + 0.008*\"british\"\n",
      "INFO : topic diff=0.357890, rho=0.447214\n",
      "INFO : PROGRESS: pass 3, at document #2000/4000\n",
      "INFO : optimized alpha [0.048600525, 0.07247473, 0.07348466, 0.07572757, 0.056576423, 0.0675462, 0.050307404, 0.05223729, 0.054628737, 0.05820134]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.049): 0.005*\"bc\" + 0.005*\"team\" + 0.005*\"player\" + 0.004*\"ball\" + 0.004*\"game\" + 0.004*\"words\" + 0.004*\"players\" + 0.004*\"series\" + 0.004*\"play\" + 0.004*\"language\"\n",
      "INFO : topic #6 (0.050): 0.010*\"river\" + 0.010*\"league\" + 0.007*\"energy\" + 0.006*\"water\" + 0.005*\"jpg\" + 0.005*\"premier\" + 0.004*\"cells\" + 0.004*\"division\" + 0.004*\"bridge\" + 0.004*\"file\"\n",
      "INFO : topic #1 (0.072): 0.006*\"water\" + 0.005*\"person\" + 0.004*\"body\" + 0.004*\"things\" + 0.004*\"example\" + 0.004*\"food\" + 0.003*\"common\" + 0.003*\"types\" + 0.003*\"light\" + 0.003*\"animals\"\n",
      "INFO : topic #2 (0.073): 0.010*\"music\" + 0.004*\"band\" + 0.004*\"movie\" + 0.004*\"album\" + 0.003*\"award\" + 0.003*\"numbers\" + 0.003*\"movies\" + 0.003*\"rock\" + 0.003*\"example\" + 0.003*\"love\"\n",
      "INFO : topic #3 (0.076): 0.013*\"actor\" + 0.011*\"politician\" + 0.011*\"singer\" + 0.011*\"actress\" + 0.010*\"german\" + 0.009*\"footballer\" + 0.009*\"french\" + 0.009*\"player\" + 0.008*\"british\" + 0.008*\"writer\"\n",
      "INFO : topic diff=0.329052, rho=0.408248\n",
      "INFO : PROGRESS: pass 3, at document #4000/4000\n",
      "INFO : optimized alpha [0.04957641, 0.072884664, 0.077622436, 0.082177326, 0.05801936, 0.069282226, 0.051407002, 0.054896213, 0.057097927, 0.05840712]\n",
      "INFO : merging changes from 2000 documents into a model of 4000 documents\n",
      "INFO : topic #0 (0.050): 0.006*\"player\" + 0.005*\"game\" + 0.005*\"team\" + 0.005*\"players\" + 0.005*\"series\" + 0.004*\"play\" + 0.004*\"ball\" + 0.004*\"television\" + 0.004*\"language\" + 0.004*\"played\"\n",
      "INFO : topic #6 (0.051): 0.011*\"river\" + 0.009*\"league\" + 0.006*\"energy\" + 0.006*\"jpg\" + 0.005*\"water\" + 0.004*\"file\" + 0.004*\"live\" + 0.004*\"bridge\" + 0.004*\"cells\" + 0.004*\"premier\"\n",
      "INFO : topic #1 (0.073): 0.006*\"person\" + 0.005*\"water\" + 0.005*\"body\" + 0.004*\"things\" + 0.004*\"food\" + 0.003*\"example\" + 0.003*\"blood\" + 0.003*\"common\" + 0.003*\"types\" + 0.003*\"animals\"\n",
      "INFO : topic #2 (0.078): 0.011*\"music\" + 0.006*\"band\" + 0.006*\"album\" + 0.005*\"movie\" + 0.004*\"rock\" + 0.003*\"song\" + 0.003*\"songs\" + 0.003*\"love\" + 0.003*\"award\" + 0.003*\"released\"\n",
      "INFO : topic #3 (0.082): 0.013*\"actor\" + 0.012*\"politician\" + 0.011*\"singer\" + 0.011*\"actress\" + 0.011*\"german\" + 0.010*\"footballer\" + 0.009*\"french\" + 0.009*\"player\" + 0.009*\"writer\" + 0.009*\"british\"\n",
      "INFO : topic diff=0.282295, rho=0.408248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "#The number of requested latent topics to be extracted from the training corpus.\n",
    "num_topics = 10\n",
    "#Number of documents to be used in each training chunk.\n",
    "chunksize = 2000\n",
    "\n",
    "passes = 4\n",
    "\n",
    "iterations = 400\n",
    "#Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
    "# Don't evaluate model perplexity, takes too much time.\n",
    "eval_every = None \n",
    "\n",
    "# Make a index to word dictionary.\n",
    "# This is only to \"load\" the dictionary.\n",
    "id2word = id2word_wiki\n",
    "#use fewer documents during training, LDA is slow\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)\n",
    "\n",
    "%time lda_model = LdaModel( \\\n",
    "    corpus=clipped_corpus, \\\n",
    "    id2word=id2word, \\\n",
    "    chunksize=chunksize, \\\n",
    "    alpha='auto', \\\n",
    "    eta='auto', \\\n",
    "    iterations=iterations, \\\n",
    "    num_topics=num_topics, \\\n",
    "    passes=passes, \\\n",
    "    eval_every=eval_every \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.050): 0.006*\"player\" + 0.005*\"game\" + 0.005*\"team\" + 0.005*\"players\" + 0.005*\"series\" + 0.004*\"play\" + 0.004*\"ball\" + 0.004*\"television\" + 0.004*\"language\" + 0.004*\"played\"\n",
      "INFO : topic #1 (0.073): 0.006*\"person\" + 0.005*\"water\" + 0.005*\"body\" + 0.004*\"things\" + 0.004*\"food\" + 0.003*\"example\" + 0.003*\"blood\" + 0.003*\"common\" + 0.003*\"types\" + 0.003*\"animals\"\n",
      "INFO : topic #2 (0.078): 0.011*\"music\" + 0.006*\"band\" + 0.006*\"album\" + 0.005*\"movie\" + 0.004*\"rock\" + 0.003*\"song\" + 0.003*\"songs\" + 0.003*\"love\" + 0.003*\"award\" + 0.003*\"released\"\n",
      "INFO : topic #3 (0.082): 0.013*\"actor\" + 0.012*\"politician\" + 0.011*\"singer\" + 0.011*\"actress\" + 0.011*\"german\" + 0.010*\"footballer\" + 0.009*\"french\" + 0.009*\"player\" + 0.009*\"writer\" + 0.009*\"british\"\n",
      "INFO : topic #4 (0.058): 0.016*\"rgb\" + 0.016*\"hex\" + 0.009*\"color\" + 0.005*\"sea\" + 0.005*\"blue\" + 0.004*\"green\" + 0.004*\"red\" + 0.004*\"country\" + 0.004*\"pink\" + 0.004*\"purple\"\n",
      "INFO : topic #5 (0.069): 0.007*\"country\" + 0.005*\"countries\" + 0.005*\"god\" + 0.005*\"government\" + 0.005*\"capital\" + 0.004*\"church\" + 0.004*\"century\" + 0.004*\"king\" + 0.004*\"important\" + 0.004*\"language\"\n",
      "INFO : topic #6 (0.051): 0.011*\"river\" + 0.009*\"league\" + 0.006*\"energy\" + 0.006*\"jpg\" + 0.005*\"water\" + 0.004*\"file\" + 0.004*\"live\" + 0.004*\"bridge\" + 0.004*\"cells\" + 0.004*\"premier\"\n",
      "INFO : topic #7 (0.055): 0.009*\"republic\" + 0.007*\"member\" + 0.006*\"great\" + 0.006*\"island\" + 0.005*\"kingdom\" + 0.005*\"islands\" + 0.004*\"york\" + 0.004*\"president\" + 0.004*\"country\" + 0.004*\"british\"\n",
      "INFO : topic #8 (0.057): 0.009*\"president\" + 0.006*\"tower\" + 0.005*\"game\" + 0.005*\"games\" + 0.004*\"mario\" + 0.004*\"transmission\" + 0.004*\"mast\" + 0.004*\"bush\" + 0.003*\"reagan\" + 0.003*\"released\"\n",
      "INFO : topic #9 (0.058): 0.005*\"earth\" + 0.005*\"windows\" + 0.003*\"internet\" + 0.003*\"language\" + 0.003*\"means\" + 0.003*\"microsoft\" + 0.003*\"light\" + 0.003*\"soviet\" + 0.003*\"china\" + 0.003*\"space\"\n"
     ]
    }
   ],
   "source": [
    "#print a few most important words for each LDA topic\n",
    "_ = lda_model.print_topics(num_topics=20, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO : CorpusAccumulator accumulated stats from 4000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.9080.\n",
      "[([(0.013439952, 'actor'),\n",
      "   (0.012421263, 'politician'),\n",
      "   (0.011165031, 'singer'),\n",
      "   (0.010921399, 'actress'),\n",
      "   (0.010623863, 'german'),\n",
      "   (0.010118966, 'footballer'),\n",
      "   (0.009358997, 'french'),\n",
      "   (0.009270418, 'player'),\n",
      "   (0.008643068, 'writer'),\n",
      "   (0.008608789, 'british'),\n",
      "   (0.0067787846, 'italian'),\n",
      "   (0.006636178, 'president'),\n",
      "   (0.004791825, 'composer'),\n",
      "   (0.004776608, 'musician'),\n",
      "   (0.004631176, 'minister'),\n",
      "   (0.0045789587, 'canadian'),\n",
      "   (0.004396165, 'russian'),\n",
      "   (0.0041882666, 'prime'),\n",
      "   (0.00416974, 'director'),\n",
      "   (0.0039325184, 'japanese')],\n",
      "  -0.3364554973149783),\n",
      " ([(0.009199669, 'republic'),\n",
      "   (0.0072313575, 'member'),\n",
      "   (0.0060036173, 'great'),\n",
      "   (0.005561149, 'island'),\n",
      "   (0.004916716, 'kingdom'),\n",
      "   (0.0048644454, 'islands'),\n",
      "   (0.004329085, 'york'),\n",
      "   (0.004258775, 'president'),\n",
      "   (0.0042336173, 'country'),\n",
      "   (0.0042253393, 'british'),\n",
      "   (0.0040855804, 'france'),\n",
      "   (0.004038063, 'french'),\n",
      "   (0.0038948997, 'africa'),\n",
      "   (0.0037614952, 'party'),\n",
      "   (0.0037432504, 'independence'),\n",
      "   (0.0035786093, 'england'),\n",
      "   (0.0035173753, 'union'),\n",
      "   (0.0030794106, 'australia'),\n",
      "   (0.0030545252, 'vocals'),\n",
      "   (0.00293322, 'football')],\n",
      "  -1.308559553654703),\n",
      " ([(0.007056556, 'country'),\n",
      "   (0.0054062353, 'countries'),\n",
      "   (0.0053597875, 'god'),\n",
      "   (0.0049960995, 'government'),\n",
      "   (0.0046490184, 'capital'),\n",
      "   (0.0043902975, 'church'),\n",
      "   (0.004325289, 'century'),\n",
      "   (0.0037624566, 'king'),\n",
      "   (0.0036918274, 'important'),\n",
      "   (0.0035189306, 'language'),\n",
      "   (0.0033293774, 'empire'),\n",
      "   (0.0032475586, 'rural'),\n",
      "   (0.0032024356, 'roman'),\n",
      "   (0.003070592, 'lake'),\n",
      "   (0.0030379384, 'land'),\n",
      "   (0.0029374622, 'said'),\n",
      "   (0.0029092242, 'europe'),\n",
      "   (0.0027968031, 'population'),\n",
      "   (0.0027883626, 'cities'),\n",
      "   (0.0027212717, 'east')],\n",
      "  -1.5018515754273656),\n",
      " ([(0.0061413324, 'person'),\n",
      "   (0.0052204966, 'water'),\n",
      "   (0.005009085, 'body'),\n",
      "   (0.004173618, 'things'),\n",
      "   (0.0035655613, 'food'),\n",
      "   (0.0033975316, 'example'),\n",
      "   (0.0033896288, 'blood'),\n",
      "   (0.003036472, 'common'),\n",
      "   (0.0028157781, 'types'),\n",
      "   (0.0026851948, 'animals'),\n",
      "   (0.0025719274, 'human'),\n",
      "   (0.0025284132, 'makes'),\n",
      "   (0.0025150077, 'way'),\n",
      "   (0.0024619075, 'disease'),\n",
      "   (0.0024203805, 'means'),\n",
      "   (0.0023913665, 'large'),\n",
      "   (0.0022609008, 'species'),\n",
      "   (0.002245397, 'cause'),\n",
      "   (0.0022092485, 'light'),\n",
      "   (0.002197366, 'women')],\n",
      "  -1.5918380338142095),\n",
      " ([(0.010527649, 'music'),\n",
      "   (0.0062764254, 'band'),\n",
      "   (0.0059274063, 'album'),\n",
      "   (0.00453371, 'movie'),\n",
      "   (0.0038562892, 'rock'),\n",
      "   (0.0033829699, 'song'),\n",
      "   (0.0033654852, 'songs'),\n",
      "   (0.003338896, 'love'),\n",
      "   (0.0032893226, 'award'),\n",
      "   (0.0032750375, 'released'),\n",
      "   (0.0032440943, 'movies'),\n",
      "   (0.0028150894, 'guitar'),\n",
      "   (0.0028083876, 'film'),\n",
      "   (0.0026483547, 'popular'),\n",
      "   (0.002625937, 'wrote'),\n",
      "   (0.002566349, 'famous'),\n",
      "   (0.002529883, 'disney'),\n",
      "   (0.0022831727, 'man'),\n",
      "   (0.0022269874, 'story'),\n",
      "   (0.0022263506, 'example')],\n",
      "  -1.6331817552152121),\n",
      " ([(0.0050098337, 'earth'),\n",
      "   (0.0048621427, 'windows'),\n",
      "   (0.0033343374, 'internet'),\n",
      "   (0.0033094198, 'language'),\n",
      "   (0.003123829, 'means'),\n",
      "   (0.0030035507, 'microsoft'),\n",
      "   (0.0029317837, 'light'),\n",
      "   (0.0027259572, 'soviet'),\n",
      "   (0.0026774916, 'china'),\n",
      "   (0.0026533552, 'space'),\n",
      "   (0.0025817761, 'software'),\n",
      "   (0.0025734638, 'way'),\n",
      "   (0.0025503638, 'example'),\n",
      "   (0.0024852315, 'sun'),\n",
      "   (0.002463158, 'countries'),\n",
      "   (0.0024279556, 'version'),\n",
      "   (0.0022233096, 'union'),\n",
      "   (0.0021835417, 'government'),\n",
      "   (0.0021822152, 'speed'),\n",
      "   (0.002160738, 'force')],\n",
      "  -1.7662895153587579),\n",
      " ([(0.005697058, 'player'),\n",
      "   (0.0053485148, 'game'),\n",
      "   (0.004874705, 'team'),\n",
      "   (0.004868727, 'players'),\n",
      "   (0.0047603417, 'series'),\n",
      "   (0.004381966, 'play'),\n",
      "   (0.00408579, 'ball'),\n",
      "   (0.0040041134, 'television'),\n",
      "   (0.0038898138, 'language'),\n",
      "   (0.0038092334, 'played'),\n",
      "   (0.0037609253, 'words'),\n",
      "   (0.003630776, 'bc'),\n",
      "   (0.0035752107, 'episode'),\n",
      "   (0.003456341, 'football'),\n",
      "   (0.0029693956, 'teams'),\n",
      "   (0.0028604034, 'season'),\n",
      "   (0.002656576, 'word'),\n",
      "   (0.0026182127, 'games'),\n",
      "   (0.0024856857, 'characters'),\n",
      "   (0.0024804722, 'award')],\n",
      "  -2.1347018819046437),\n",
      " ([(0.010852014, 'river'),\n",
      "   (0.008867515, 'league'),\n",
      "   (0.0058885626, 'energy'),\n",
      "   (0.0058301366, 'jpg'),\n",
      "   (0.005379828, 'water'),\n",
      "   (0.0040040947, 'file'),\n",
      "   (0.003994428, 'live'),\n",
      "   (0.0039024714, 'bridge'),\n",
      "   (0.0036065446, 'cells'),\n",
      "   (0.0035687666, 'premier'),\n",
      "   (0.0034423973, 'division'),\n",
      "   (0.0034246766, 'chicago'),\n",
      "   (0.0033922375, 'largest'),\n",
      "   (0.0032169889, 'species'),\n",
      "   (0.003044584, 'kansas'),\n",
      "   (0.002550136, 'population'),\n",
      "   (0.002499886, 'club'),\n",
      "   (0.00242789, 'chemical'),\n",
      "   (0.0024126484, 'million'),\n",
      "   (0.0023622564, 'usa')],\n",
      "  -2.573225812803742),\n",
      " ([(0.009456007, 'president'),\n",
      "   (0.0059797037, 'tower'),\n",
      "   (0.005253497, 'game'),\n",
      "   (0.00499079, 'games'),\n",
      "   (0.0040664016, 'mario'),\n",
      "   (0.0039425013, 'transmission'),\n",
      "   (0.003939759, 'mast'),\n",
      "   (0.003643667, 'bush'),\n",
      "   (0.0034454227, 'reagan'),\n",
      "   (0.003422777, 'released'),\n",
      "   (0.0026848642, 'party'),\n",
      "   (0.0025906255, 'jackson'),\n",
      "   (0.0025790895, 'children'),\n",
      "   (0.0025420818, 'video'),\n",
      "   (0.0024090228, 'married'),\n",
      "   (0.0024051855, 'nintendo'),\n",
      "   (0.0023746123, 'house'),\n",
      "   (0.0023404786, 'album'),\n",
      "   (0.0023184598, 'election'),\n",
      "   (0.0021217528, 'government')],\n",
      "  -2.6063854844208354),\n",
      " ([(0.015887044, 'rgb'),\n",
      "   (0.01570111, 'hex'),\n",
      "   (0.008857144, 'color'),\n",
      "   (0.0049940418, 'sea'),\n",
      "   (0.004644485, 'blue'),\n",
      "   (0.0043975976, 'green'),\n",
      "   (0.0042946455, 'red'),\n",
      "   (0.004266856, 'country'),\n",
      "   (0.0036228097, 'pink'),\n",
      "   (0.0035540063, 'purple'),\n",
      "   (0.0033663313, 'london'),\n",
      "   (0.003317715, 'al'),\n",
      "   (0.0031443762, 'web'),\n",
      "   (0.0030418057, 'light'),\n",
      "   (0.0029523163, 'ocean'),\n",
      "   (0.0028740244, 'ff'),\n",
      "   (0.0026298643, 'east'),\n",
      "   (0.0025788916, 'san'),\n",
      "   (0.002558332, 'coffee'),\n",
      "   (0.0024707129, 'px')],\n",
      "  -3.627867965094378)]\n"
     ]
    }
   ],
   "source": [
    "#Get the topics with the highest coherence score the coherence for each topic.\n",
    "top_topics = lda_model.top_topics(clipped_corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_1 = 'https://en.wikipedia.org/wiki/Jean_Baptiste_Fran%C3%A7ois_Pitra'\n",
    "url_2 = 'https://en.wikipedia.org/wiki/Richard_Speiser'\n",
    "url_3 = 'https://en.wikipedia.org/wiki/Siale_Piutau'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This https://en.wikipedia.org/wiki/Jean_Baptiste_Fran%C3%A7ois_Pitra link have topic possibility : \n",
      "[(0, 0.07736982), (1, 0.01857841), (2, 0.09405043), (3, 0.0623004), (4, 0.03790229), (5, 0.032031614), (6, 0.081969656), (8, 0.063751094), (9, 0.5320338)]\n"
     ]
    }
   ],
   "source": [
    "url_text(url_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This https://en.wikipedia.org/wiki/Richard_Speiser link have topic possibility : \n",
      "[(0, 0.1151802), (1, 0.024985855), (2, 0.040951565), (3, 0.013642275), (4, 0.039383303), (6, 0.12123422), (8, 0.07866426), (9, 0.56592315)]\n"
     ]
    }
   ],
   "source": [
    "url_text(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This https://en.wikipedia.org/wiki/Siale_Piutau link have topic possibility : \n",
      "[(0, 0.18130021), (2, 0.0805679), (3, 0.019114021), (4, 0.11154415), (6, 0.07292845), (8, 0.086495854), (9, 0.44282055)]\n"
     ]
    }
   ],
   "source": [
    "url_text(url_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "This https://en.wikipedia.org/wiki/Siale_Piutau link have topic possibility : \n",
      "[(0, 0.18130015), (2, 0.08056819), (3, 0.019114044), (4, 0.11154427), (6, 0.072928496), (8, 0.0864959), (9, 0.44282058)]\n"
     ]
    }
   ],
   "source": [
    "url_test = 'https://en.wikipedia.org/wiki/Siale_Piutau'\n",
    "response = requests.get(url_test)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#print(response.content)\n",
    "#print(soup)\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "    \n",
    "test_tokens = tokenize(response.text)\n",
    "print(type(test_tokens))\n",
    "#test_doc = preprocess_text(test_tokens)\n",
    "#bow_vector_1 = id2word_wiki.doc2bow(tokenize(text))\n",
    "bow_test_doc = id2word_wiki.doc2bow(test_tokens)\n",
    "\n",
    "print(\"This {} link have topic possibility : \\n{}\".format(url_test, lda_model.get_document_topics(bow_test_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : PROGRESS: processing document #10000\n",
      "INFO : PROGRESS: processing document #20000\n",
      "INFO : PROGRESS: processing document #30000\n",
      "INFO : PROGRESS: processing document #40000\n",
      "INFO : PROGRESS: processing document #50000\n",
      "INFO : PROGRESS: processing document #60000\n",
      "INFO : PROGRESS: processing document #70000\n",
      "INFO : calculating IDF weights for 74654 documents and 34462 features (7169754 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using serial LSI version on this node\n",
      "INFO : updating model with new documents\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (34462, 300) action matrix\n",
      "INFO : orthonormalizing (34462, 300) action matrix\n",
      "INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 15.066% of energy spectrum)\n",
      "INFO : processed documents up to #20000\n",
      "INFO : topic #0(15.852): -0.239*\"footballer\" + -0.223*\"politician\" + -0.218*\"actor\" + -0.200*\"actress\" + -0.188*\"german\" + -0.183*\"singer\" + -0.162*\"french\" + -0.159*\"writer\" + -0.154*\"player\" + -0.136*\"british\"\n",
      "INFO : topic #1(10.687): -0.197*\"footballer\" + -0.176*\"politician\" + -0.154*\"actor\" + 0.150*\"music\" + -0.146*\"actress\" + -0.116*\"singer\" + -0.104*\"writer\" + -0.089*\"player\" + 0.082*\"band\" + -0.079*\"italian\"\n",
      "INFO : topic #2(8.372): 0.297*\"district\" + -0.284*\"music\" + 0.249*\"coat\" + 0.235*\"arms\" + -0.201*\"band\" + -0.188*\"album\" + 0.168*\"municipalities\" + 0.147*\"county\" + 0.144*\"river\" + 0.132*\"towns\"\n",
      "INFO : topic #3(7.767): -0.343*\"district\" + -0.320*\"coat\" + -0.301*\"arms\" + 0.204*\"king\" + -0.196*\"municipalities\" + -0.180*\"music\" + -0.168*\"band\" + -0.154*\"album\" + -0.141*\"towns\" + 0.123*\"emperor\"\n",
      "INFO : topic #4(7.482): 0.270*\"music\" + 0.266*\"king\" + 0.145*\"emperor\" + 0.131*\"band\" + 0.130*\"england\" + 0.116*\"album\" + 0.113*\"pope\" + 0.110*\"ii\" + 0.108*\"church\" + -0.102*\"water\"\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (34462, 300) action matrix\n",
      "INFO : orthonormalizing (34462, 300) action matrix\n",
      "INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 13.933% of energy spectrum)\n",
      "INFO : merging projections: (34462, 200) + (34462, 200)\n",
      "INFO : keeping 200 factors (discarding 13.250% of energy spectrum)\n",
      "INFO : processed documents up to #40000\n",
      "INFO : topic #0(18.522): 0.143*\"actor\" + 0.139*\"footballer\" + 0.133*\"politician\" + 0.133*\"german\" + 0.129*\"actress\" + 0.124*\"singer\" + 0.122*\"player\" + 0.115*\"french\" + 0.109*\"british\" + 0.103*\"writer\"\n",
      "INFO : topic #1(13.126): 0.243*\"footballer\" + 0.229*\"politician\" + 0.209*\"actor\" + 0.193*\"actress\" + 0.149*\"german\" + -0.148*\"album\" + 0.148*\"writer\" + 0.148*\"singer\" + 0.127*\"french\" + 0.122*\"italian\"\n",
      "INFO : topic #2(12.759): -0.565*\"league\" + -0.239*\"football\" + -0.223*\"team\" + -0.188*\"nhl\" + -0.175*\"club\" + -0.163*\"premier\" + -0.160*\"division\" + -0.152*\"cup\" + -0.135*\"hockey\" + -0.129*\"played\"\n",
      "INFO : topic #3(11.857): -0.356*\"album\" + -0.279*\"band\" + -0.204*\"music\" + 0.203*\"county\" + -0.182*\"song\" + -0.166*\"released\" + -0.142*\"guitar\" + -0.140*\"albums\" + -0.119*\"chart\" + -0.117*\"songs\"\n",
      "INFO : topic #4(10.404): -0.742*\"county\" + -0.152*\"album\" + -0.135*\"district\" + -0.123*\"band\" + -0.119*\"river\" + -0.113*\"counties\" + -0.100*\"town\" + -0.096*\"towns\" + 0.082*\"hurricane\" + -0.081*\"province\"\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (34462, 300) action matrix\n",
      "INFO : orthonormalizing (34462, 300) action matrix\n",
      "INFO : 2nd phase: running dense svd on (300, 20000) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 14.836% of energy spectrum)\n",
      "INFO : merging projections: (34462, 200) + (34462, 200)\n",
      "INFO : keeping 200 factors (discarding 11.766% of energy spectrum)\n",
      "INFO : processed documents up to #60000\n",
      "INFO : topic #0(21.720): 0.125*\"actor\" + 0.111*\"album\" + 0.109*\"movie\" + 0.107*\"actress\" + 0.103*\"player\" + 0.103*\"singer\" + 0.101*\"music\" + 0.099*\"politician\" + 0.098*\"german\" + 0.091*\"british\"\n",
      "INFO : topic #1(15.108): -0.412*\"album\" + -0.267*\"band\" + -0.215*\"released\" + -0.212*\"song\" + 0.210*\"league\" + -0.182*\"chart\" + -0.180*\"music\" + -0.179*\"albums\" + -0.142*\"guitar\" + -0.132*\"vocals\"\n",
      "INFO : topic #2(14.788): -0.374*\"league\" + -0.233*\"nhl\" + -0.225*\"team\" + -0.178*\"championship\" + -0.175*\"hockey\" + -0.174*\"played\" + -0.164*\"football\" + -0.142*\"cup\" + -0.134*\"album\" + -0.125*\"season\"\n",
      "INFO : topic #3(14.255): -0.225*\"actor\" + 0.210*\"river\" + -0.200*\"actress\" + -0.191*\"politician\" + -0.190*\"footballer\" + -0.167*\"singer\" + 0.144*\"county\" + 0.135*\"jpg\" + -0.130*\"writer\" + -0.114*\"german\"\n",
      "INFO : topic #4(12.677): 0.371*\"championship\" + 0.358*\"wrestling\" + 0.344*\"match\" + 0.316*\"wwe\" + 0.250*\"defeated\" + 0.197*\"tag\" + -0.195*\"nhl\" + -0.179*\"league\" + -0.140*\"hockey\" + 0.136*\"heavyweight\"\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (34462, 300) action matrix\n",
      "INFO : orthonormalizing (34462, 300) action matrix\n",
      "INFO : 2nd phase: running dense svd on (300, 14654) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 200 factors (discarding 14.107% of energy spectrum)\n",
      "INFO : merging projections: (34462, 200) + (34462, 200)\n",
      "INFO : keeping 200 factors (discarding 9.919% of energy spectrum)\n",
      "INFO : processed documents up to #74654\n",
      "INFO : topic #0(23.754): 0.118*\"actor\" + 0.116*\"movie\" + 0.103*\"album\" + 0.102*\"politician\" + 0.100*\"actress\" + 0.096*\"player\" + 0.095*\"music\" + 0.095*\"singer\" + 0.094*\"president\" + 0.088*\"german\"\n",
      "INFO : topic #1(16.328): -0.378*\"album\" + 0.245*\"county\" + -0.221*\"song\" + -0.220*\"band\" + -0.212*\"released\" + -0.178*\"chart\" + -0.174*\"music\" + -0.163*\"albums\" + 0.137*\"river\" + 0.118*\"district\"\n",
      "INFO : topic #2(15.957): -0.331*\"league\" + -0.196*\"team\" + -0.181*\"football\" + 0.181*\"county\" + -0.172*\"nhl\" + -0.157*\"hockey\" + -0.152*\"player\" + -0.150*\"played\" + -0.144*\"cup\" + 0.138*\"album\"\n",
      "INFO : topic #3(15.528): -0.271*\"league\" + 0.187*\"politician\" + 0.181*\"actor\" + -0.171*\"team\" + -0.165*\"template\" + -0.157*\"nhl\" + 0.156*\"actress\" + 0.131*\"footballer\" + -0.126*\"county\" + 0.124*\"president\"\n",
      "INFO : topic #4(14.498): -0.667*\"county\" + 0.292*\"template\" + 0.155*\"text\" + 0.130*\"bar\" + -0.125*\"album\" + 0.120*\"parameter\" + -0.115*\"district\" + -0.103*\"kansas\" + -0.103*\"census\" + -0.093*\"river\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "#the LSI transformation goes from a space of high dimensionality (TFIDF, tens of thousands) in to a space of low dimensionality(200)\n",
    "#For this reason it can also seen as dimensinality reduction\n",
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki,num_topics=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to D:/Downloads/36_wiki_tfidf.mm\n",
      "INFO : saving sparse matrix to D:/Downloads/36_wiki_tfidf.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : PROGRESS: saving document #26000\n",
      "INFO : PROGRESS: saving document #27000\n",
      "INFO : PROGRESS: saving document #28000\n",
      "INFO : PROGRESS: saving document #29000\n",
      "INFO : PROGRESS: saving document #30000\n",
      "INFO : PROGRESS: saving document #31000\n",
      "INFO : PROGRESS: saving document #32000\n",
      "INFO : PROGRESS: saving document #33000\n",
      "INFO : PROGRESS: saving document #34000\n",
      "INFO : PROGRESS: saving document #35000\n",
      "INFO : PROGRESS: saving document #36000\n",
      "INFO : PROGRESS: saving document #37000\n",
      "INFO : PROGRESS: saving document #38000\n",
      "INFO : PROGRESS: saving document #39000\n",
      "INFO : PROGRESS: saving document #40000\n",
      "INFO : PROGRESS: saving document #41000\n",
      "INFO : PROGRESS: saving document #42000\n",
      "INFO : PROGRESS: saving document #43000\n",
      "INFO : PROGRESS: saving document #44000\n",
      "INFO : PROGRESS: saving document #45000\n",
      "INFO : PROGRESS: saving document #46000\n",
      "INFO : PROGRESS: saving document #47000\n",
      "INFO : PROGRESS: saving document #48000\n",
      "INFO : PROGRESS: saving document #49000\n",
      "INFO : PROGRESS: saving document #50000\n",
      "INFO : PROGRESS: saving document #51000\n",
      "INFO : PROGRESS: saving document #52000\n",
      "INFO : PROGRESS: saving document #53000\n",
      "INFO : PROGRESS: saving document #54000\n",
      "INFO : PROGRESS: saving document #55000\n",
      "INFO : PROGRESS: saving document #56000\n",
      "INFO : PROGRESS: saving document #57000\n",
      "INFO : PROGRESS: saving document #58000\n",
      "INFO : PROGRESS: saving document #59000\n",
      "INFO : PROGRESS: saving document #60000\n",
      "INFO : PROGRESS: saving document #61000\n",
      "INFO : PROGRESS: saving document #62000\n",
      "INFO : PROGRESS: saving document #63000\n",
      "INFO : PROGRESS: saving document #64000\n",
      "INFO : PROGRESS: saving document #65000\n",
      "INFO : PROGRESS: saving document #66000\n",
      "INFO : PROGRESS: saving document #67000\n",
      "INFO : PROGRESS: saving document #68000\n",
      "INFO : PROGRESS: saving document #69000\n",
      "INFO : PROGRESS: saving document #70000\n",
      "INFO : PROGRESS: saving document #71000\n",
      "INFO : PROGRESS: saving document #72000\n",
      "INFO : PROGRESS: saving document #73000\n",
      "INFO : PROGRESS: saving document #74000\n",
      "INFO : saved 74654x34462 matrix, density=0.279% (7169754/2572726148)\n",
      "INFO : saving MmCorpus index to D:/Downloads/36_wiki_tfidf.mm.index\n",
      "INFO : storing corpus in Matrix Market format to D:/Downloads/37_wiki_lsa.mm\n",
      "INFO : saving sparse matrix to D:/Downloads/37_wiki_lsa.mm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : PROGRESS: saving document #5000\n",
      "INFO : PROGRESS: saving document #6000\n",
      "INFO : PROGRESS: saving document #7000\n",
      "INFO : PROGRESS: saving document #8000\n",
      "INFO : PROGRESS: saving document #9000\n",
      "INFO : PROGRESS: saving document #10000\n",
      "INFO : PROGRESS: saving document #11000\n",
      "INFO : PROGRESS: saving document #12000\n",
      "INFO : PROGRESS: saving document #13000\n",
      "INFO : PROGRESS: saving document #14000\n",
      "INFO : PROGRESS: saving document #15000\n",
      "INFO : PROGRESS: saving document #16000\n",
      "INFO : PROGRESS: saving document #17000\n",
      "INFO : PROGRESS: saving document #18000\n",
      "INFO : PROGRESS: saving document #19000\n",
      "INFO : PROGRESS: saving document #20000\n",
      "INFO : PROGRESS: saving document #21000\n",
      "INFO : PROGRESS: saving document #22000\n",
      "INFO : PROGRESS: saving document #23000\n",
      "INFO : PROGRESS: saving document #24000\n",
      "INFO : PROGRESS: saving document #25000\n",
      "INFO : PROGRESS: saving document #26000\n",
      "INFO : PROGRESS: saving document #27000\n",
      "INFO : PROGRESS: saving document #28000\n",
      "INFO : PROGRESS: saving document #29000\n",
      "INFO : PROGRESS: saving document #30000\n",
      "INFO : PROGRESS: saving document #31000\n",
      "INFO : PROGRESS: saving document #32000\n",
      "INFO : PROGRESS: saving document #33000\n",
      "INFO : PROGRESS: saving document #34000\n",
      "INFO : PROGRESS: saving document #35000\n",
      "INFO : PROGRESS: saving document #36000\n",
      "INFO : PROGRESS: saving document #37000\n",
      "INFO : PROGRESS: saving document #38000\n",
      "INFO : PROGRESS: saving document #39000\n",
      "INFO : PROGRESS: saving document #40000\n",
      "INFO : PROGRESS: saving document #41000\n",
      "INFO : PROGRESS: saving document #42000\n",
      "INFO : PROGRESS: saving document #43000\n",
      "INFO : PROGRESS: saving document #44000\n",
      "INFO : PROGRESS: saving document #45000\n",
      "INFO : PROGRESS: saving document #46000\n",
      "INFO : PROGRESS: saving document #47000\n",
      "INFO : PROGRESS: saving document #48000\n",
      "INFO : PROGRESS: saving document #49000\n",
      "INFO : PROGRESS: saving document #50000\n",
      "INFO : PROGRESS: saving document #51000\n",
      "INFO : PROGRESS: saving document #52000\n",
      "INFO : PROGRESS: saving document #53000\n",
      "INFO : PROGRESS: saving document #54000\n",
      "INFO : PROGRESS: saving document #55000\n",
      "INFO : PROGRESS: saving document #56000\n",
      "INFO : PROGRESS: saving document #57000\n",
      "INFO : PROGRESS: saving document #58000\n",
      "INFO : PROGRESS: saving document #59000\n",
      "INFO : PROGRESS: saving document #60000\n",
      "INFO : PROGRESS: saving document #61000\n",
      "INFO : PROGRESS: saving document #62000\n",
      "INFO : PROGRESS: saving document #63000\n",
      "INFO : PROGRESS: saving document #64000\n",
      "INFO : PROGRESS: saving document #65000\n",
      "INFO : PROGRESS: saving document #66000\n",
      "INFO : PROGRESS: saving document #67000\n",
      "INFO : PROGRESS: saving document #68000\n",
      "INFO : PROGRESS: saving document #69000\n",
      "INFO : PROGRESS: saving document #70000\n",
      "INFO : PROGRESS: saving document #71000\n",
      "INFO : PROGRESS: saving document #72000\n",
      "INFO : PROGRESS: saving document #73000\n",
      "INFO : PROGRESS: saving document #74000\n",
      "INFO : saved 74654x200 matrix, density=100.000% (14930800/14930800)\n",
      "INFO : saving MmCorpus index to D:/Downloads/37_wiki_lsa.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "#we can store this \"LSA via TFIDF via bag-of-words\" corpus the same way again:\n",
    "%time gensim.corpora.MmCorpus.serialize('D:/Downloads/36_wiki_tfidf.mm', tfidf_model[mm_corpus])\n",
    "%time gensim.corpora.MmCorpus.serialize('D:/Downloads/37_wiki_lsa.mm', lsi_model[tfidf_model[mm_corpus]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('normally', 1), ('blood', 2), ('produced', 1), ('cell', 2)]\n"
     ]
    }
   ],
   "source": [
    "text = 'A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood'\n",
    "\n",
    "#transform text into the bag-of-words space\n",
    "bow_vector_1 = id2word_wiki.doc2bow(tokenize(text))\n",
    "print([(id2word_wiki[id], count) for id, count in bow_vector_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.91577274), (2, 0.0117065795), (3, 0.012393454), (5, 0.010448696)]\n",
      "0.006*\"person\" + 0.005*\"water\" + 0.005*\"body\" + 0.004*\"things\" + 0.004*\"food\" + 0.003*\"example\" + 0.003*\"blood\" + 0.003*\"common\" + 0.003*\"types\" + 0.003*\"animals\"\n"
     ]
    }
   ],
   "source": [
    "#transfrom into LDA space\n",
    "lda_vector_1 = lda_model[bow_vector_1]\n",
    "print(lda_vector_1)\n",
    "print(lda_model.print_topic(max(lda_vector_1,key=lambda item:item[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# store all trained models to disk\n",
    "lda_model.save('./data/lda_wiki.model')\n",
    "lsi_model.save('./data/lsi_wiki.model')\n",
    "tfidf_model.save('./data/tfidf_wiki.model')\n",
    "id2word_wiki.save('./data/wiki.dictionary')\n",
    "# load the same model back; the result is equal to `lda_model`\n",
    "same_lda_model = gensim.models.LdaModel.load('./data/lda_wiki.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : adding document #10000 to Dictionary(163649 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #20000 to Dictionary(241250 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #30000 to Dictionary(302107 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #40000 to Dictionary(365107 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #50000 to Dictionary(429710 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #60000 to Dictionary(463634 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : adding document #70000 to Dictionary(522621 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...)\n",
      "INFO : built Dictionary(547170 unique tokens: ['abdicated', 'abdicates', 'abraham', 'additionally', 'adolf']...) from 74654 documents (total 15921985 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 2s\n"
     ]
    }
   ],
   "source": [
    "doc_stream = (tokens for _, tokens in iter_wiki('D:/Downloads/31_simplewiki-20200301-pages-articles.xml.bz2', \\\n",
    "                                                'Wikipedia Category File Portal template MediaWiki User Help Book Draft'))\n",
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(293, 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = \"A blood cell, also called a hematocyte, is a cell produced by hematopoiesis and normally found in blood.\"\n",
    "doc_2='human human huma'\n",
    "bow = id2word_wiki.doc2bow(tokenize(doc_2))\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isonames\n"
     ]
    }
   ],
   "source": [
    "print(id2word_wiki[34461])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gensim.corpora.wikicorpus.extract_pages(f, filter_namespaces=False, filter_articles=None)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bz2.BZ2File at 0x25866c24e10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_or_filename(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-e5eaf8fab7b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtitle_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpageid_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwikicorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "title_1, text_1, pageid_1 = gensim.corpora.wikicorpus.extract_pages(smart_open(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_func = gensim.corpora.wikicorpus.extract_pages(file_or_filename(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, generator found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-10ad054a81c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwikicorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\wikicorpus.py\u001b[0m in \u001b[0;36mfilter_wiki\u001b[1;34m(raw, promote_remaining, simplify_links)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;31m# parsing of the wiki markup is not perfect, but sufficient for our purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;31m# contributions to improving this code are welcome :)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_htmlentities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# '&amp;nbsp;' --> '\\xa0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mremove_markup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpromote_remaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimplify_links\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, generator found"
     ]
    }
   ],
   "source": [
    "print(gensim.corpora.wikicorpus.filter_wiki(filter_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function gensim.corpora.wikicorpus.extract_pages(f, filter_namespaces=False, filter_articles=None)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Defination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
