{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Paper Reading\n",
    "- Pick one of the following papers, understand its key ideas, and replicate the experiments using the datasets described in the paper or your own dataset.\n",
    "    - Convolutional Neural Network Architectures for Matching Natural Language Sentences, https://arxiv.org/pdf/1503.03244.pdf\n",
    "    - Matching Networks for One Shot Learning , https://arxiv.org/pdf/1606.04080v2.pdf\n",
    "    - Prototypical Networks for Few-shot Learning, https://arxiv.org/pdf/1703.05175v2.pdf\n",
    "- Requirements:\n",
    "    - Write a report summarizing the key idea, your experiment results, and your analysis and critique\n",
    "    - Give a 5-10 min oral presentation in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Siamese CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Assignment, let's use CNN to detect duplicate sentences. Two datasets have been prepared for you: train.csv and test.csv. Both files are in the following format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|question1 | question2 |is_duplicate|\n",
    "|------|------|-------|\n",
    "|How do you take a screenshot on a Mac laptop?|  How do I take a screenshot on my MacBook Pro? ...|   1 |\n",
    "|Is the US election rigged?|  Was the US election rigged?|   1 |\n",
    "|How scary is it to drive on the road to Hana g...|  Do I need a four-wheel-drive car to drive all ...\t|  0  |\n",
    "|...|...| ...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions below to clasify the sentence pairs in the training dataset and then test the model using test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define a **class** called \"text_processor\" to preporcess text:\n",
    "- first create **\"\\_\\_init\\_\\_\"** function:\n",
    "    - set the *class attributes*: MAX_SEN_LEN (max sentence length) and MAX_WORDS (max number of words in corpus). You'll need to explore the dataset to set these two parameters properly\n",
    "    - initialize a tokenizer with parameter num_words = MAX_WORDS and set the tokenizer object as a *class object*\n",
    "    - fit the tokenizer using the training sentence pairs (i.e. method \"*fit_on_texts*\")\n",
    "- create a function **\"generate_seq\"** which does the following:\n",
    "    - take a list of sentences as an input \n",
    "    - generates padded sequences from the sentences using the class tokenizer object define above\n",
    "    - retrun the padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add import\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, Dropout, Activation, Input, Flatten, Concatenate\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# fix random number\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text_preprocessor class\n",
    "class text_preprocessor(object):\n",
    "    \n",
    "    # define __init__ function    \n",
    "    def __init__(self, max_sen_len, max_words, docs):\n",
    "        \n",
    "        # add your code here\n",
    "        # set sentence/document length\n",
    "        self.max_sen_len = max_sen_len\n",
    "        \n",
    "        # set the maximum number of words to be used\n",
    "        self.max_words = max_words\n",
    "        \n",
    "        #set data to self.doc\n",
    "        self.docs = docs\n",
    "        #print(self.docs.head()) #success\n",
    "        \n",
    "        \n",
    "        \n",
    "    # define generate_seq function \n",
    "    def generate_seq(self, docs):\n",
    "        \n",
    "        sequences = None\n",
    "        \n",
    "        # add your code here\n",
    "        # convert each document to a list of word index as a sequence\n",
    "        # get a Keras tokenizer\n",
    "        # https://keras.io/preprocessing/text/\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        #Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. \n",
    "        #So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 \n",
    "        #it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. \n",
    "        #So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "        \n",
    "        tokenizer.fit_on_texts(self.docs)\n",
    "        #Transforms each text in texts to a sequence of integers. So it basically takes each word in the text \n",
    "        #and replaces it with its corresponding integer value from the word_index dictionary. \n",
    "        #Nothing more, nothing less, certainly no magic involved.\n",
    "        \n",
    "        sequences = tokenizer.texts_to_sequences(self.docs)\n",
    "        \n",
    "        #this is final result, it's a \n",
    "        padded_sequences = pad_sequences(sequences, \\\n",
    "                                 maxlen=self.max_sen_len, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "        word_index = tokenizer.word_index\n",
    "        #print('Number of Unique Tokens',len(word_index))\n",
    "        #print(padded_sequences[24])\n",
    "        #print(padded_sequences.shape)\n",
    "        #print(type(padded_sequences))\n",
    "        \n",
    "        return padded_sequences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "0          Android phone is best up to range of 15000?\n",
      "1    I forgot my Gmail username and have no access ...\n",
      "2    What were the major effects of the cambodia ea...\n",
      "3    What are the best ways to clean a Jansport bac...\n",
      "4                   How can I download Arrow season 5?\n",
      "Name: question1, dtype: object\n",
      "Number of Unique Tokens 12011\n",
      "[  13  500   40   11  178    1  266    9 5694 3801 2924    6   32   82\n",
      " 2388 2925  281    1 1999    9   82  433    6   32  130 2925    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a function \"cnn_model\" to define a CNN model as follows:\n",
    "- take parameters: FILTER_SIZERS (a list of Conv1D filter sizes), NUM_FILTERS (the number of filters), MAX_WORDS, MAX_SEN_LEN, and EMBEDDING_DIM (dimision of word vectors)\n",
    "- define a CNN model with **Conv1D** using the specifified FILTER_SIZERS and NUM_FILTERS. For example, if FILTER_SIZERS=[1,2,3] and NUM_FILTERS=64, your model may look like the figure below.\n",
    "- return this CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='04_Images/12_cnn_model.png' width='50%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "def cnn_model(EMBEDDING_DIM, \\\n",
    "              # word vector dimension\n",
    "              FILTER_SIZES,\\\n",
    "              # filter sizes as a list\n",
    "              MAX_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_SEN_LEN, \\\n",
    "              # max words in a doc\n",
    "              #NAME = 'cnn',\\\n",
    "              NUM_FILTERS\\\n",
    "              #add input layer from outside\n",
    "              #outside_input\n",
    "              ):            \n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    # define input layer, where a sentence represented as\n",
    "    # 1 dimension array with integers\n",
    "    main_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='main_input')\n",
    "    \n",
    "    # define the embedding layer\n",
    "    # input_dim is the size of all words +1,because we will use wide convolution, we add zero before first input layer\n",
    "    # where 1 is for the padding symbol\n",
    "    # output_dim is the word vector dimension\n",
    "    # input_length is the max. length of a document\n",
    "    # input to embedding layer is the \"main_input\" layer\n",
    "    embed_1 = Embedding(input_dim=MAX_WORDS+1, \\\n",
    "                    output_dim=EMBEDDING_DIM, \\\n",
    "                    input_length=MAX_SEN_LEN,\\\n",
    "                    name='embedding')(main_input)  \n",
    "    \n",
    "\n",
    "    # define 1D convolution layer\n",
    "    # 64 filters are used\n",
    "    # a filter slides through each word (kernel_size=1)\n",
    "    # input to this layer is the embedding layer\n",
    "    conv1d_1= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_SIZES[0], \\\n",
    "                     name='conv_unigram',\\\n",
    "                     activation='relu')(embed_1)\n",
    "\n",
    "    # define a 1-dimension MaxPooling \n",
    "    # to take the output of the previous convolution layer\n",
    "    # the convolution layer produce \n",
    "    # MAX_SEN_LEN-1+1 values as ouput (???)\n",
    "    pool_1 = MaxPooling1D(MAX_SEN_LEN-1+1, \\\n",
    "                          name='pool_unigram')(conv1d_1)\n",
    "\n",
    "    # The pooling layer creates output \n",
    "    # in the size of (# of sample, 1, 64)  \n",
    "    # remove one dimension since the size is 1\n",
    "    flat_1 = Flatten(name='flat_unigram')(pool_1)\n",
    "\n",
    "    \n",
    "    #***********************************************************************************#\n",
    "    \n",
    "    \n",
    "    # following the same logic to define \n",
    "    # filters for bigram\n",
    "    conv1d_2= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_SIZES[1], \\\n",
    "                     name='conv_bigram',\\\n",
    "                     activation='relu')(embed_1)\n",
    "    pool_2 = MaxPooling1D(MAX_SEN_LEN-2+1, name='pool_bigram')(conv1d_2)\n",
    "    flat_2 = Flatten(name='flat_bigram')(pool_2)\n",
    "\n",
    "    \n",
    "    \n",
    "    #***********************************************************************************#\n",
    "        \n",
    "    # filters for trigram\n",
    "    conv1d_3= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_SIZES[2], \\\n",
    "                     name='conv_trigram',activation='relu')(embed_1)\n",
    "    pool_3 = MaxPooling1D(MAX_SEN_LEN-3+1, name='pool_trigram')(conv1d_3)\n",
    "    flat_3 = Flatten(name='flat_trigram')(pool_3)\n",
    "\n",
    "    # Concatenate flattened output\n",
    "    z=Concatenate(name='concate')([flat_1, flat_2, flat_3])\n",
    "\n",
    "    # create the model with input layer\n",
    "    # and the output layer\n",
    "    model = Model(inputs=main_input, outputs=z)\n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define three architecutres as described below. You may add appropriate regularizers in each model:\n",
    "- Model A: Use one CNN described above to process each question **without any parameter sharing**. Concatenate features extracted from both CNNs and then use a dense layer to predict the output\n",
    "- Model B: Use a **shared** CNN to process both questions. Concatenate features extracted from the CNN and then use a dense layer to predict the output\n",
    "- Model C: Use a shared CNN to process both questions. Then take the **absolute difference** between the feature vectors extracted from the CNN (hint, use keras \"Lambda\" layer), and connect the difference to a dense layer to predict the output\n",
    "- Model D (**Bonus**): You can come up with your own architecture as long as it can outperform the above models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model A | Model B   | Model C |\n",
    "|:------:|:------:|:---------:|\n",
    "|   <img src=\"04_Images/09_model_a.png\"/>| <img src=\"04_Images/10_model_b.png\" />| <img src=\"04_Images/11_model_c.png\"/> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Model: \"model_58\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1_input (InputLayer)           (None, 105)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2_input (InputLayer)           (None, 105)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_56 (Model)                (None, 192)          2077192     q1_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_57 (Model)                (None, 192)          2077192     q2_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concate (Concatenate)           (None, 384)          0           model_56[1][0]                   \n",
      "                                                                 model_57[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           concate[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 192)          73920       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            193         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,228,497\n",
      "Trainable params: 4,228,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x25a65c66668>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test for model\n",
    "\n",
    "FILTER_SIZES = [1, 2, 3]\n",
    "\n",
    "#how manny feature maps to extract\n",
    "NUM_FILTERS = 64\n",
    "\n",
    "#I think this parameter is saame as MAX_SEN_LEN\n",
    "#MAX_DOC_LEN = 500\n",
    "\n",
    "#for these two column sentence, the lengthist is 105 words in single sentence\n",
    "#MAX_SEN_LEN = len(df['question1'].max())\n",
    "MAX_SEN_LEN = 105\n",
    "\n",
    "#this is definnation of tokenzier max token value, smaller and less feature, and greater value will get more features\n",
    "#typical we setup 10000 to adapat requirement of word feature extract\n",
    "MAX_WORDS = 10000\n",
    "\n",
    "#the output dimention of embedding input layers \n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "NUM_EPOCHES = 10\n",
    "a = model_A(EMBEDDING_DIM, FILTER_SIZES,NUM_FILTERS,MAX_WORDS,MAX_SEN_LEN)\n",
    "\n",
    "a.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model A\n",
    "def model_A(EMBEDDING_DIM, FILTER_SIZES,NUM_FILTERS,MAX_WORDS,MAX_SEN_LEN):\n",
    "    \n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    q1_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q1_input')\n",
    "    q2_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q2_input')\n",
    "    \n",
    "    \n",
    "    left_cnn = cnn_model(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS)(q1_input)\n",
    "    print(type(left_cnn))\n",
    "    right_cnn = cnn_model(EMBEDDING_DIM, FILTER_SIZES,MAX_WORDS, MAX_SEN_LEN, NUM_FILTERS)(q2_input)\n",
    "    print(type(right_cnn))\n",
    "    con_layer=Concatenate(name='concate')([left_cnn, right_cnn])\n",
    "    \n",
    "    # Create a dropout layer\n",
    "    # In each iteration only 50% units are turned on\n",
    "    drop_1=Dropout(rate=0.5, name='dropout')(con_layer)\n",
    "\n",
    "    # Create a dense layer\n",
    "    dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "    # Create the output layer\n",
    "    preds = Dense(1, activation='sigmoid', name='output')(dense_1)\n",
    "    model = Model(inputs=[q1_input,q2_input], outputs=preds)\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model B\n",
    "\n",
    "def model_B(FILTER_SIZES,NUM_FILTERS, MAX_SEN_LEN, MAX_WORDS, EMBEDDING_DIM):\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    q1_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q1_input')\n",
    "    q2_input = Input(shape=(MAX_SEN_LEN,), dtype='int32', name='q2_input')\n",
    "    \n",
    "    \n",
    "    left_cnn = cnn_model(FILTER_SIZES,\\\n",
    "              # filter sizes as a list\n",
    "              MAX_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_SEN_LEN, \\\n",
    "              # max words in a doc\n",
    "              #NAME = 'cnn',\\\n",
    "              EMBEDDING_DIM, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS,\\\n",
    "              #add input layer from outside\n",
    "              q1_input)\n",
    "    \n",
    "    right_cnn = cnn_model(FILTER_SIZES,\\\n",
    "              # filter sizes as a list\n",
    "              MAX_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_SEN_LEN, \\\n",
    "              # max words in a doc\n",
    "              #NAME = 'cnn',\\\n",
    "              EMBEDDING_DIM, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS,\\\n",
    "              #add input layer from outside\n",
    "              q2_input)\n",
    "    \n",
    "    con_layer=Concatenate(name='concate')([left_cnn, right_cnn])\n",
    "    \n",
    "    # Create a dropout layer\n",
    "    # In each iteration only 50% units are turned on\n",
    "    drop_1=Dropout(rate=0.5, name='dropout')(con_layter)\n",
    "\n",
    "    # Create a dense layer\n",
    "    dense_1 = Dense(192, activation='relu', name='dense')(drop_1)\n",
    "    # Create the output layer\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model C\n",
    "\n",
    "def model_C(FILTER_SIZES,NUM_FILTERS, MAX_SEN_LEN, MAX_WORDS, EMBEDDING_DIM):\n",
    "    \n",
    "    model = None\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define a function \"train_model\" to:\n",
    "- Train a model provided as an input parameter\n",
    "- Use appropriate techniques to ensure you don't overfit the model\n",
    "- Plot training history to make sure your model is reasonable good\n",
    "- Using the testing dataset, calculate precision, recall, and F-1 score of each class (assuming 0.5 probabbility threshould), and also report AUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_model function\n",
    "\n",
    "def train_model(model, \\\n",
    "                q1_train, q2_train, y1_train, y2_train # training subset\n",
    "                #q1_val, q2_val,y_val, # evaluation subset\n",
    "                #q1_test, q2_test,y_test # evaluation subset\n",
    "                #BATCH_SIZE\n",
    "                #NUM_EPOCHES\n",
    "               ):\n",
    "    \n",
    "    # compile and train model\n",
    "    # process test dataset\n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", \\\n",
    "              metrics=[\"accuracy\"])\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHES = 10\n",
    "\n",
    "    # fit the model and save fitting history to \"training\"\n",
    "    training=model.fit(q1_train, y1_train, \\\n",
    "                   batch_size=BATCH_SIZE, \\\n",
    "                   epochs=NUM_EPOCHES,\\\n",
    "                   validation_data=[q1_val, y1_val], \\\n",
    "                   verbose=2)\n",
    "    \n",
    "    # plot training history\n",
    "\n",
    "    \n",
    "    # predict testing samples\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print classfication report\n",
    "    \n",
    "    \n",
    "    \n",
    "    # calculate ROC AUC\n",
    "    \n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Call the function \"train_model\" to get the performance of each model. Show your analysis as markdowns on the following:\n",
    "- How did you choose the hyperparaters: FILTER_SIZES,NUM_FILTERS, MAX_SEN_LEN, MAX_WORDS, EMBEDDING_DIM\n",
    "- Analyze each architecture to understand its pros and cons\n",
    "- Which architecture is the most effective and why is it effective for this classification task?\n",
    "- What regularizers did you use and why did it work?\n",
    "- What features do you think CNN can successfully extract? What kind of useful features could be missed by CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Model: \"model_55\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "q1_input (InputLayer)           (None, 105)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "q2_input (InputLayer)           (None, 105)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_53 (Model)                (None, 192)          2077192     q1_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "model_54 (Model)                (None, 192)          2077192     q2_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concate (Concatenate)           (None, 384)          0           model_53[1][0]                   \n",
      "                                                                 model_54[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 384)          0           concate[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 192)          73920       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            193         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,228,497\n",
      "Trainable params: 4,228,497\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From d:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[   2,   11,    1, ...,    0,    0,    0],\n       [   2,    3,    1, ...,    0,    0,    0],\n       [  24,  249,  120, ...,    0,    0,    0],\n       ...,\n       [  38,   10,   15, ...,    0, ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-e534bd31553f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     train_model(model_A, \\\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mq1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2_train\u001b[0m \u001b[1;31m# training subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[1;31m#q1_val, q2_val,y_val, # evaluation subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[1;31m#q1_test, q2_test,y_test # evaluation subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-251-4d312ca8af55>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, q1_train, q2_train, y1_train, y2_train)\u001b[0m\n\u001b[0;32m     23\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq1_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                    verbose=2)\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# plot training history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[   2,   11,    1, ...,    0,    0,    0],\n       [   2,    3,    1, ...,    0,    0,    0],\n       [  24,  249,  120, ...,    0,    0,    0],\n       ...,\n       [  38,   10,   15, ...,    0, ..."
     ]
    }
   ],
   "source": [
    "# Train and test each model\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    #read data\n",
    "    df_train = pd.read_csv('03_data/21_train.csv')\n",
    "    df_test = pd.read_csv('03_data/22_test.csv')\n",
    "    \n",
    "    # Set hyper parameters\n",
    "    #maybe means, first is unigram(kernel_size=1), second is bigram(kernel_size=2), third is trigram(kernel_size=3)\n",
    "    FILTER_SIZES = [1, 2, 3]\n",
    "    \n",
    "    #how manny feature maps to extract\n",
    "    NUM_FILTERS = 64\n",
    "    \n",
    "    #I think this parameter is saame as MAX_SEN_LEN\n",
    "    #MAX_DOC_LEN = 500\n",
    "    \n",
    "    #for these two column sentence, the lengthist is 105 words in single sentence\n",
    "    MAX_SEN_LEN = len(df['question1'].max())\n",
    "    \n",
    "    #this is definnation of tokenzier max token value, smaller and less feature, and greater value will get more features\n",
    "    #typical we setup 10000 to adapat requirement of word feature extract\n",
    "    MAX_WORDS = 10000\n",
    "    \n",
    "    #the output dimention of embedding input layers \n",
    "    EMBEDDING_DIM = 200\n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    \n",
    "    NUM_EPOCHES = 10\n",
    "    \n",
    "    #try to find the max length of these two questions set\n",
    "    print(len(df_train['question1'].max()))\n",
    "\n",
    "    # process training dataset\n",
    "    seq_1 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, df_train['question1'])\n",
    "    data_1= seq_1.generate_seq(df_train['question1'])\n",
    "    #print(type(data_1))\n",
    "    #print(data_1.shape)\n",
    "    \n",
    "    seq_2 = text_preprocessor(MAX_SEN_LEN, MAX_WORDS, df['question2'])\n",
    "    data_2 = seq_2.generate_seq(df['question2'])\n",
    "    \n",
    "   \n",
    "    #split into train and validation set\n",
    "    q1_train, q1_val, y1_train, y1_val = train_test_split(\\\n",
    "                        data_1, df_train['is_duplicate'],\\\n",
    "                        test_size=0.3, random_state=1)\n",
    "    q2_train, q2_val, y2_train, y2_val = train_test_split(\\\n",
    "                        data_2, df_train['is_duplicate'],\\\n",
    "                        test_size=0.3, random_state=1)\n",
    "    \n",
    "    \n",
    "    q1_test = df_test['question1']\n",
    "    q2_test = df_test['question2']\n",
    "    y_test = df_test['is_duplicate']\n",
    "    \n",
    "\n",
    "    # train and test model A/B/C\n",
    "    model_A= model_A(EMBEDDING_DIM, FILTER_SIZES,NUM_FILTERS,MAX_WORDS,MAX_SEN_LEN)\n",
    "    #model_A.summary\n",
    "    \n",
    "\n",
    "    train_model(model_A, \\\n",
    "                q1_train, q2_train,y1_train, y2_train # training subset\n",
    "                #q1_val, q2_val,y_val, # evaluation subset\n",
    "                #q1_test, q2_test,y_test # evaluation subset\n",
    "               )\n",
    "\n",
    "    \n",
    "    # add your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
